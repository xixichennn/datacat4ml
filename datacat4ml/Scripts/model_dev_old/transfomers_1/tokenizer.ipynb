{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: st (Python 3.12.2)\n",
    "import os\n",
    "import sys\n",
    "from datacat4ml.const import DATA_DIR, FETCH_DATA_DIR, FETCH_FIG_DIR, FEATURIZE_DATA_DIR, FEATURIZE_FIG_DIR\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gpcr_ki is: (139416, 28)\n",
      "The shape of or_ki is: (13533, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3315799/2581894743.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gpcr_ki = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ki_maxcur_8_data.csv'))\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "or_chembl_id = ['CHEMBL233', 'CHEMBL237', 'CHEMBL236', 'CHEMBL2014']\n",
    "gpcr_ki = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ki_maxcur_8_data.csv'))\n",
    "print(f\"The shape of gpcr_ki is: {gpcr_ki.shape}\")\n",
    "# extract the rows where the 'target_chembl_id' is one of the elements in the list OR_chembl_id\n",
    "or_ki = gpcr_ki[gpcr_ki['target_chembl_id'].isin(or_chembl_id)]\n",
    "print(f\"The shape of or_ki is: {or_ki.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcr_smis = gpcr_ki['canonical_smiles'].values\n",
    "or_smis = or_ki['canonical_smiles'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize SMILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepchem\n",
    "\n",
    "- Molecule Tokenizers\n",
    "    - SmilesTokenizer\n",
    "    - BasicSmilesTokenizer\n",
    "    - HuggingFaceFeaturizer\n",
    "- Other Featurizers\n",
    "    - BertFeaturizer\n",
    "    - RobertaFeaturizer\n",
    "    - RxnFeaturizer\n",
    "    - UserDefinedFeaturizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer shared by Markus\n",
    "\n",
    "- shared by Markus\n",
    "- return: encode --> token_ids\n",
    "- return: encode --> list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESTokenizer:\n",
    "    def __init__(self):\n",
    "        self.pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\!|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        self.vocab = {}\n",
    "        self.inv_vocab = {}\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.max_len = None\n",
    "    def tokenize(self, smiles):\n",
    "        \"\"\"Tokenizes a SMILES string using the predefined regular expression.\"\"\"\n",
    "        return re.findall(self.pattern, smiles)\n",
    "    def build_vocab(self, smiles_list):\n",
    "        \"\"\"Builds vocabulary from a list of SMILES strings.\"\"\"\n",
    "        all_tokens = set()\n",
    "        for smiles in smiles_list:\n",
    "            tokens = self.tokenize(smiles)\n",
    "            all_tokens.update(tokens)\n",
    "        tokens = [self.pad_token, self.unk_token, self.start_token, self.end_token]\n",
    "        all_tokens = sorted(all_tokens)\n",
    "        all_tokens = tokens + all_tokens\n",
    "        self.vocab = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "        self.inv_vocab = {idx: token for token, idx in self.vocab.items()}\n",
    "    def encode(self, smiles, max_len=None):\n",
    "        \"\"\"Encodes a SMILES string into a list of token indices, optionally padding to max_len.\"\"\"\n",
    "        tokens = self.tokenize(smiles)\n",
    "        tokens = [self.start_token] + tokens + [self.end_token]\n",
    "        token_ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        if max_len:\n",
    "            token_ids = token_ids[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(token_ids))\n",
    "        return token_ids\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decodes a list of token indices back into a SMILES string.\"\"\"\n",
    "        tokens = [self.inv_vocab.get(token_id, self.unk_token) for token_id in token_ids]\n",
    "        tokens = [token for token in tokens if token not in [self.start_token, self.end_token, self.pad_token]]\n",
    "        return ''.join(tokens)\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Returns the size of the vocabulary.\"\"\"\n",
    "        return len(self.vocab)\n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        \"\"\"Pads a sequence to the maximum length.\"\"\"\n",
    "        return sequence[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is: 79\n",
      "The first tokenized SMILES is: ['C', 'N', '1', 'C', 'C', '[C@]', '2', '3', 'c', '4', 'c', '5', 'c', 'c', 'c', '(', 'O', ')', 'c', '4', 'O', '[C@H]', '2', '[C@@H]', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '[C@H]', '2', 'C', 'C', '[C@@]', '4', '(', 'O', ')', '[C@H]', '6', 'C', 'c', '7', 'c', 'c', 'c', '(', 'O', ')', 'c', '8', 'c', '7', '[C@@]', '4', '(', 'C', 'C', 'N', '6', 'C', ')', '[C@H]', '2', 'O', '8', ')', 'C', 'C', '[C@@]', '3', '(', 'O', ')', '[C@H]', '1', 'C', '5']\n",
      "The first encoded SMILES is: [2, 21, 25, 10, 21, 21, 46, 11, 12, 75, 13, 75, 14, 75, 75, 75, 5, 26, 6, 75, 13, 26, 45, 11, 43, 5, 25, 21, 5, 19, 26, 6, 21, 25, 21, 5, 19, 26, 6, 21, 21, 21, 5, 19, 26, 6, 25, 21, 21, 5, 19, 26, 6, 25, 45, 11, 21, 21, 44, 13, 5, 26, 6, 45, 15, 21, 75, 16, 75, 75, 75, 5, 26, 6, 75, 17, 75, 16, 44, 13, 5, 21, 21, 25, 15, 21, 6, 45, 11, 26, 17, 6, 21, 21, 44, 12, 5, 26, 6, 45, 10, 21, 14, 3]\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary from the GPCR SMILES\n",
    "tokenizer = SMILESTokenizer()\n",
    "tokenizer.build_vocab(gpcr_smis)\n",
    "print(f\"The size of the vocabulary is: {tokenizer.vocab_size()}\")\n",
    "tokenized_gpcr_smis = [tokenizer.tokenize(smiles) for smiles in gpcr_smis]\n",
    "print(f\"The first tokenized SMILES is: {tokenized_gpcr_smis[0]}\")\n",
    "encode_gpcr_smis = [tokenizer.encode(smiles) for smiles in gpcr_smis]\n",
    "print(f\"The first encoded SMILES is: {encode_gpcr_smis[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is: 0\n",
      "The length of tokenized_gpcr_smis is 139416\n",
      "The first element of tokenized_gpcr_smis is:\n",
      " ['C', 'N', '1', 'C', 'C', '[C@]', '2', '3', 'c', '4', 'c', '5', 'c', 'c', 'c', '(', 'O', ')', 'c', '4', 'O', '[C@H]', '2', '[C@@H]', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '[C@H]', '2', 'C', 'C', '[C@@]', '4', '(', 'O', ')', '[C@H]', '6', 'C', 'c', '7', 'c', 'c', 'c', '(', 'O', ')', 'c', '8', 'c', '7', '[C@@]', '4', '(', 'C', 'C', 'N', '6', 'C', ')', '[C@H]', '2', 'O', '8', ')', 'C', 'C', '[C@@]', '3', '(', 'O', ')', '[C@H]', '1', 'C', '5']\n",
      "The length of tokenized_or_smis is 13533\n",
      "The first element of tokenized_or_smis is:\n",
      " ['C', 'N', '1', 'C', 'C', '[C@]', '2', '3', 'c', '4', 'c', '5', 'c', 'c', 'c', '(', 'O', ')', 'c', '4', 'O', '[C@H]', '2', '[C@@H]', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '[C@H]', '2', 'C', 'C', '[C@@]', '4', '(', 'O', ')', '[C@H]', '6', 'C', 'c', '7', 'c', 'c', 'c', '(', 'O', ')', 'c', '8', 'c', '7', '[C@@]', '4', '(', 'C', 'C', 'N', '6', 'C', ')', '[C@H]', '2', 'O', '8', ')', 'C', 'C', '[C@@]', '3', '(', 'O', ')', '[C@H]', '1', 'C', '5']\n"
     ]
    }
   ],
   "source": [
    "smi_tokenizer_M = SMILESTokenizer()\n",
    "\n",
    "print(f\"The size of the vocabulary is: {smi_tokenizer_M.vocab_size()}\")\n",
    "\n",
    "tokenized_gpcr_smis_M = [smi_tokenizer_M.tokenize(smi) for smi in gpcr_smis]\n",
    "# encode_gpcr_smis_M = [smi_tokenizer_M.encode(smi) for smi in gpcr_smis] --> KeyError: '<UNK>' \n",
    "print(f'The length of tokenized_gpcr_smis is {len(tokenized_gpcr_smis_M)}')\n",
    "print(f\"The first element of tokenized_gpcr_smis is:\\n {tokenized_gpcr_smis_M[0]}\")\n",
    "#print(f\"The length of encode_gpcr_smis is {len(encode_gpcr_smis_M)}\")\n",
    "\n",
    "tokenized_or_smis_M = [smi_tokenizer_M.tokenize(smi) for smi in or_smis]\n",
    "print(f'The length of tokenized_or_smis is {len(tokenized_or_smis_M)}')\n",
    "print(f\"The first element of tokenized_or_smis is:\\n {tokenized_or_smis_M[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Smiles Tokenizer\n",
    "- a regex tokenization pattern to tokenise SMILES strings\n",
    "- This tokenizer is to be used when a tokenizer that does not require the transformers library by HuggingFace is required.\n",
    "- firstly developed in MolecularTransformer (Schwaller et. al), and used in OpenNMT-py\n",
    "- return: tokens\n",
    "- return type: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "/tmp/ipykernel_2720419/3451568287.py:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "  pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n"
     ]
    }
   ],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of tokenized_gpcr_smis_B is:\n",
      " C N 1 C C [C@] 2 3 c 4 c 5 c c c ( O ) c 4 O [C@H] 2 [C@@H] ( N C ( = O ) C N C ( = O ) C C C ( = O ) N C C ( = O ) N [C@H] 2 C C [C@@] 4 ( O ) [C@H] 6 C c 7 c c c ( O ) c 8 c 7 [C@@] 4 ( C C N 6 C ) [C@H] 2 O 8 ) C C [C@@] 3 ( O ) [C@H] 1 C 5\n"
     ]
    }
   ],
   "source": [
    "tokenized_gpcr_smis_B = [smi_tokenizer(smi) for smi in gpcr_smis]\n",
    "print(f'The first element of tokenized_gpcr_smis_B is:\\n {tokenized_gpcr_smis_B[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From ChemBerta Transformer\n",
    "\n",
    "- used in MoleculeACE, also available in deepchem\n",
    "- retruns: A numpy arrray containing a featurized representation of datapoints\n",
    "- return type: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemberta_tokenizer(smi:str, max_smi_len: int=200, padding: bool=True, truncation: bool=True, \n",
    "                        auto_tokenizer: str = 'seyonec/PubChem10M_SMILES_BPE_450k'):\n",
    "    \"\"\"\n",
    "    Tokenize SMILES for a ChemBerta Transformer\n",
    "    \n",
    "    :param smi: (str)SMILES string\n",
    "    :param max_smi_len: (int) maximum SMILES length\n",
    "    :param padding: (bool) padding\n",
    "    :param truncation: (bool) allow truncation (you will need this for heterogenous SMILES strings)\n",
    "    :param auto_tokenizer: (str) tokenizer name provided by HuggingFace\n",
    "    \"\"\"\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(auto_tokenizer)\n",
    "    tokens = tokenizer(smi, return_tensors='pt', padding=padding, truncation=truncation, max_length=max_smi_len)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/miniconda3/envs/st/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/storage/homefs/yc24j783/miniconda3/envs/st/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of tokenized_smis_C is:\n",
      " {'input_ids': tensor([[ 0, 39,  2]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenized_smis_C = [chemberta_tokenizer(smi) for smi in gpcr_smis[0]]\n",
    "print(f'The first element of tokenized_smis_C is:\\n {tokenized_smis_C[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize assay-related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[147162, 957, 1732, 1360, 65644]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpcr_assay_ids = gpcr_ki['assay_id'].values.tolist()\n",
    "gpcr_assay_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of gpcr_assay_descs is <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Displacement of [3H]EK from Opioid receptor delta 1 in guinea pig brain membrane',\n",
       " 'Affinity for 5-hydroxytryptamine 1A receptor subtype',\n",
       " 'Affinity for 5-hydroxytryptamine 1D receptor subtype',\n",
       " 'Affinity for 5-hydroxytryptamine 1B receptor subtype',\n",
       " 'Binding affinity towards human ETA receptor expressed in CHO-K1 cells in the presence of 0.05 nM [125I]-labeled endothelin 1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpcr_assay_descs = gpcr_ki['assay_desc'].values.tolist()\n",
    "print(f\"The data type of gpcr_assay_descs is {type(gpcr_assay_descs)}\")\n",
    "# Create a temporary file to store assay descriptions\n",
    "with open('gpcr_assay_descs.txt', 'w') as f:\n",
    "    for desc in gpcr_assay_descs:\n",
    "        f.write(desc + '\\n')\n",
    "gpcr_assay_descs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "# initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# customize pre-tokenizer and decoder\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "# train tokenizer\n",
    "trainer = trainers.BpeTrainer(vocab_size=9000, min_frequency=2, limit_alphabet=55, special_tokens=['affinity', 'displacement', '3H', '125I', 'camp', 'gtp', 'calcium', 'ca2+', 'IP1', 'IP3', 'arrest', 'agonist'])\n",
    "tokenizer.train(['gpcr_assay_descs.txt'], trainer=trainer) #? shall I remove duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assay_desc_tokenizer(sentence):\n",
    "    '''Tokenize a sentense, optimized for assay description'''\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    my_list = [item for item in encoded.tokens] \n",
    "    my_list = ' '.join(my_list)\n",
    "\n",
    "    return my_list\n",
    "\n",
    "def enzyme_sentence_tokenizer(sentence):\n",
    "    '''\n",
    "    Tokenize a sentenze, optimized for enzyme-like descriptions & names\n",
    "    '''\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    my_list = [item for item in encoded.tokens if 'Ġ' != item]\n",
    "    my_list = [item.replace('Ġ', '_') for item in my_list]\n",
    "    my_list = ' '.join(my_list)\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Displacement _of _[ 3H ] EK _from _Opioid _receptor _delta _1 _in _guinea _pig _brain _membrane',\n",
       " '_Affinity _for _5 - hydroxytryptamine _1 A _receptor _subtype',\n",
       " '_Affinity _for _5 - hydroxytryptamine _1 D _receptor _subtype',\n",
       " '_Affinity _for _5 - hydroxytryptamine _1 B _receptor _subtype',\n",
       " '_Binding affinity _towards _human _ETA _receptor _expressed _in _CHO - K 1 _cells _in _the _presence _of _0 . 05 _nM _[ 125I ]- labeled _endothelin _1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_assay_descs = [enzyme_sentence_tokenizer(assay_desc) for assay_desc in gpcr_assay_descs]\n",
    "tokenized_assay_descs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identifiers in assay table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the tokenized SMILES and tokenized assay-related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method in Enzymatic Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES + assay_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES + assay-desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES + assay-related info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
