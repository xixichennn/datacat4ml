{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: st(Python 3.12.2)\n",
    "import os\n",
    "import sys\n",
    "from datacat4ml.const import DATA_DIR\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2340026/1397084437.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gpcr_ki = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ki_maxcur_8_data.csv'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gpcr_ki is: (139416, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2340026/1397084437.py:4: DtypeWarning: Columns (7,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gpcr_ic50 = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ic50_maxcur_8_data.csv'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gpcr_ic50 is: (88622, 28)\n",
      "The shape of gpcr_ec50 is: (56294, 28)\n",
      "The shape of or_ki is: (13533, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2340026/1397084437.py:6: DtypeWarning: Columns (7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gpcr_ec50 = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ec50_maxcur_8_data.csv'))\n"
     ]
    }
   ],
   "source": [
    "or_chembl_id = ['CHEMBL233', 'CHEMBL237', 'CHEMBL236', 'CHEMBL2014']\n",
    "gpcr_ki = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ki_maxcur_8_data.csv'))\n",
    "print(f\"The shape of gpcr_ki is: {gpcr_ki.shape}\")\n",
    "gpcr_ic50 = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ic50_maxcur_8_data.csv'))\n",
    "print(f\"The shape of gpcr_ic50 is: {gpcr_ic50.shape}\")\n",
    "gpcr_ec50 = pd.read_csv(os.path.join(DATA_DIR, 'data_prep', '1_data_fetch', 'ec50_maxcur_8_data.csv'))\n",
    "print(f\"The shape of gpcr_ec50 is: {gpcr_ec50.shape}\")\n",
    "# extract the rows where the 'target_chembl_id' is one of the elements in the list OR_chembl_id\n",
    "or_ki = gpcr_ki[gpcr_ki['target_chembl_id'].isin(or_chembl_id)]\n",
    "print(f\"The shape of or_ki is: {or_ki.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique assay_desc in gpcr_ki is: 12520\n",
      "The number of unique assay_desc in gpcr_ic50 is: 8234\n",
      "The number of unique assay_desc in gpcr_ec50 is: 5488\n"
     ]
    }
   ],
   "source": [
    "gpcr_ki_assay_descs = gpcr_ki['assay_desc'].unique()\n",
    "print(f\"The number of unique assay_desc in gpcr_ki is: {len(gpcr_ki_assay_descs)}\")\n",
    "gpcr_ic50_assay_descs = gpcr_ic50['assay_desc'].unique()\n",
    "print(f\"The number of unique assay_desc in gpcr_ic50 is: {len(gpcr_ic50_assay_descs)}\")\n",
    "gpcr_ec50_assay_descs = gpcr_ec50['assay_desc'].unique()\n",
    "print(f\"The number of unique assay_desc in gpcr_ec50 is: {len(gpcr_ec50_assay_descs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique assay_desc in gpcr_assay_descs is: 26242\n",
      "The data type of gpcr_assay_descs is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# combine gpcr_ki_assay_desc, gpcr_ic50_assay_desc, gpcr_ec50_assay_desc into a single list\n",
    "gpcr_assay_descs = gpcr_ki_assay_descs.tolist() + gpcr_ic50_assay_descs.tolist() + gpcr_ec50_assay_descs.tolist()\n",
    "print(f\"The number of unique assay_desc in gpcr_assay_descs is: {len(gpcr_assay_descs)}\")\n",
    "print(f\"The data type of gpcr_assay_descs is: {type(gpcr_assay_descs)}\")\n",
    "\n",
    "# save the list to a file\n",
    "with open('gpcr_assay_descs.txt', 'w') as f:\n",
    "    for item in gpcr_assay_descs:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compound smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique canonical_smiles in gpcr_ki is: 67262\n",
      "The number of unique canonical_smiles in gpcr_ic50 is: 58753\n",
      "The number of unique canonical_smiles in gpcr_ec50 is: 33821\n"
     ]
    }
   ],
   "source": [
    "gpcr_ki_smis = gpcr_ki['canonical_smiles'].unique()\n",
    "print(f\"The number of unique canonical_smiles in gpcr_ki is: {len(gpcr_ki_smis)}\")\n",
    "gpcr_ic50_smis = gpcr_ic50['canonical_smiles'].unique()\n",
    "print(f\"The number of unique canonical_smiles in gpcr_ic50 is: {len(gpcr_ic50_smis)}\")\n",
    "gpcr_ec50_smis = gpcr_ec50['canonical_smiles'].unique()\n",
    "print(f\"The number of unique canonical_smiles in gpcr_ec50 is: {len(gpcr_ec50_smis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique assay_desc in gpcr_assay_smis is: 159836\n",
      "The data type of gpcr_assay_smis is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# combine gpcr_ki_smis, gpcr_ic50_smis, gpcr_ec50_smis into a single list\n",
    "gpcr_smis = gpcr_ki_smis.tolist() + gpcr_ic50_smis.tolist() + gpcr_ec50_smis.tolist()\n",
    "print(f\"The number of unique assay_desc in gpcr_assay_smis is: {len(gpcr_smis)}\")\n",
    "print(f\"The data type of gpcr_assay_smis is: {type(gpcr_smis)}\")\n",
    "\n",
    "# save the list to a file\n",
    "with open('gpcr_smis.txt', 'w') as f:\n",
    "    for item in gpcr_smis:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compound SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SMILESTokenizer:\n",
    "\n",
    "    \"\"\"\n",
    "    Shared by Markus.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\!|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        self.vocab = {}\n",
    "        self.inv_vocab = {}\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.max_len = None\n",
    "    def tokenize(self, smiles):\n",
    "        \"\"\"Tokenizes a SMILES string using the predefined regular expression.\"\"\"\n",
    "        return re.findall(self.pattern, smiles)\n",
    "    def build_vocab(self, smiles_list):\n",
    "        \"\"\"Builds vocabulary from a list of SMILES strings.\"\"\"\n",
    "        all_tokens = set()\n",
    "        for smiles in smiles_list:\n",
    "            tokens = self.tokenize(smiles)\n",
    "            all_tokens.update(tokens)\n",
    "        tokens = [self.pad_token, self.unk_token, self.start_token, self.end_token]\n",
    "        all_tokens = sorted(all_tokens)\n",
    "        all_tokens = tokens + all_tokens\n",
    "        self.vocab = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "        self.inv_vocab = {idx: token for token, idx in self.vocab.items()}\n",
    "    def encode(self, smiles, max_len=None):\n",
    "        \"\"\"Encodes a SMILES string into a list of token indices, optionally padding to max_len.\"\"\"\n",
    "        tokens = self.tokenize(smiles)\n",
    "        tokens = [self.start_token] + tokens + [self.end_token]\n",
    "        token_ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        if max_len:\n",
    "            token_ids = token_ids[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(token_ids))\n",
    "        return token_ids\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decodes a list of token indices back into a SMILES string.\"\"\"\n",
    "        tokens = [self.inv_vocab.get(token_id, self.unk_token) for token_id in token_ids]\n",
    "        tokens = [token for token in tokens if token not in [self.start_token, self.end_token, self.pad_token]]\n",
    "        return ''.join(tokens)\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Returns the size of the vocabulary.\"\"\"\n",
    "        return len(self.vocab)\n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        \"\"\"Pads a sequence to the maximum length.\"\"\"\n",
    "        return sequence[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is: 93\n",
      "The first tokenized SMILES is: ['C', 'N', '1', 'C', 'C', '[C@]', '2', '3', 'c', '4', 'c', '5', 'c', 'c', 'c', '(', 'O', ')', 'c', '4', 'O', '[C@H]', '2', '[C@@H]', '(', 'N', 'C', '(', '=', 'O', ')', 'C', 'N', 'C', '(', '=', 'O', ')', 'C', 'C', 'C', '(', '=', 'O', ')', 'N', 'C', 'C', '(', '=', 'O', ')', 'N', '[C@H]', '2', 'C', 'C', '[C@@]', '4', '(', 'O', ')', '[C@H]', '6', 'C', 'c', '7', 'c', 'c', 'c', '(', 'O', ')', 'c', '8', 'c', '7', '[C@@]', '4', '(', 'C', 'C', 'N', '6', 'C', ')', '[C@H]', '2', 'O', '8', ')', 'C', 'C', '[C@@]', '3', '(', 'O', ')', '[C@H]', '1', 'C', '5']\n",
      "The first encoded SMILES is: [2, 25, 29, 13, 25, 25, 53, 14, 15, 89, 16, 89, 17, 89, 89, 89, 8, 30, 9, 89, 16, 30, 52, 14, 50, 8, 29, 25, 8, 22, 30, 9, 25, 29, 25, 8, 22, 30, 9, 25, 25, 25, 8, 22, 30, 9, 29, 25, 25, 8, 22, 30, 9, 29, 52, 14, 25, 25, 51, 16, 8, 30, 9, 52, 18, 25, 89, 19, 89, 89, 89, 8, 30, 9, 89, 20, 89, 19, 51, 16, 8, 25, 25, 29, 18, 25, 9, 52, 14, 30, 20, 9, 25, 25, 51, 15, 8, 30, 9, 52, 13, 25, 17, 3]\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary from the GPCR SMILES\n",
    "tokenizer = SMILESTokenizer()\n",
    "tokenizer.build_vocab(gpcr_smis)\n",
    "print(f\"The size of the vocabulary is: {tokenizer.vocab_size()}\")\n",
    "tokenized_gpcr_smi_1 = tokenizer.tokenize(gpcr_smis[0]) \n",
    "print(f\"The first tokenized SMILES is: {tokenized_gpcr_smi_1}\")\n",
    "encode_gpcr_smi_1 = tokenizer.encode(gpcr_smis[0])\n",
    "print(f\"The first encoded SMILES is: {encode_gpcr_smi_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assay-related text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BPE_tokenizer(special_tokens: List[str] = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[3H]\", \"[125I]\", \"[35S]\"], \n",
    "                        assay_files: List[str]=['./gpcr_assay_descs.txt']):\n",
    "\n",
    "    \"\"\" Based on BPE model, train a tokenizer from scratch using the entire dataset.\n",
    "    \n",
    "    Params:\n",
    "    - special_tokens: List(str) - A list of special tokens that we want to add to the tokenizer\n",
    "    - all_X_txt: List[str] - A list of path to the files that we should use for training\n",
    "    \n",
    "    Returns:\n",
    "    - tokenizer: PreTrainedTokenizerFast object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "    # Initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # Specify a pre-tokenizer before training\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "    # Train the tokenizer\n",
    "    trainer = trainers.BpeTrainer(vocab_size=25_000, min_frequency=2, special_tokens=special_tokens)\n",
    "    # To ensure a comprehensive vocabulary, use the entire datasets including those that will later be split into training and validation sets\n",
    "    tokenizer.train(trainer=trainer, files=assay_files) \n",
    "\n",
    "    # Specify a decoder and post-process before training\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "    # Wrap this in a Tranformers tokenizer object:\n",
    "    from transformers import PreTrainedTokenizerFast\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The number of tokens in the vocabulary is: 10873\n",
      "The first 10 tokens are: ['TX', 'Ġcauses', 'Ġaround', 'mes', 'hym', 'viroc', 'yers', 'activation', 'ĠNaF', 'ĠLiaw']\n",
      "The tokenized sentence is: ['ĠDisplacement', 'Ġof', 'Ġ', '[3H]', 'ĠN', 'al', 't', 'indole', 'Ġfrom', 'Ġhuman', 'Ġdelta', 'Ġopioid', 'Ġreceptor', 'Ġexpressed', 'Ġin', 'ĠCHO', 'Ġcell', 'Ġmembrane']\n",
      "The encoded sentence is: {'input_ids': [197, 154, 112, 4, 355, 209, 86, 1915, 188, 147, 601, 424, 149, 170, 135, 202, 160, 462], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/miniconda3/envs/st/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BPE_tokenizer = train_BPE_tokenizer()\n",
    "# have a look at the vocabulary\n",
    "vocab = BPE_tokenizer.get_vocab()\n",
    "print(f\"The number of tokens in the vocabulary is: {len(vocab)}\")\n",
    "# print the first 10 tokens\n",
    "print(f\"The first 10 tokens are: {list(vocab.keys())[:10]}\")\n",
    "# use this tokenizer to tokenize a sentence and then encode it\n",
    "sentence = \"Displacement of [3H]Naltindole from human delta opioid receptor expressed in CHO cell membrane\"\n",
    "tokenized_sentence = BPE_tokenizer.tokenize(sentence)\n",
    "encoded = BPE_tokenizer(sentence)\n",
    "print(f\"The tokenized sentence is: {tokenized_sentence}\")\n",
    "print(f\"The encoded sentence is: {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try wordlevl and wordpiece tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the following config for the model\n",
    "from transformers import RobertaConfig\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the tokenizer in transformers\n",
    "# It is necessary because it ennsures compatibility between the tokenizer and the transformer model. \n",
    "# The `transformers` provides additional features and integration that may not be available in the `tokenizers` library alone\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./from_scratch\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model.\n",
    "# As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint.\n",
    "\n",
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "model.num_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
