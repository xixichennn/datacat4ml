[2024-09-28 17:29:15,759 INFO] Extracting features...
[2024-09-28 17:29:15,778 INFO]  * number of source features: 0.
[2024-09-28 17:29:15,778 INFO]  * number of target features: 0.
[2024-09-28 17:29:15,778 INFO] Building `Fields` object...
[2024-09-28 17:29:15,778 INFO] Building & saving training data...
[2024-09-28 17:29:15,778 WARNING] Shards for corpus train already exist, won't be overwritten, pass the `-overwrite` option if you want to.
[2024-09-28 17:29:15,778 INFO] Building & saving validation data...
[2024-09-28 17:29:15,778 WARNING] Shards for corpus valid already exist, won't be overwritten, pass the `-overwrite` option if you want to.
[2024-09-28 17:29:16,841 INFO]  * src vocab size = 214
[2024-09-28 17:29:16,841 INFO]  * tgt vocab size = 214
[2024-09-28 17:29:16,842 INFO] Building model...
[2024-09-28 17:29:20,201 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(214, 384, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(214, 384, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=384, out_features=384, bias=True)
          (linear_values): Linear(in_features=384, out_features=384, bias=True)
          (linear_query): Linear(in_features=384, out_features=384, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=384, out_features=384, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=384, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=384, bias=True)
          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=384, out_features=214, bias=True)
    (1): Cast()
    (2): LogSoftmax(dim=-1)
  )
)
[2024-09-28 17:29:20,205 INFO] encoder: 8755712
[2024-09-28 17:29:20,205 INFO] decoder: 11206614
[2024-09-28 17:29:20,205 INFO] * number of parameters: 19962326
[2024-09-28 17:29:23,313 INFO] Starting training on GPU: [0]
[2024-09-28 17:29:23,314 INFO] Start training loop and validate every 5000 steps...
[2024-09-28 17:29:23,314 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:23,326 INFO] number of examples: 229
[2024-09-28 17:29:25,170 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:25,173 INFO] number of examples: 229
[2024-09-28 17:29:25,448 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:25,450 INFO] number of examples: 229
[2024-09-28 17:29:25,672 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:25,674 INFO] number of examples: 229
[2024-09-28 17:29:25,951 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:25,953 INFO] number of examples: 229
[2024-09-28 17:29:26,175 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:26,177 INFO] number of examples: 229
[2024-09-28 17:29:26,452 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:26,454 INFO] number of examples: 229
[2024-09-28 17:29:26,676 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:26,678 INFO] number of examples: 229
[2024-09-28 17:29:26,954 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:26,956 INFO] number of examples: 229
[2024-09-28 17:29:27,177 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:27,180 INFO] number of examples: 229
[2024-09-28 17:29:27,456 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:27,458 INFO] number of examples: 229
[2024-09-28 17:29:27,679 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:27,681 INFO] number of examples: 229
[2024-09-28 17:29:27,957 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:27,959 INFO] number of examples: 229
[2024-09-28 17:29:28,180 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:28,182 INFO] number of examples: 229
[2024-09-28 17:29:28,457 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:28,460 INFO] number of examples: 229
[2024-09-28 17:29:28,681 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:28,683 INFO] number of examples: 229
[2024-09-28 17:29:28,959 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:28,961 INFO] number of examples: 229
[2024-09-28 17:29:29,181 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:29,183 INFO] number of examples: 229
[2024-09-28 17:29:29,460 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:29,462 INFO] number of examples: 229
[2024-09-28 17:29:29,683 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:29,685 INFO] number of examples: 229
[2024-09-28 17:29:29,961 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:29,963 INFO] number of examples: 229
[2024-09-28 17:29:30,183 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:30,185 INFO] number of examples: 229
[2024-09-28 17:29:30,462 INFO] Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/preprocessed_data.train.0.pt
[2024-09-28 17:29:30,464 INFO] number of examples: 229
[2024-09-28 17:29:30,521 INFO] Saving checkpoint /storage/homefs/yc24j783/datacat4ml/datacat4ml/Scripts/model_dev/enztrans_test/data/ki_mor_1_test2/model_step_100.pt
