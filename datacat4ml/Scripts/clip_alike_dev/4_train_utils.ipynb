{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350b18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/miniconda3/envs/clamp_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# conda env: datacat (python=3.8.2)\n",
    "# for `utils.py`\n",
    "# ==== all.py ====\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ==== metrics.py ====\n",
    "from sklearn import metrics\n",
    "\n",
    "# ==== dataloader.py ====\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "try: # only if Graph-Model is used\n",
    "    import dgl\n",
    "except: pass\n",
    "\n",
    "# ==== utils.py ====\n",
    "import json\n",
    "from loguru import logger\n",
    "import mlflow\n",
    "import mlflow.entities\n",
    "from datacat4ml.Scripts.model_dev.model_def import DotProduct\n",
    "import datacat4ml.Scripts.model_dev.model_def as model_def\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiplicativeLR\n",
    "from torch.utils.data import Subset, RandomSampler, SequentialSampler, BatchSampler\n",
    "from scipy.special import expit as sigmoid\n",
    "import scipy \n",
    "\n",
    "# ==== train.py ====\n",
    "import argparse\n",
    "#import mlflow\n",
    "import random\n",
    "import wandb\n",
    "from time import time\n",
    "\n",
    "from datacat4ml.const import DATA_DIR, SCRIPTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ee8d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxixichennn\u001b[0m (\u001b[33mxixichennn-freie-universit-t-berlin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xixichennn-freie-universit-t-berlin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "print(wandb.api.default_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704d9bb",
   "metadata": {},
   "source": [
    "# models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c8a09",
   "metadata": {},
   "source": [
    "`DotProduct` - The difference between `forward` and `forward_dense`\n",
    "- forward: `preactivations = (compound_embeddings * assay_embeddings).sum(axis=1)`\n",
    "- forward_dense: `preactivations = (compound_embeddings @ assay_embeddings.T)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound_embeddings:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "compound_embeddings.shape: torch.Size([3, 2])\n",
      "compound_embeddings.shape[0]: 3\n",
      "compound_embeddings.shape[1]: 2 \n",
      "\n",
      "assay_embeddings:\n",
      " tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n",
      "assay_embeddings.shape: torch.Size([3, 2])\n",
      "assay_embeddings.shape[0]: 3\n",
      "assay_embeddings.shape[1]: 2 \n",
      "\n",
      "compound_embeddings * assay_embeddings:\n",
      " tensor([[ 10,  40],\n",
      "        [ 90, 160],\n",
      "        [250, 360]])\n",
      "(compound_embeddings * assay_embeddings).sum(axis=0):\n",
      " tensor([350, 560])\n",
      "Forward: tensor([ 50, 250, 610]) \n",
      "\n",
      "compound_embeddings @ assay_embeddings.T:\n",
      " tensor([[ 50, 110, 170],\n",
      "        [110, 250, 390],\n",
      "        [170, 390, 610]])\n",
      "assay_embeddings.T:\n",
      " tensor([[10, 30, 50],\n",
      "        [20, 40, 60]])\n",
      "forward_dense:\n",
      " tensor([[ 50, 110, 170],\n",
      "        [110, 250, 390],\n",
      "        [170, 390, 610]])\n",
      "forward_dense.diag():\n",
      " tensor([ 50, 250, 610])\n"
     ]
    }
   ],
   "source": [
    "# Suppose Embedding size(D) = 2, number of compounds(N) = 3, number of assays(M) = 2\n",
    "compound_embeddings = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"compound_embeddings:\\n\", compound_embeddings)\n",
    "print(\"compound_embeddings.shape:\", compound_embeddings.shape) \n",
    "print(\"compound_embeddings.shape[0]:\", compound_embeddings.shape[0])  \n",
    "print(\"compound_embeddings.shape[1]:\", compound_embeddings.shape[1], \"\\n\")\n",
    "\n",
    "assay_embeddings = torch.tensor([[10, 20], [30, 40], [50, 60]])\n",
    "print(\"assay_embeddings:\\n\", assay_embeddings)\n",
    "print(\"assay_embeddings.shape:\", assay_embeddings.shape)  \n",
    "print(\"assay_embeddings.shape[0]:\", assay_embeddings.shape[0])\n",
    "print(\"assay_embeddings.shape[1]:\", assay_embeddings.shape[1], \"\\n\")\n",
    "\n",
    "forward = (compound_embeddings * assay_embeddings).sum(axis=1) \n",
    "#  [1*10 + 2*20, 3*30 + 4*40, 5*50 + 6*60] = （C1*A1, C2*A2, C3*A3）\n",
    "print(\"compound_embeddings * assay_embeddings:\\n\", compound_embeddings * assay_embeddings)\n",
    "print(\"(compound_embeddings * assay_embeddings).sum(axis=0):\\n\", (compound_embeddings * assay_embeddings).sum(axis=0))\n",
    "print(\"forward:\", forward, \"\\n\")\n",
    "\n",
    "forward_dense = (compound_embeddings @ assay_embeddings.T) \n",
    "# [[1*10 +2*20, 1*30 + 2*40, 1*50 + 2*60],\n",
    "#  [3*10 + 4*20, 3*30 + 4*40, 3*50 + 4*60],\n",
    "#  [5*10 + 6*20, 5*30 + 6*40, 5*50 + 6*60]]\n",
    "# = \n",
    "# [[C1A1, C1A2, C1A3],\n",
    "#  [C2A1, C2A2, C2A3],\n",
    "#  [C3A1, C3A2, C3A3]]\n",
    "print(\"compound_embeddings @ assay_embeddings.T:\\n\", forward_dense)\n",
    "print('assay_embeddings.T:\\n', assay_embeddings.T)\n",
    "print(\"forward_dense:\\n\", forward_dense)\n",
    "print(\"forward_dense.diag():\\n\", forward_dense.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2671561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_embeds.shape: torch.Size([3, 512])\n",
      "a_embeds.shape: torch.Size([3, 512]) \n",
      "\n",
      "fwd: tensor([129.9870, 128.8514, 130.4847])\n",
      "fwd.shape: torch.Size([3]) \n",
      "\n",
      "fwd_dense:\n",
      " tensor([[129.9870, 130.7846, 136.4513],\n",
      "        [125.9640, 128.8514, 129.9631],\n",
      "        [124.3015, 125.3976, 130.4847]])\n",
      "fwd_dense.shape: torch.Size([3, 3])\n",
      "torch.diag(fwd_dense): tensor([129.9870, 128.8514, 130.4847])\n",
      "The length of c_embeds is len 3\n",
      "c_embeds.shape[0] is 3\n"
     ]
    }
   ],
   "source": [
    "# given assay_embedding.size = (3, 512), compound_embedding.size = (3, 512); i.e. 3 compounds and 3 assays, each with 512-dimensional embeddings\n",
    "# initialize 'c_embeds' and 'a_embeds' with random integers \n",
    "#c_embeds = torch.randi((3, 512))\n",
    "\n",
    "c_embeds = torch.rand((3, 512))\n",
    "a_embeds = torch.rand((3, 512))\n",
    "print(\"c_embeds.shape:\", c_embeds.shape)\n",
    "print(\"a_embeds.shape:\", a_embeds.shape, \"\\n\")\n",
    "\n",
    "fwd = (c_embeds * a_embeds).sum(axis=1)\n",
    "print(\"fwd:\", fwd)\n",
    "print(\"fwd.shape:\", fwd.shape, \"\\n\")\n",
    "\n",
    "fwd_dense = (c_embeds @ a_embeds.T)\n",
    "print(\"fwd_dense:\\n\", fwd_dense)\n",
    "print(\"fwd_dense.shape:\", fwd_dense.shape)\n",
    "print(\"torch.diag(fwd_dense):\", torch.diag(fwd_dense))\n",
    "\n",
    "print(f'The length of c_embeds is len {len(c_embeds)}')\n",
    "print(f'c_embeds.shape[0] is {c_embeds.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c29fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input before scaling:\n",
      " tensor([[129.9870, 130.7846, 136.4513],\n",
      "        [125.9640, 128.8514, 129.9631],\n",
      "        [124.3015, 125.3976, 130.4847]])\n",
      "\n",
      "beta is 0.5773502691896258\n",
      "\n",
      "target before scaling:\n",
      " tensor([1, 1, 0])\n",
      "\n",
      "target*2: tensor([2, 2, 0])\n",
      "target*2 - 1: tensor([ 1,  1, -1])\n",
      "\n",
      "input after scaling:\n",
      " tensor([[ 75.0480,  75.5085, -78.7802],\n",
      "        [ 72.7253,  74.3924, -75.0342],\n",
      "        [ 71.7655,  72.3983, -75.3354]]) \n",
      "\n",
      "target after scaling: tensor([0, 1, 2])\n",
      "loss calculated by CustomCE is 86.402587890625\n"
     ]
    }
   ],
   "source": [
    "# ===================== CustomCE ======================\n",
    "input = fwd_dense\n",
    "print(f'input before scaling:\\n {input}\\n')\n",
    "# class customCE(nn.CrossEntropyLoss)\n",
    "# def forward\n",
    "#beta\n",
    "beta = 1/(input.shape[0]**(1/2)) # = 1/√3 = 0.5773502691896257\n",
    "print(f'beta is {beta}\\n')\n",
    "\n",
    "target = torch.tensor([1, 1, 0])\n",
    "print(f'target before scaling:\\n {target}\\n')\n",
    "print(f'target*2: {target*2}')\n",
    "print(f'target*2 - 1: {target*2 - 1}\\n')\n",
    "# input\n",
    "input = input*(target*2 - 1)*beta\n",
    "print(f'input after scaling:\\n {input} \\n')\n",
    "\n",
    "#target\n",
    "target = torch.arange(0,len(input)).to(input.device)\n",
    "print(f'target after scaling: {target}')\n",
    "\n",
    "class CustomCE(nn.CrossEntropyLoss):\n",
    "    \"\"\" Cross entropy loss \"\"\"\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        beta = 1/(input.shape[0]**(1/2))\n",
    "        input = input*(target*2-1)*beta # target from [0,1] to [-1,1]\n",
    "        target = torch.arange(0,len(input)).to(input.device)\n",
    "        \n",
    "        return F.cross_entropy(input, target, weight=self.weight,\n",
    "                        ignore_index=self.ignore_index, reduction=self.reduction)\n",
    "\n",
    "criterion = CustomCE()\n",
    "loss = criterion(input, target)\n",
    "\n",
    "print(f'loss calculated by CustomCE is {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd9888e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input before scaling:\n",
      " tensor([[125.0471, 127.5372, 128.1412],\n",
      "        [122.0491, 129.2777, 130.8487],\n",
      "        [125.1797, 129.8019, 126.7569]])\n",
      "\n",
      "target before scaling:\n",
      " tensor([0, 1, 0])\n",
      "\n",
      "bs is 3\n",
      "torch.eye(bs) is \n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "(1-torch.eye(bs)) is \n",
      "tensor([[0., 1., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 1., 0.]])\n",
      "(target*2-1) is tensor([-1,  1, -1])\n",
      "torch.eye(bs)*(target*2-1) is \n",
      "tensor([[-1.,  0., -0.],\n",
      "        [-0.,  1., -0.],\n",
      "        [-0.,  0., -1.]])\n",
      "modif:\n",
      "tensor([[-1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.]])\n",
      "\n",
      "input after scaling:\n",
      " tensor([[-125.0471,  127.5372,  128.1412],\n",
      "        [ 122.0491,  129.2777,  130.8487],\n",
      "        [ 125.1797,  129.8019, -126.7569]])\n",
      "\n",
      "diag_idx: tensor([0, 1, 2])\n",
      "\n",
      "loss calculated by ConLoss is 4.809605598449707\n"
     ]
    }
   ],
   "source": [
    "# ===================== Contrastive Loss ======================\n",
    "# params\n",
    "input = fwd_dense\n",
    "print(f'input before scaling:\\n {input}\\n')\n",
    "target = torch.tensor([0, 1, 0])\n",
    "print(f'target before scaling:\\n {target}\\n')\n",
    "\n",
    "# def forward\n",
    "sigma = 1\n",
    "bs = target.shape[0] # batch size.\n",
    "print(f'bs is {bs}')\n",
    "\n",
    "# modif\n",
    "modif = (1-torch.eye(bs)).to(target.device) + (torch.eye(bs).to(target.device)*(target*2-1))\n",
    "print(f'torch.eye(bs) is \\n{torch.eye(bs)}')\n",
    "print(f'(1-torch.eye(bs)) is \\n{(1-torch.eye(bs))}')\n",
    "print(f'(target*2-1) is {(target*2-1)}')\n",
    "print(f'torch.eye(bs)*(target*2-1) is \\n{torch.eye(bs)*(target*2-1)}')\n",
    "print(f'modif:\\n{modif}\\n')\n",
    "\n",
    "#input\n",
    "input = input*modif/sigma\n",
    "print(f'input after scaling:\\n {input}\\n')\n",
    "\n",
    "diag_idx = torch.arange(0,len(input)).to(input.device)\n",
    "print(f'diag_idx: {diag_idx}\\n')\n",
    "\n",
    "class ConLoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"\"Contrastive Loss\"\"\"\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        sigma = 1\n",
    "        bs = target.shape[0]\n",
    "        #only modify diag that is a negative\n",
    "        # eg makes this from a target of [0, 1, 0]\n",
    "        #tensor([[ -1.,  1.,  1.],\n",
    "        #        [  1.,  1.,  1.],\n",
    "        #        [  1.,  1., -1.]])\n",
    "        modif = (1-torch.eye(bs)).to(target.device) + (torch.eye(bs).to(target.device)*(target*2-1)) \n",
    "        input = input*modif/sigma\n",
    "\n",
    "        diag_idx = torch.arange(0,len(input)).to(input.device)\n",
    "\n",
    "        #label_smoothing = hparams.get('label_smoothing', 0.0)\n",
    "        #if label_smoothing is None: #if it's in label_smoothing but still None\n",
    "        label_smoothing = 0.0\n",
    "\n",
    "        mol2txt = F.cross_entropy(input,   diag_idx, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction, label_smoothing=label_smoothing)\n",
    "        txt2mol = F.cross_entropy(input.T, diag_idx, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction, label_smoothing=label_smoothing)\n",
    "        return mol2txt+txt2mol\n",
    "\n",
    "criterion = ConLoss()\n",
    "loss = criterion(input, target)\n",
    "print(f'loss calculated by ConLoss is {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1d2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# =============== top_k_accuracy: pos, neg ==============\n",
    "from \n",
    "activity = torch.randint(0, 2, (10,))\n",
    "print(f'activity is {activity}')\n",
    "ks = [1, 5, 10, 50] \n",
    "\n",
    "# Ground-truth matches = diagonal indices\n",
    "y_true = torch.arange(0, 10)\n",
    "print(f'y_true is {y_true}')\n",
    "\n",
    "# --- positives only ---\n",
    "pos_mask = activity == 1\n",
    "if pos_mask.any(): # Check if there are any True values, i.e., this batch of activities contains at least one '1'.\n",
    "    tkaccs_pos, arocc_pos = top_k_accuracy(y_true[pos_mask],preactivations[pos_mask], k=ks, ret_arocc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161010f",
   "metadata": {},
   "source": [
    "# metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd5633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_data(m, i):\n",
    "    \"\"\"\n",
    "    Get the non-zero data from a sparse matrix by an index.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    m : scipy.sparse.csr_matrix\n",
    "    i: index\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m.indptr[i] to m.indptr[i + 1] data: List[float]\n",
    "        A list of non-zero values in the sparse matrix row at index `i`.\n",
    "    \"\"\"\n",
    "    return [m.data[index] for index in range(m.indptr[i], m.indptr[i + 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48205ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m is\n",
      "  (0, 2)\t3\n",
      "  (1, 0)\t4\n",
      "  (1, 2)\t5\n",
      "  (3, 0)\t6\n",
      "  (3, 1)\t7,\n",
      "m.data is [3 4 5 6 7],\n",
      "m.indices is [2 0 2 0 1],\n",
      "m.indptr is [0 1 3 3 5]\n",
      "\n",
      "range(m.indptr[i], m.indptr[i + 1]) is range(0, 1)\n",
      "\n",
      "m.indptr[0] is 0, m.indptr[1] is 1, m.indptr[2] is 3, m.indptr[3] is 3, m.indptr[4] is 5\n",
      "\n",
      "get_sparse_data(m, 0) is [3]\n",
      "get_sparse_data(m, 1) is [4, 5]\n",
      "get_sparse_data(m, 2) is []\n",
      "get_sparse_data(m, 3) is [6, 7]\n"
     ]
    }
   ],
   "source": [
    "# A dense matrix example\n",
    "A= [\n",
    "    [0, 0, 3],\n",
    "    [4, 0, 5],\n",
    "    [0, 0, 0],\n",
    "    [6, 7, 0]\n",
    "]\n",
    "\n",
    "# convert to CSR: compressed sparse row.\n",
    "m = sparse.csr_matrix(A)\n",
    "\n",
    "print(f'm is\\n{m},\\nm.data is {m.data},\\nm.indices is {m.indices},\\nm.indptr is {m.indptr}\\n') # indptr is the index pointer (the index of data), which indicates where each row starts in the data array.\n",
    "\n",
    "print(f'range(m.indptr[i], m.indptr[i + 1]) is {range(m.indptr[0], m.indptr[0 + 1])}\\n')\n",
    "print(f'm.indptr[0] is {m.indptr[0]}, m.indptr[1] is {m.indptr[1]}, m.indptr[2] is {m.indptr[2]}, m.indptr[3] is {m.indptr[3]}, m.indptr[4] is {m.indptr[4]}\\n')\n",
    "\n",
    "print(f'get_sparse_data(m, 0) is {get_sparse_data(m, 0)}')\n",
    "print(f'get_sparse_data(m, 1) is {get_sparse_data(m, 1)}')\n",
    "print(f'get_sparse_data(m, 2) is {get_sparse_data(m, 2)}')\n",
    "print(f'get_sparse_data(m, 3) is {get_sparse_data(m, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad74e7",
   "metadata": {},
   "source": [
    "## BEDROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee0040c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_proba[:, 1] is\n",
      "[0.8 0.9 0.6 0.7]\n",
      "\n",
      "score is\n",
      "[(0.8, 1), (0.9, 0), (0.6, 1), (0.7, 0)]\n",
      "\n",
      "score sorted by the first element is\n",
      "[(0.9, 0), (0.8, 1), (0.7, 0), (0.6, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = np.array([[0.2, 0.8], [0.1, 0.9], [0.4, 0.6], [0.3, 0.7]])\n",
    "y_true = np.array([1, 0, 1, 0])\n",
    "\n",
    "print(f'y_pred_proba[:, 1] is\\n{y_pred_proba[:, 1]}\\n')\n",
    "\n",
    "score = list(zip(y_pred_proba[:, 1], y_true))\n",
    "print(f'score is\\n{score}\\n')\n",
    "\n",
    "score.sort(key=lambda x: x[0], reverse=True)\n",
    "print (f'score sorted by the first element is\\n{score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8b4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ef calc_bedroc(y_true, y_pred, decreasing=True, alpha=20.0):\n",
    "#   \"\"\"\n",
    "#   Computes the BEDROC (Boltzmann-Enhanced Discrimination of Receiver Operating Characteristic) score.\n",
    "#\n",
    "#   Params:\n",
    "#       y_true (array-like):\n",
    "#           Binary class labels. 1 for positive class, 0 otherwise.\n",
    "#       y_pred (array-like):\n",
    "#           Prediction values.\n",
    "#       decreasing (bool):\n",
    "#           True if high values of `y_pred` correlates to positive class.\n",
    "#       alpha (float):\n",
    "#           Early recognition parameter.\n",
    "#   Returns:\n",
    "#       float:\n",
    "#           Value in interval [0, 1] indicating degree to which the predictive technique employed detects (early) the positive class.\n",
    "#\n",
    "#   \"\"\"\n",
    "#\n",
    "#   assert len(y_true) == len(y_pred), 'The number of scores must be equal to the number of labels.'\n",
    "#\n",
    "#   big_n = len(y_true)\n",
    "#   n = sum(y_true == 1)\n",
    "#\n",
    "#   if decreasing:\n",
    "#       order = np.argsort(-y_pred)\n",
    "#   else:\n",
    "#       order = np.argsort(y_pred)\n",
    "#\n",
    "#   m_rank = (y_true[order] == 1).nonzero()[0] + 1\n",
    "#\n",
    "#   s = np.sum(np.exp(-alpha * m_rank / big_n))\n",
    "#\n",
    "#   r_a = n / big_n\n",
    "#\n",
    "#   rand_sum = r_a * (1 - np.exp(-alpha)) / (np.exp(alpha/big_n) -1)\n",
    "#\n",
    "#   fac = r_a * np.sinh(alpha / 2) / (np.cosh(alpha /2) - np.cosh(alpha/2 -alpha * r_a))\n",
    "#\n",
    "#   cte = 1 / (1 - np.exp(alpha * (1 - r_a)))\n",
    "#\n",
    "#   return s * fac / rand_sum + cte\n",
    "\n",
    "from rdkit.ML.Scoring.Scoring import CalcBEDROC\n",
    "\n",
    "def calc_bedroc_on_clip(y_true, y_score, alpha: float = 20.0):\n",
    "    \"\"\" Calculates the bedroc score unsing rdkit.ML.Scoring.CalcBEDROC.\n",
    "    The source code is available at https://github.com/rdkit/rdkit/blob/master/rdkit/ML/Scoring/Scoring.py#L103\n",
    "    This function is defined as `def CalcBEDROC(score, col, alpha)`, \n",
    "        where `score` is ordered list with tuples of (pred_proba, true value), with pred_proba being descendingly sorted,\n",
    "        'col' is the column index for true values, i.e. 1 for the positive class (1), \n",
    "        and `alpha` is the early recognition parameter.\n",
    "\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    y_true: (lst/array) a list of true values for all compounds.\n",
    "    y_p: (lst/array) a list of predicted probabilities for all compounds, i.e. the value of model.predict_proba(x_test). \n",
    "                   y_pred_proba[:, 1] is the probability of the positive class (1).\n",
    "    alpha: (float)  early recognition parameter. \n",
    "            alpha = 80.5, 2% of the top-ranked compounds of the all compounds were calculated; 2% represents the proportion of active compounds in the DUD-E database;\n",
    "            alpha = 321.5, 0.5% of the top-ranked compounds of the all compounds  were calculated; 4 times smaller than 2% --> early recognition.\n",
    "            alpha = 20.0(default), 8% of the top-ranked compounds of the all compounds were calculated; 4 times larger than 2% --> is interesting for the cases where relatively high-throughput experiments are available.\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    (float) BEDROC score\n",
    "    \"\"\"\n",
    "\n",
    "    pair = list(zip(y_score, y_true)) # pair the predicted scores with the true values\n",
    "    pair.sort(key=lambda x: x[0], reverse=True)\n",
    "    bedroc_score= CalcBEDROC(pair, 1, alpha) # 1 is the column index for the ground-truth values (y_true)\n",
    "\n",
    "    return bedroc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f855848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swipe_threshold_sparse(targets, scores, bedroc_alpha = 20, verbose=True, ret_dict=False):\n",
    "    \"\"\"\n",
    "    This function computes metrics per assay (i.e., column-wise):\n",
    "\n",
    "    Compute ArgMaxJ, AUROC, AVGP, AUPRC and BEDROC (and more if ret_dict=True) metrics for the true binary values\n",
    "    `targets` given the predictions `scores`.\n",
    "\n",
    "    Params\n",
    "    ---------\n",
    "    targets: :class:`scipy.sparse.csc_matrix`, shape(N, M) # N refers to the number of compounds, M refers to the number of assays.\n",
    "        True target values.\n",
    "    scores: :class:`scipy.sparse.csc_matrix`, shape(N, M)\n",
    "        Predicted values\n",
    "    bedroc_alpha: float\n",
    "        Early recognition parameter for BEDROC. Default is 20.0, which is interesting for the cases where relatively high-throughput experiments are available.\n",
    "    verbose: bool\n",
    "        Be verbose if True.\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    tuple of dict\n",
    "        - ArgMaxJ of each valid column keyed by the column index (assay index), # get the optimal threshold that maximizes the difference between true positive rate (TPR) and false positive rate (FPR).\n",
    "        - AUROC of each valid column keyed by the column index (assay index) # AUROC\n",
    "        - AVGP of each valid column keyed by the column index (assay index) # average precision score\n",
    "        - NegAVGP of each valid column keyed by the column index (assay index) # average precision score for the negative class (1 - y_true)\n",
    "        - dAVGP of each valid column keyed by the column index (assay index) # difference between average precision and the mean of y_true\n",
    "        - dNegAVGP of each valid column keyed by the column index (assay index) # difference between average precision for the negative class and the mean of 1 - y_true\n",
    "        - AUPRC of each valid column keyed by the column index (assay index) # area under the precision-recall curve\n",
    "        - BEDROC of each valid column keyed by the column index (assay index) # early recognition.\n",
    "    \"\"\"\n",
    "\n",
    "    assert targets.shape == scores.shape, '\"targets\" and \"scores\" must have the same shape.' # assert <condition>, <error message>\n",
    "    \n",
    "    # find non-empty columns\n",
    "    # (https://mike.place/2015/sparse/ for CSR, but works for CSC, too)\n",
    "    non_empty_idx = np.where(np.diff(targets.indptr) != 0)[0] # Return the compounds that have at least one assay with a non-zero value?\n",
    "\n",
    "    counter_invalid = 0\n",
    "    argmax_j, auroc, avgp, neg_avgp, davgp, dneg_avgp, auprc, bedroc = {}, {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    for col_idx in non_empty_idx: # This function computes metrics per assay (i.e., column-wise):\n",
    "        y_true = np.array(list(get_sparse_data(targets, col_idx)))\n",
    "        if len(pd.unique(y_true)) == 1: # `pd.unique` is faster than `np.unique` and `set`.\n",
    "            counter_invalid += 1\n",
    "            continue\n",
    "        y_score = np.array(list(get_sparse_data(scores, col_idx)))\n",
    "        assert len(y_true) == len(y_score)\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
    "        assert len(fpr) == len(tpr) == len(thresholds), 'Length mismatch between \"fpr\", \"tpr\", and \"thresholds\".'\n",
    "        argmax_j[col_idx] = thresholds[np.argmax(tpr - fpr)] \n",
    "\n",
    "        auroc[col_idx] = metrics.roc_auc_score(y_true, y_score)\n",
    "        avgp[col_idx] = metrics.average_precision_score(y_true, y_score)\n",
    "        neg_avgp[col_idx] = metrics.average_precision_score(1 - y_true, 1 - y_score)\n",
    "        davgp[col_idx] = avgp[col_idx] - y_true.mean()\n",
    "        dneg_avgp[col_idx] = neg_avgp[col_idx] - (1 - y_true.mean())\n",
    "\n",
    "        # check if the auprc is same as avgp.\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_score)\n",
    "        auprc[col_idx] = metrics.auc(recall, precision)\n",
    "        \n",
    "        bedroc[col_idx] = calc_bedroc_on_clip(y_true, y_score, alpha=bedroc_alpha)\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f'Found {len(auroc)} columns with both positive and negative samples.')\n",
    "        logger.info(f'Found and skipped {counter_invalid} columns with only positive or negative samples.')\n",
    "\n",
    "    if ret_dict:\n",
    "        return {'argmax_j':argmax_j, 'auroc':auroc, 'avgp':avgp, 'neg_avgp':neg_avgp,\n",
    "                'davgp':davgp, 'dneg_avgp':dneg_avgp, 'auprc':auprc, 'bedroc':bedroc}\n",
    "\n",
    "    return argmax_j, auroc, avgp, neg_avgp, davgp, dneg_avgp, auprc, bedroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3d42f",
   "metadata": {},
   "source": [
    "# dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238d1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_indices_and_data(m, i):\n",
    "    \"\"\"Get the indices and data of a sparse matrix.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    m: a sparse matrix in CSR format\n",
    "    i: the index of the row for which to extract the non-zero elements. #? non-zero?\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple: (indices, data)\n",
    "        col_indices: the column indices of the non-zero data in row i. (in csr format, only the non-zero elements are stored)\n",
    "        data: the values of the non-zero elements in row i.\n",
    "    \"\"\"\n",
    "    # `m.data`: the non-zero values of the sparse matrix\n",
    "    # `m.indices`: the column indices of the non-zero values\n",
    "    # `m.indptr`: which maps the elements of `data` and `indices` to the rows of the sparse matrix. Explanation: https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr\n",
    "    col_indices = m.indices[m.indptr[i]:m.indptr[i+1]] \n",
    "    data = m.data[m.indptr[i]:m.indptr[i+1]]\n",
    "    return col_indices, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5397a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryClamp(Dataset):\n",
    "    \"\"\"\n",
    "    Subclass of :class:`torch.utils.data.Dataset` holding BioBert activity data, \n",
    "    that is, activity triplets, and compound and assay feature vectors.\n",
    "\n",
    "    :class:`InMemoryClamp` supports two different indexing (and iteration) styles. \n",
    "    The default style is to itreate over `(compound, assay, activity)` COO triplets, however they are sorted.\n",
    "    The \"meta-assays\" style consists in interating over unique compounds using a CSR sparse structure,\n",
    "    and averaging the feature vectors of the positive and negative assays of each compound. \n",
    "\n",
    "    By inheriting from :class:`torch.utils.data.Dataset`, this class must implement at least two methods:\n",
    "    - :meth:`__len__` to return the size of the dataset.\n",
    "    - :meth:`__getitem__` to retrieve a single data point from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: Union[str, Path],\n",
    "            assay_mode: str,\n",
    "            compound_mode: str = None, \n",
    "            train_size: float = 0.6, \n",
    "            aid_max: int = None, #? Yu: could be removed\n",
    "            cid_max: int = None, #? Yu: could be removed\n",
    "            verbose: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the dataset class.\n",
    "\n",
    "        - The data is loaded in memory with the :meth:`_load_dataset` method.\n",
    "        - Splits are created separately along compounds and along assays with the :meth:`_find_splits` method. Compound and assay splits can be interwoven with the :meth:`subset` method.\n",
    "\n",
    "        Params:\n",
    "        root: str or :class:`pathlib.Path`\n",
    "            Path to a directory of ready BioBert files.\n",
    "        assay_mode: str\n",
    "            Type of assay features (\"biobert-last\", \"biobert-two-last\", or \"lsa\").\n",
    "        train_size: float (between 0 and 1)\n",
    "            Fraction of compounds and assays assigned to training data.\n",
    "        verbose: bool\n",
    "            Be verbose if True.\n",
    "        \"\"\"\n",
    "        self.root = Path(root)\n",
    "        self.assay_mode = assay_mode\n",
    "        self.compound_mode = compound_mode\n",
    "        self.train_size = train_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self._load_dataset()\n",
    "        self.find_splits()\n",
    "\n",
    "        self.meta_assays = False\n",
    "        self.assay_onehot = None\n",
    "\n",
    "    def _load_dataset(self) -> None:\n",
    "        \"\"\"\n",
    "        Load prepared dataset from the `root` directory:\n",
    "\n",
    "        - `activity`: Parquet file containing `(compound, assay, activity)` triplets. Compounds and assays are represented by indices, \n",
    "        and thus the file is directly loaded into a :class:`scipy.sparse.coo_matrix` with rows corresponding to compounds and columns corresponding to assays.\n",
    "\n",
    "        - `compound_names`: Parquet file containing the mapping between the compound index used in `activity` and the corresponding compound name.\n",
    "        It is loaded into a :class:`pandas.DataFrame`.\n",
    "\n",
    "        - `assay_names`: Parquet file containing the mapping between the assay index used in `activity` and the corresponding assay name. \n",
    "        It is loaded into a :class:`pandas.DataFrame`.\n",
    "\n",
    "        - `compound_features`: npz file containing the compound features array, where the feature vector for the compound indexed by `idx` is stored in the `idx`-th row. \n",
    "        It is loaded into a :class:`scipy.sparse.csr_matrix`.\n",
    "\n",
    "        - `assay_features`: npy file containing the assay features array, where the feature vector for the assay indexed by `idx` is stored in the `idx`-th row.\n",
    "        It is loaded into a :class:`numpy.ndarray`.\n",
    "\n",
    "        Compute the additional basic dataset attributes `num_compounds`, `num_assays`, `compound_feature_size`, `assay_feature_size`.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            logger.info(f'Load dataset from \"{self.root} with {self.assay_mode}\" assay features.')\n",
    "\n",
    "        #======= Load compound data =======\n",
    "        with open(self.root / 'compound_names.parquet', 'rb') as f:\n",
    "            self.compound_names = pd.read_parquet(f)\n",
    "        self.num_compounds = len(self.compound_names)\n",
    "\n",
    "        compound_modes = self.compound_mode.split('||') if self.compound_mode is not None else 1 #? Yu: replace `||` with `+` ?        if len(compound_modes) >1:\n",
    "        if len(compound_modes) > 1:\n",
    "            logger.info('Multiple compound modes are concatenated ')\n",
    "            self.compound_features = np.concatenate([self._load_compound(cm) for cm in compound_modes], axis=1)\n",
    "        else:\n",
    "            self.compound_features = self._load_compound(self.compound_mode)\n",
    "        # compound_feature_size\n",
    "        if 'graph' in self.compound_mode and (not 'graphormer' in self.compound_mode):\n",
    "            self.compound_features_size = self.compound_features[0].ndata['h'].shape[1] # in_edge_feats. #? Yu\n",
    "        elif isinstance(self.compound_features, pd.DataFrame):\n",
    "            self.compound_features_size = 40000 #? Yu\n",
    "        else:\n",
    "            if len(self.compound_features.shape)>1:\n",
    "                self.compound_features_size = self.compound_features.shape[1]\n",
    "            else:\n",
    "                self.compound_features_size = 1\n",
    "\n",
    "        #======== Load assay data ========\n",
    "        with open(self.root / 'assay_info.parquet', 'rb') as f:\n",
    "            self.assay_names = pd.read_parquet(f)\n",
    "        self.num_assays = len(self.assay_names)\n",
    "\n",
    "        assay_modes = self.assay_mode.split('||')\n",
    "        if len(assay_modes)>1:\n",
    "            logger.info('Multiple assay modes are concatenated')\n",
    "            self.assay_features = np.concatenate([self._load_assay(am) for am in assay_modes], axis=1)\n",
    "        else:\n",
    "            self.assay_features = self._load_assay(self.assay_mode)\n",
    "\n",
    "        # assay_feature_size\n",
    "        if (self.assay_features is None):\n",
    "            self.assay_features_size = 512 #wild guess also 512#? Yu\n",
    "        elif len(self.assay_features.shape)==1:\n",
    "            # its only a list, so probably text\n",
    "            self.assay_features_size = 768 #? Yu\n",
    "        else:\n",
    "            self.assay_features_size = self.assay_features.shape[1]\n",
    "        \n",
    "        #======= Load activity data =======\n",
    "        with open(self.root / 'activity.parquet', 'rb') as f:\n",
    "            activity_df = pd.read_parquet(f)\n",
    "            self.activity_df = activity_df\n",
    "        \n",
    "        # ? Yu: will the :meth:`sparse.coo_matrix` only keep the non-zero values? If so, only the active compounds (where activity  is not 0) will be kept?\n",
    "        self.activity = sparse.coo_matrix(\n",
    "            (\n",
    "                activity_df['activity'],# activity is the value\n",
    "                (activity_df['compound_idx'], activity_df['assay_idx']) # compound in row, assay in column.\n",
    "            ),\n",
    "            shape=(self.num_compounds, self.num_assays),\n",
    "        )\n",
    "    \n",
    "    def _load_compound(self, compound_mode=None):\n",
    "        cmpfn = f'compound_features{\"_\"+compound_mode if compound_mode else \"\"}'\n",
    "        #?Yu: if 'graph' is not used, remove the below code\n",
    "        if 'graph' in compound_mode and (not 'graphormer' in compound_mode):\n",
    "            logger.info(f'graph in compound mode: loading '+cmpfn)\n",
    "            import dgl\n",
    "            from dgl.data.utils import load_graphs\n",
    "            compound_features = load_graphs(str(self.root/(cmpfn+\".bin\")))[0]\n",
    "            compound_features = np.array(compound_features)\n",
    "        elif compound_mode == 'smiles':\n",
    "            compound_features = pd.read_parquet(self.root/('compound_smiles.parquet'))['CanonicalSMILES'].values\n",
    "        else:\n",
    "            try: #tries to open npz files else npy\n",
    "                with open(self.root/(cmpfn+\".npz\"), 'rb') as f:\n",
    "                    compound_features = sparse.load_npz(f)\n",
    "            except:\n",
    "                logger.info(f'loading '+cmpfn+'.npz failed, using .npy instead')\n",
    "                try:\n",
    "                    compound_features = np.load(self.root/(cmpfn+\".npy\"))\n",
    "                except:\n",
    "                    logger.info(f'loading '+cmpfn+'.npy failed, trying to compute it on the fly')\n",
    "                    compound_features = pd.read_parquet(self.root/('compound_smiles.parquet'))\n",
    "        return compound_features\n",
    "    \n",
    "    def _load_assay(self, assay_mode='lsa') -> None: #? Yu: 'lsa'\n",
    "        \"\"\" loads assay \"\"\"\n",
    "        if assay_mode =='':\n",
    "            print('no assay features')\n",
    "            return None\n",
    "        \n",
    "        #? Yu: if the below assay modes are not used, remove them.\n",
    "        if assay_mode == 'biobert-last':\n",
    "            with open(self.root/('assay_features_dmis-lab_biobert-large-cased-v1.1_last_layer.npy'), 'rb') as f:\n",
    "                return np.load(f, allow_pickle=True)\n",
    "        elif assay_mode == 'biobert-two-last':\n",
    "            with open(self.root/('assay_features_dmis-lab_biobert-large-cased-v1.1_penultimate_and_last_layer.npy'), 'rb') as f:\n",
    "                return  np.load(f, allow_pickle=True)\n",
    "        \n",
    "        # load the prepared assay features\n",
    "        try: # tries to open npz file else npy\n",
    "            with open(self.root/(f'assay_features_{assay_mode}.npz'), 'rb') as f:\n",
    "                return sparse.load_npz(f)\n",
    "        except:\n",
    "            with open(self.root/(f'assay_features_{assay_mode}.npy'), 'rb') as f:\n",
    "                return np.load(f, allow_pickle=True)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _find_splits(self) -> None:\n",
    "        \"\"\"\n",
    "        We assume that during the preparation of the PubChem data, compounds(assays) have been indexed \n",
    "        so that a larger compound(assay) index corresponds to a compound(assay) incorporated to PubChem later in time.\n",
    "        This function finds the compound(assay) index cut-points to create three chronological disjoint splits.\n",
    "\n",
    "        The oldest `train_size` fraction of compounds(assays) are assigned to training. \n",
    "        From the remaining compounds(assays), the oldest half are assigned to vailidation, and the newest half are assigned to test.\n",
    "        Only the index cut points are stored.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            logger.info(f'Find split cut-points for compound and assay indices (train_size={self.train_size}).')\n",
    "\n",
    "        first_cut, second_cut = self._chunk(self.num_compounds, self.train_size)\n",
    "        self.compound_cut = {'train': first_cut, 'valid': second_cut}\n",
    "\n",
    "        first_cut, second_cut = self._chunk(self.num_assays, self.train_size)\n",
    "        self.assay_cut = {'train': first_cut, 'valid': second_cut}\n",
    "\n",
    "    def _chunk(n:int, first_cut_ratio:float) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Find the two cut points required to chunk a sequence of `n` items into three parts, \n",
    "        the first having `first_cut_ratio` of the items, \n",
    "        the second and the third having approximately the half of the remaining items.\n",
    "\n",
    "        Params\n",
    "        -------\n",
    "        n: int\n",
    "            Length of the sequence to chunk.\n",
    "        first_cut_ratio: float\n",
    "            Portion of items in the first chunk. This is the `train_size`\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int, int\n",
    "            Positions where the first and second cut occurs.\n",
    "        \"\"\"\n",
    "        first_cut = int(round(first_cut_ratio * n))\n",
    "        second_cut = first_cut + int(round((n - first_cut) / 2))\n",
    "\n",
    "        return first_cut, second_cut\n",
    "\n",
    "    def subset(\n",
    "            self, \n",
    "            c_low: Optional[int] = None,\n",
    "            c_high: Optional[int] = None,\n",
    "            a_low: Optional[int] = None,\n",
    "            a_high: Optional[int] = None,\n",
    "    ) -> np.ndarray:\n",
    "        if c_low is None: # sef the compound low index to 0\n",
    "            c_low = 0\n",
    "        if c_high is None: # set the compound high index to the number of compounds\n",
    "            c_high = self.num_compounds\n",
    "        if a_low is None: # set the assay low index to 0\n",
    "            a_low = 0\n",
    "        if a_high is None: # set the assay high index to the number of assays\n",
    "            a_high = self.num_assays\n",
    "\n",
    "        if self.verbose:\n",
    "            logger.info(f'Find activity triplets where {c_low} <= compound_idx <= {c_high} and {a_low} <= assay_idx <= {a_high}.')\n",
    "        \n",
    "        activity_bool = np.logical_and.reduce( # take multiple Boolean conditions and combines them using logical AND across all conditions.\n",
    "            (\n",
    "                self.activity.row >= c_low,\n",
    "                self.activity.row < c_high,\n",
    "                self.activity.col >= a_low,\n",
    "                self.activity.col < a_high\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return np.flatnonzero(activity_bool) # applies the logical condition to the COO matrix and returns the indices that satisfy the condition.\n",
    "\n",
    "    def get_unique_names(\n",
    "            self, \n",
    "            activity_idx: Union[int, Iterable[int], slice]\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the unique compound and assay names within the `activity` triplets  indexed by `activity_idx` in default, COO style.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        activity_idx: int, iterable of int, slice\n",
    "            Index to one or multiple `activity` triplets.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        compound_names: :class:`pandas.DataFrame`\n",
    "        assay_names: :class:`pandas.DataFrame`\n",
    "        \"\"\"\n",
    "\n",
    "        compound_idx = self.activity.row[activity_idx]\n",
    "        assay_idx = self.activity.col[activity_idx]\n",
    "\n",
    "        if isinstance(compound_idx, np.ndarray) and isinstance(assay_idx, np.ndarray):\n",
    "            compound_idx = pd.unique(compound_idx)\n",
    "            assay_idx = pd.unique(assay_idx)\n",
    "        \n",
    "        elif isinstance(compound_idx, (int, np.integer)) and isinstance(assay_idx, (int, np.integer)):\n",
    "            pass # a single index means a single compound and assay, so no need to do anything.\n",
    "\n",
    "        else:\n",
    "            raise ValueError('activity_idx must be an int, iterable of int, or slice.')\n",
    "\n",
    "        compound_names = self.compound_names.iloc[compound_idx]\n",
    "        assay_names = self.assay_names.iloc[assay_idx]\n",
    "\n",
    "        return compound_names.sort_index(), assay_names.sort_index() # sort the names alphabetically\n",
    "\n",
    "    def getitem(\n",
    "            self,\n",
    "            activity_idx: Union[int, Iterable[int], slice],\n",
    "            ret_np=False\n",
    "    ) -> Tuple[Any, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        activity_idx: int, iterable of int, slice\n",
    "            Specifies the indices of the activity triplets to retrieve.\n",
    "        ret_np: bool\n",
    "            Determines the format of the returned data. If True, returns numpy arrays; If False, returns PyTorch tensors.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of :class:`torch.Tensor`\n",
    "        - `activity_idx`: the original indices provided as input. This will enable to reconstruct the order in which the dataset has been visited.\n",
    "        - `compound_features`: shape(len(activity_idx), compound_feature_size)\n",
    "        - `assay_features`: shape(len(activity_idx), assay_feature_size)\n",
    "        - `activity`: shape(len(activity_idx), ).\n",
    "        \"\"\"\n",
    "        compound_idx = self.activity.row[activity_idx]\n",
    "        assay_idx = self.activity.col[activity_idx]\n",
    "        activity = self.activity.data[activity_idx]\n",
    "\n",
    "        # ===== get compound_features =====\n",
    "        if isinstance(self.compound_features, pd.DataFrame):\n",
    "            compound_smiles = self.compound_features.iloc[compound_idx]['CanonicalSMILES'].values\n",
    "            from datacat4ml.Scripts.data_prep.data_featurize.compound_featurize.encode_compound  import convert_smiles_to_fp\n",
    "            if self.compound_mode == 'MxFP':\n",
    "                fptype = 'maccs+morganc+topologicaltorsion+erg+atompair+pattern+rdkc+mhfp+rdkd'\n",
    "            else:\n",
    "                fptype = self.compound_mode\n",
    "            # Todo: fp_size as input parameter\n",
    "            fp_size = 40000 #? Yu\n",
    "            compound_features = convert_smiles_to_fp(compound_smiles, fp_size=fp_size, which=fptype, radius=2, njobs=1).astype(np.float32)\n",
    "        else:\n",
    "            compound_features = self.compound_features[compound_idx]\n",
    "            if isinstance(compound_features, sparse.csr_matrix):\n",
    "                compound_features = compound_features.toarray()\n",
    "        \n",
    "\n",
    "        # ===== get assay_features =====\n",
    "        assay_features = self.assay_features[assay_idx]\n",
    "        if isinstance(assay_features, sparse.csr_matrix):\n",
    "            assay_features = assay_features.toarray()\n",
    "        \n",
    "        #? Yu: if not used, remove the below code\n",
    "        try:\n",
    "            assay_onehot = self.assay_onehot[assay_idx].toarray()\n",
    "        except (TypeError, ValueError):\n",
    "            assay_onehot = np.zeros_like(assay_features)\n",
    "        \n",
    "        # ===== Handle single indices =====\n",
    "        # If `activity_idx`is a single integer or a list with only one element, the retrieved feature vectors are reshaped into 1D arrays to maintain the consistency of the output format.\n",
    "        if isinstance(activity_idx, (int, np.integer)):\n",
    "            compound_features = compound_features.reshape(-1) \n",
    "            assay_features = assay_features.reshape(-1) \n",
    "            assay_onehot = assay_onehot.reshape(-1) \n",
    "            activity = [activity]\n",
    "        elif isinstance(activity_idx, list):\n",
    "            if len(activity_idx) == 1:\n",
    "                compound_features = compound_features.reshape(-1)\n",
    "                assay_features = assay_features.reshape(-1)\n",
    "                assay_onehot = assay_onehot.reshape(-1)\n",
    "        activity = np.array(activity)\n",
    "\n",
    "        # ===== Return =====\n",
    "        # return the data as Numpy arrays.\n",
    "        if ret_np:\n",
    "            return(\n",
    "                activity_idx,\n",
    "                compound_features, #already float32\n",
    "                assay_features if not isinstance(assay_features[0], str) else assay_features, # already float32\n",
    "                assay_onehot if not isinstance(assay_onehot[0], str) else assay_onehot, # already float32\n",
    "                (float(activity)) # torch.nn.BCEWithLogitsLoss needs this to be float too...\n",
    "            )\n",
    "\n",
    "        # return the data as PyTorch tensors.\n",
    "        if self.compound_mode == 'smiles':\n",
    "            comp_feat = compound_features\n",
    "        elif isinstance(compound_features, np.ndarray):\n",
    "            comp_feat = torch.from_numpy(compound_features)\n",
    "        elif not isinstance(compound_features[0], dgl.DGLGraph):\n",
    "            comp_feat = dgl.batch(compound_features)\n",
    "        else:\n",
    "            comp_feat = compound_features\n",
    "\n",
    "        return  (\n",
    "            activity_idx, \n",
    "            comp_feat, # alread float32\n",
    "            torch.from_numpy(assay_features) if not isinstance(assay_features[0], str) else assay_features, # already float32\n",
    "            torch.from_numpy(assay_onehot.astype(int)) if not isinstance(assay_onehot[0], str) else assay_onehot, # already float32\n",
    "            torch.from_numpy(activity).float() # torch.nn.BCEWithLogitsLoss needs this to be float too...\n",
    "        )\n",
    "\n",
    "    def getitem_meta_assay(\n",
    "            self,\n",
    "            compound_idx: Union[int, List[int], slice]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        For a given compound (or list), retrieve the data in the `meta-assay` style, \n",
    "        which involves summarizing assay feature vectors (positive and negative) for each compound.\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        compound_idx: int, iterable of int, slice\n",
    "            Index to one or multiple compounds.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of :class:`torch.Tensor`\n",
    "        - `compound_features`: shape(N, compound_feature_size)\n",
    "        - `assay_features`: shape(N, assay_feature_size)\n",
    "        - `activity`: shape(N, )\n",
    "        \"\"\"\n",
    "        \n",
    "        # extract the data for the specified compounds\n",
    "        activity_slice = self.activity.tocsr()[compound_idx] \n",
    "\n",
    "        # find non-empty rows\n",
    "        # `activity_slic.indptr`: pointer to the start of each row in the sparse matrix.\n",
    "        # `np.diff(activity_slice.indptr)`: measures the number of elements in each row.\n",
    "        # `np.where(...!=0)`: finds rows that contain non-zero elements. (i.e., rows with at least one assay-related to the compound)`\n",
    "        non_empty_row_idx = np.where(np.diff(activity_slice.indptr)!=0)[0] #?\n",
    "\n",
    "        # initialize containers for results\n",
    "        compound_features_l = [] # list of compound features\n",
    "        assay_positive_features_l, assay_negative_features_l = [], [] # averaged features of positive assays, and negative assays\n",
    "        activity_l = [] # activity lables\n",
    "\n",
    "        # process each non-empty row\n",
    "        for row_idx in non_empty_row_idx:\n",
    "            positive_l, negative_l  = [], []\n",
    "            for col_idx, activity in get_sparse_indices_and_data(activity_slice, row_idx):\n",
    "                if activity == 0:\n",
    "                    negative_l.append(self.assay_features[col_idx]) \n",
    "                else:\n",
    "                    positive_l.append(self.assay_features[col_idx])\n",
    "            \n",
    "            if len(negative_l) > 0:\n",
    "                compound_features_l.append(self.compound_features[row_idx])\n",
    "                negative = np.vstack(negative_l).mean(axis=0)\n",
    "                assay_negative_features_l.append(negative)\n",
    "                activity_l.append(0)\n",
    "            \n",
    "            if len(positive_l) > 0:\n",
    "                compound_features_l.append(self.compound_features[row_idx])\n",
    "                positive = np.vstack(positive_l).mean(axis=0)\n",
    "                assay_positive_features_l.append(positive)\n",
    "                activity_l.append(1)\n",
    "\n",
    "        compound_features = sparse.vstack(compound_features_l).toarray()\n",
    "        assay_features_l = np.vstack(\n",
    "            assay_negative_features_l + assay_positive_features_l # '+' is used to concatenate the two lists\n",
    "        )\n",
    "\n",
    "        activity = np.array(activity_l)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(compound_features), # already float32\n",
    "            torch.from_numpy(assay_features_l), # already float32\n",
    "            torch.from_numpy(activity).float() # torch.nn.BCEWithLogitsLoss needs this to be float too...\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch_as_list:list) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Necessary for :meth:`getitem_meta_assay` if using a :class:`torch.utils.data.DataLoader`.\n",
    "        Not necessaryif using :class:`torch.utils.data.BatchSampler`, as I typically do.\n",
    "\n",
    "        Params\n",
    "        -------\n",
    "        batch_as_list: list\n",
    "            Result of :meth:`getitem_meta_assay` for a mini-batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of :class:`torch.Tensor`\n",
    "            Data for a mini-batch.\n",
    "        \"\"\"\n",
    "        compound_features_t, assay_features_t, activty_t = zip(*batch_as_list)\n",
    "        return(\n",
    "            torch.cat(compound_features_t, dim=0),\n",
    "            torch.cat(assay_features_t, dim=0),\n",
    "            torch.cat(activty_t, dim=0)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx: Union[int, Iterable[int], slice]) -> Tuple:\n",
    "        \"\"\"\n",
    "        Index or slice `activity` by `idx`. The indexing mode depends on the value of `meta_assays`. \n",
    "        If False(default), the indexing is over COO triplets.\n",
    "        If True, the indexing is over unique compounds.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.meta_assays:\n",
    "            return self.getitem_meta_assay(idx)\n",
    "        else:\n",
    "            return self.getitem(idx)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "\n",
    "        - If `meta_assays` is False (default), length is defined as the number of `(compound, assay, activity)` COO triplets.\n",
    "        - If `meta_assays` is True, length is defined as the number of the unique compounds.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.meta_assays:\n",
    "            return self.num_compounds\n",
    "        else:\n",
    "            return self.activity.nnz # the number of non-zero elements in the sparse matrix\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'InMemoryClamp\\n' \\\n",
    "               f'\\troot=\"{self.root}\"\\n' \\\n",
    "               f'\\tassay_mode=\"{self.assay_mode}\"\\n' \\\n",
    "               f'\\ttrain_size={self.train_size}\\n' \\\n",
    "               f'\\tactivity.shape={self.activity.shape}\\n' \\\n",
    "               f'\\tactivity.nnz={self.activity.nnz}\\n' \\\n",
    "               f'\\tmeta_assays={self.meta_assays}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5e2be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_bool: \n",
      "[False False False False False]\n",
      "\n",
      "np.flatnonzero(activity_bool): \n",
      "[]\n",
      "\n",
      "self_activity.data: \n",
      "[1 0 1 0 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# About funciton `subset`\n",
    "c_low = 1\n",
    "c_high = 4\n",
    "a_low = 2\n",
    "a_high = 5\n",
    "\n",
    "activity_df = pd.DataFrame({\n",
    "    \"compound_idx\": [0, 1, 2, 3, 4],\n",
    "    \"assay_idx\": [0, 0, 0, 0, 0],\n",
    "    \"activity\": [1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "self_activity = sparse.coo_matrix(\n",
    "    (\n",
    "        activity_df['activity'], \n",
    "        (activity_df['compound_idx'], activity_df['assay_idx'])\n",
    "    ),\n",
    "    shape=(5, 5),)\n",
    "\n",
    "activity_bool = np.logical_and.reduce( # take multiple Boolean conditions and combines them using logical AND across all conditions.\n",
    "            (\n",
    "                self_activity.row >= c_low,\n",
    "                self_activity.row < c_high,\n",
    "                self_activity.col >= a_low,\n",
    "                self_activity.col < a_high\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f'activity_bool: \\n{activity_bool}\\n')\n",
    "print(f'np.flatnonzero(activity_bool): \\n{np.flatnonzero(activity_bool)}\\n')\n",
    "\n",
    "print(f'self_activity.data: \\n{self_activity.data}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c70596",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ba7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hidden_layers(s:str):\n",
    "    \"\"\"\n",
    "    #? Yu: why this function necessary?\n",
    "    Parse a string in the form of [32, 32] into a list of integers.\"\"\"\n",
    "    try:\n",
    "        res = [int(ls) for ls in s.strip('[]').split(',')]\n",
    "    except:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            f\"Invalid hidden layers format: {s}. Expected format is [32, 32].\"\n",
    "        )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b1cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME2FORMATTER = {\n",
    "    'assay_mode': str,\n",
    "    'model': str,\n",
    "    'multitask_temperature': float, #?Yu: if not used later, remove\n",
    "    'optimizer': str,\n",
    "    'hidden_layers': parse_hidden_layers, #?Yu: number of hidden layers?\n",
    "    'compound_layer_sizes': parse_hidden_layers, #?Yu:?\n",
    "    'assay_layer_sizes': parse_hidden_layers, #?Yu:?\n",
    "    'embedding_size': int, \n",
    "    'lr_ini': float,\n",
    "    'epoch_max': int, \n",
    "    'batch_size': int,\n",
    "    'dropout_input': float,\n",
    "    'dropout_hidden': float,\n",
    "    'l2': float, #?Yu: L2 regularization?\n",
    "    'nonlinearity': str,\n",
    "    'pooling_mode': str,\n",
    "    'lr_factor': float,\n",
    "    'patience': int,\n",
    "    'attempts': int, # not used in public version #?Yu: don't understand, remove?\n",
    "    'loss_fun': str, \n",
    "    'tokenizer': str,\n",
    "    'transformer': str,\n",
    "    'warmup_epochs': int,\n",
    "    'train_balanced': int,\n",
    "    'beta': float,\n",
    "    'norm': bool,\n",
    "    'label_smoothing': float,\n",
    "    'gpu': int,\n",
    "    'checkpoint': str,\n",
    "    'verbose': bool,\n",
    "    'hyperparams': str,\n",
    "    'format': str,\n",
    "    'f': str, #?Yu: file path?\n",
    "    'support_set_size': int,\n",
    "    'train_only_actives': bool,\n",
    "    'random': int, \n",
    "    'seed': int, \n",
    "    'dataset': str,\n",
    "    'experiment': float,\n",
    "    'split': str,\n",
    "    'wandb': str,\n",
    "    'compound_mode': str,\n",
    "    'train_subsample': float, #?Yu:?\n",
    "}\n",
    "\n",
    "EVERY = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f052dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams(path, mode='logs', verbose=False):\n",
    "    \"\"\"\n",
    "    Get hyperparameters from a path. If logs uses path /params/* files form mlflow.\n",
    "    If mode is json: loads in the file provided in path. \n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    path: str\n",
    "        Path to the hyperparameters file.\n",
    "    mode: str\n",
    "        Mode of the hyperparameters file. Default is 'logs'.\n",
    "    verbose: bool\n",
    "        Be verbose if True.\n",
    "    \"\"\"\n",
    "    if isinstance(path, str):\n",
    "        path = Path(path)\n",
    "    hparams = {}\n",
    "    if mode =='logs':\n",
    "        for fn in os.listdir(path/'params'):\n",
    "            try:\n",
    "                with open(path/f'params/{fn}') as f:\n",
    "                    lines = f.readlines()\n",
    "                    try:\n",
    "                        hparams[fn] = NAME2FORMATTER.get(fn, str)(lines[0])\n",
    "                    except:\n",
    "                        hparams[fn] = None if len(lines)==0 else lines[0]\n",
    "            except:\n",
    "                pass\n",
    "    elif mode == 'json':\n",
    "        with open(path) as f:\n",
    "            hparams = json.load(f)\n",
    "    if verbose:\n",
    "        logger.info(\"loaded hparams:\\n\", hparams)\n",
    "    \n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b25b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=70135):\n",
    "    \"\"\" does what it says ;) - from https://gist.github.com/KirillVladimirov/005ec7f762293d2321385580d3dbe335\"\"\"\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import os\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device(gpu=0, verbose=False): #?Yu: verbose is not used\n",
    "    \"Set device to gpu or cpu.\"\n",
    "    if gpu == 'any':\n",
    "        gpu = 0\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f'cuda:{gpu}')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2450bb",
   "metadata": {},
   "source": [
    "**def train_and_test()**\n",
    "- Function signature and parameters \n",
    "- Model initialization \n",
    "- Optimizer and loss function initialization \n",
    "- Learning rate Scheduler \n",
    "- Batch sampler \n",
    "- Training loop \n",
    "- Validation loop \n",
    "- Testing loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6729989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_checkpoint(path, device, verbose=False):\n",
    "    \"\"\"\n",
    "    load from path if path is not None, otherwise return empty dict.\n",
    "    \"\"\"\n",
    "    if path is not None:\n",
    "        if verbose:\n",
    "            logger.info('Load checkpoint.')\n",
    "        return torch.load(path, map_location=device)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724224b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_paths(run_info: mlflow.entities.RunInfo):\n",
    "    \"\"\"\n",
    "    Return paths to the artifacts directory and the model weights.\n",
    "    \"\"\"\n",
    "    artifacts_dir = Path('mlruns', run_info.experiment_id, run_info.run_id, 'artifacts')\n",
    "    checkpoint_file_path = artifacts_dir / 'checkpoint.pt'\n",
    "    metrics_file_path = artifacts_dir / 'metrics.parquet'\n",
    "    return artifacts_dir, checkpoint_file_path, metrics_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a90f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # adapted from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "    # Early stopping is a regularization technique to prevent overfitting. \n",
    "    # During training, it monitors the validation loss and stops training when the validation loss does not improve for a specified number of epochs (patience).\n",
    "    # This helps the model to generalize better rather than just memorizing the training data.\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience # number of epochs with no improvement after which training will be stopped\n",
    "        self.min_delta = min_delta # minimum change to consider it an improvement\n",
    "        self.counter = 0 # counter for the number of epochs with no improvement\n",
    "        self.min_validation_loss = np.inf # the best (lowest) validation loss seen so far\n",
    "        self.improved = False # flag to indicate if the last validation loss has improved\n",
    "\n",
    "    def __call__(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            self.improved = True\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            self.improved = False\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0340c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model( #?Yu: understand and finalize this function.\n",
    "        compound_feature_size: int,\n",
    "        assay_feature_size: int,\n",
    "        hp: dict,\n",
    "        verbose: bool = False\n",
    ") -> DotProduct:\n",
    "    \"\"\"\n",
    "    Initialize PyTorch model.\n",
    "\n",
    "    Params\n",
    "    -------\n",
    "    compound_feature_size: int\n",
    "        Input size of the compound encoder.\n",
    "    assay_feature_size: int\n",
    "        Input size of the assay encoder.\n",
    "    hp: dict\n",
    "        Hyperparameters.\n",
    "    verbose: bool\n",
    "        Be verbose if True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`DotProduct``\n",
    "        Model instance.\n",
    "    \"\"\"\n",
    "    #Yu: `model_str` is not used later, so it could be removed.\n",
    "    #model_str = hp['model'] # TAn example of `model_str` is 'MLPLayerNorm'. \n",
    "    #modes = ['Multitask', 'Scaled', 'GNN', 'Pretrained'] #?Yu: is it necessary to have these modes?\n",
    "    #selected_mode = ''\n",
    "    #for mode in modes:\n",
    "    #    if mode in model_str:\n",
    "    #        model_str = model_str.replace(mode, '')\n",
    "    #        selected_mode = mode\n",
    "#\n",
    "    #logger.info(selected_mode+' has been selected.')\n",
    "\n",
    "    #?Yu: if not used, remove the below code\n",
    "    # if not (hasattr(models, model_str) or hasattr(gnn, 'GNN'+model_str)):\n",
    "    #    raise NotImplementedError(f'Model \"{hp[\"model\"]}\" is not known.')\n",
    "\n",
    "    # ['Linear', 'MLPLayerNorm', 'ScaledMLPLayerNorm', 'MultitaskMLPLayerNorm']\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f'Initialize \"{hp[\"model\"]}\" model.')\n",
    "\n",
    "    init_dict = hp.copy()\n",
    "    init_dict.pop('embedding_size') #?Yu remove the embedding size, since it has to be provided as positional argument.\n",
    "\n",
    "    # For getattr(clamp.models, hp['model']) to work, the class must be exposed at the package level in /clamp/models/__init__.py. \n",
    "    # Typically, __init__.py will import selected classes from those submodules, making them accessible like clamp.models.MyModelClass.\n",
    "    model = getattr(model_def, hp['model'])( #?Yu model_def: class, hp['model']: the specified attribute. \n",
    "        compound_features_size=compound_feature_size,\n",
    "        assay_features_size=assay_feature_size,\n",
    "        embedding_size=hp['embedding_size'],\n",
    "        **init_dict\n",
    "    )\n",
    "\n",
    "    if wandb.run:\n",
    "        wandb.watch(model, log_freq=100, log_graph=(True))  # Log model weights and gradients for visualization in wandb, generate and log the computational graph automatically.\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fb0122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp: {'model': 'MLPLayerNorm', 'hidden_layers': [2048, 1024], 'embedding_size': 512, 'lr_ini': 1e-05, 'epoch_max': 50, 'batch_size': 256, 'dropout_input': 0.1, 'dropout_hidden': 0.3, 'l2': 0.0005, 'lr_factor': 1, 'patience': 3, 'attempts': 1, 'optimizer': 'AdamW', 'loss_fun': 'BCE', 'warmup_epochs': 2, 'train_balanced': 0, 'train_subsample': 0, 'beta': 1, 'bedroc_alpha': 20.0}\n",
      "init_dict: {'model': 'MLPLayerNorm', 'hidden_layers': [2048, 1024], 'lr_ini': 1e-05, 'epoch_max': 50, 'batch_size': 256, 'dropout_input': 0.1, 'dropout_hidden': 0.3, 'l2': 0.0005, 'lr_factor': 1, 'patience': 3, 'attempts': 1, 'optimizer': 'AdamW', 'loss_fun': 'BCE', 'warmup_epochs': 2, 'train_balanced': 0, 'train_subsample': 0, 'beta': 1, 'bedroc_alpha': 20.0}\n"
     ]
    }
   ],
   "source": [
    "#Yu: Comprehend the below code.\n",
    "path = os.path.join(DATA_DIR, 'model_dev', 'hparams', 'default.json')\n",
    "hp = get_hparams(path, mode='json', verbose=False)\n",
    "init_dict = hp.copy()\n",
    "init_dict.pop('embedding_size')\n",
    "\n",
    "print(f'hp: {hp}')\n",
    "print(f'init_dict: {init_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55312178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(dict_to_filter, thing_with_kwargs):\n",
    "    \"\"\"\n",
    "    Yu: integrate this function into other functions, so that it can be removed.\n",
    "    filter dict_to_filter by the arguments of the object or function of thing_with_kwargs.\n",
    "    so that you can stressfree do this: thing_with_kwargs(**filter_dict_return)\n",
    "    returns: filtered_dict\n",
    "    modified from https://stackoverflow.com/questions/26515595/how-does-one-ignore-unexpected-keyword-arguments-passed-to-a-function\n",
    "    \"\"\"\n",
    "    import inspect # a python standard library\n",
    "    sig = inspect.signature(thing_with_kwargs) # get the list of valid argument names for the function or class `thing_with_kwargs`.\n",
    "    filter_keys =[param.name for param in sig.parameters.values() if param.kind == param.POSITIONAL_OR_KEYWORD] # only `POSITIONAL_OR_KEYWORD` parameters are considered.\n",
    "    inters = set(dict_to_filter.keys()).intersection(filter_keys) # do filter\n",
    "\n",
    "    return {k:dict_to_filter[k] for k in inters}\n",
    "\n",
    "\n",
    "def init_optimizer(model, hp, verbose=False):\n",
    "    \"\"\"\n",
    "    Initialize optimizer for the model.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        logger.info(f\"Trying to initialize '{hp['optimizer']}' optimizer from torch.optim.\")\n",
    "    hp['lr'] = hp.pop('lr_ini') # remove 'lr_ini' and rename it to 'lr'\n",
    "    hp['weight_decay'] = hp.pop('l2') # remove 'l2' and rename it to 'weight_decay'\n",
    "    optimizer = getattr(torch.optim, hp['optimizer']) # fetch the optimizer class (e.g., `Adam`)\n",
    "    filtered_dict = filter_dict(hp, optimizer) # remove any keys in `hp` that are not valid arguments for the selected optimizer class.\n",
    "    \n",
    "    return optimizer(params=model.parameters(), **filtered_dict) # optimize all the model's parameters by using the filtered hyperparameters of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb2b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig: (params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: Union[float, torch.Tensor] = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.01, amsgrad: bool = False, *, maximize: bool = False, foreach: Union[bool, NoneType] = None, capturable: bool = False, differentiable: bool = False, fused: Union[bool, NoneType] = None)\n",
      "filter_keys: ['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad']\n",
      "inters: {'lr_factor', 'patience', 'bedroc_alpha', 'epoch_max', 'lr', 'optimizer', 'attempts', 'loss_fun', 'weight_decay', 'warmup_epochs', 'train_balanced', 'batch_size', 'train_subsample', 'embedding_size', 'beta', 'model', 'hidden_layers', 'dropout_hidden', 'dropout_input'}\n",
      "inters_intersection: {'weight_decay', 'lr'}\n",
      "return_by_filter_dict: {'weight_decay': 0.0005, 'lr': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "# Yu: comprehend the code below.\n",
    "path = os.path.join(DATA_DIR, 'model_dev', 'hparams', 'default.json')\n",
    "hp = get_hparams(path, mode='json', verbose=False)\n",
    "\n",
    "# arguments for `filter_dict`\n",
    "dict_to_filter = hp.copy()\n",
    "dict_to_filter['lr'] = dict_to_filter.pop('lr_ini')\n",
    "dict_to_filter['weight_decay'] = dict_to_filter.pop('l2')\n",
    "thing_with_kwargs = getattr(torch.optim, hp['optimizer'])\n",
    "\n",
    "# code in `filter_dict``\n",
    "import inspect\n",
    "sig = inspect.signature(thing_with_kwargs)\n",
    "print(f'sig: {sig}')\n",
    "\n",
    "filter_keys = [p.name for p in sig.parameters.values() if p.kind == p.POSITIONAL_OR_KEYWORD]\n",
    "print(f'filter_keys: {filter_keys}')\n",
    "\n",
    "inters = set(dict_to_filter.keys())\n",
    "print(f'inters: {inters}')\n",
    "\n",
    "inters_intersection = inters.intersection(filter_keys)\n",
    "print(f'inters_intersection: {inters_intersection}')\n",
    "\n",
    "return_by_filter_dict = {k:dict_to_filter[k] for k in inters_intersection}\n",
    "print(f'return_by_filter_dict: {return_by_filter_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb88815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(y_true, y_pred, k=5, ret_arocc=False, ret_mrocc=False, verbose=False, count_equal_as_correct=False, eps_noise=0):\n",
    "    \"\"\"\n",
    "    partly from http://stephantul.github.io/python/pytorch/2020/09/18/fast_topk/\n",
    "    count_equal counts equal values as being a correct choice. e.g. all preds = 0 --> T1acc=1\n",
    "    ret_mrocc ... also return median rank of correct choice\n",
    "    eps_noise ... if > 0, and noise*eps to y_pred .. recommended e.g. 1e-10 #?Yu\n",
    "    \"\"\"\n",
    "    if eps_noise > 0:\n",
    "        if torch.is_tensor(y_pred):#?Yu\n",
    "            y_pred = y_pred + torch.rand(y_pred.shape)*eps_noise\n",
    "        else:\n",
    "            y_pred = y_pred + np.random.rand(*y_pred.shape)*eps_noise\n",
    "    if count_equal_as_correct:\n",
    "        greater = (y_pred > y_pred[range(len(y_pred)), y_true][:,None]).sum(1) # how many are bigger\n",
    "    else:\n",
    "        greater = (y_pred >= y_pred[range(len(y_pred)), y_true][:,None]).sum(1) # how many are bigger or equal\n",
    "    if torch.is_tensor(y_pred):\n",
    "        greater = greater.long()\n",
    "    if isinstance(k, int): k = [k] # pack it into a list\n",
    "    tkaccs = []\n",
    "    for ki in k:\n",
    "        if count_equal_as_correct:\n",
    "            tkacc = (greater<=(ki-1))\n",
    "        else:\n",
    "            tkacc = (greater<=(ki))\n",
    "\n",
    "        if torch.is_tensor(y_pred):\n",
    "            tkacc = tkacc.float().mean().detach().cpu().numpy()\n",
    "        else:\n",
    "            tkacc = tkacc.mean()\n",
    "        tkaccs.append(tkacc)\n",
    "        if verbose:\n",
    "            print('Top', ki, 'acc:\\t', str(tkacc)[:6])\n",
    "    \n",
    "    if ret_arocc:\n",
    "        arocc = greater.float().mean()+1\n",
    "        if torch.is_tensor(arocc):\n",
    "            arocc = arocc.detach().cpu().numpy()\n",
    "        return (tkaccs[0], arocc) if len(tkaccs) == 1 else (tkaccs, arocc)\n",
    "    if ret_mrocc:\n",
    "        mrocc = greater.median()+1\n",
    "        if torch.is_tensor(mrocc):\n",
    "            mrocc = mrocc.float().detach().cpu().numpy()\n",
    "        return (tkaccs[0], mrocc) if len(tkaccs) == 1 else (tkaccs, mrocc)\n",
    "    \n",
    "    return tkaccs[0] if len(tkaccs) == 1 else tkaccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(\n",
    "        InMemory: InMemoryClamp,\n",
    "        train_idx: np.ndarray,\n",
    "        valid_idx: np.ndarray,\n",
    "        test_idx: np.ndarray,\n",
    "        hparams: dict,\n",
    "        run_info: mlflow.entities.RunInfo,\n",
    "        checkpoint_file: Optional[Path] = None,\n",
    "        keep: bool = True,\n",
    "        device: str = 'cpu',\n",
    "        bf16: bool = False, #?Yu: when to set bf16=True?\n",
    "        verbose: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train a model on `InMemory[train_idx]` while validating on `InMemory[valid_idx]`.\n",
    "    Once the training is finished, evaluate the model on `InMemory[test_idx]`.\n",
    "\n",
    "    A moodel-optimizer PyTorch checkpoint can be passed to resume training.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    InMemory: :class:`dataset.InMemoryClamp`\n",
    "         Dataset instance.\n",
    "    train_idx: :class:`numpy.ndarray`\n",
    "        Activity indices of the training split.\n",
    "    valid_idx: :class:`numpy.ndarray`\n",
    "        Activity indices of the validation split.\n",
    "    test_idx: :class:`numpy.ndarray`\n",
    "        Activity indices of the test split.\n",
    "    hparams: dict\n",
    "        Model characteristics and training strategy.\n",
    "    run_info: :class:`mlflow.entities.RunInfo`\n",
    "        MLflow's run details (for logging purposes).\n",
    "    checkpoint_file: str or :class:`pathlib.Path`\n",
    "        Path to a model-optimizer checkpoint from which to resume training.\n",
    "    keep: bool\n",
    "        Keep the persisted model weights if True, remove them otherwise.\n",
    "    device: str\n",
    "        Device to use for training (e.g., \"cpu\" or \"cuda\").\n",
    "    verbose: bool\n",
    "        Print verbose messages if True.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        if checkpoint_file is None:\n",
    "            message = 'Strat training.'\n",
    "        else:\n",
    "            message = f'Resume training from {checkpoint_file}.'\n",
    "        logger.info(message)\n",
    "\n",
    "\n",
    "    # ================================= Function signature and parameters =================================\n",
    "    # initialize checkpoint, if any; if no checkpoint is given, an empty dict is returned.\n",
    "    checkpoint = init_checkpoint(checkpoint_file, device)\n",
    "    # get paths to the artifacts directory and the model weights.\n",
    "    artifacts_dir, checkpoint_file_path, metrics_file_path = get_log_paths(run_info)\n",
    "    early_stopping = EarlyStopper(patience=hparams['patience'], min_delta=0.0001)\n",
    "\n",
    "    # ================================= Model initialization =================================\n",
    "    print(hparams)\n",
    "    \n",
    "    #?Yu: Regard different assays or targets as different tasks. Keep the below `Multitask` related code if used later, otherwise remove it.\n",
    "    #?Yu: why `setup_assay_onehot` is used here?`\n",
    "    if 'Multitask' in hparams.get('model'):\n",
    "\n",
    "        _, train_assays = InMemory.get_unique_names(train_idx)\n",
    "        InMemory.setup_assay_onehot(size=train_assays.index.max() + 1)\n",
    "        train_assay_features = InMemory.assay_features[:train_assays.index.max() + 1] #?Yu: no `assay_features` defined neither in the primary code or 'InMemoryClamp` before.\n",
    "        train_assay_features_norm = F.normalize(torch.from_numpy(train_assay_features), #?Yu: why set this here but use it quite later?\n",
    "            p=2, dim=1 #Yu: p=2: the exponent value in the norm formulation; dim=1: the dimension to reduce.\n",
    "        ).to(device)\n",
    "\n",
    "        model = init_model(\n",
    "            compound_features_size=InMemory.compound_features_size,\n",
    "            assay_feature_size=InMemory.assay_onehot.size, #?Yu: `assay_onehot` has not been defined in the `InMemoryClamp` class before.\n",
    "            hp=hparams,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        model = init_model(\n",
    "            compound_features_size=InMemory.compound_features_size,\n",
    "            assay_features_size=InMemory.assay_features_size,\n",
    "            hp=hparams,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    \n",
    "    #Yu: 'model_state_dict' is only available in class `Pretrained(DotProduct)`\n",
    "    #Yu: if not used, remove the below code\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        if verbose:\n",
    "            logger.info('Load model_state_dict from checkpoint into model.')\n",
    "        model.load_state_dict(checkpoint['model_state_dict']) # `load_state_dict` is the attribute for `nn.Module``\n",
    "        model.train() # `train` is a method of `nn.Module` that sets the module in training mode.\n",
    "    \n",
    "    model = model.to(device)\n",
    "    # ================================= Optimizer and loss function initialization =================================\n",
    "    # initialize optimizer\n",
    "    # Moving a model to the GPU should be done before the creation of its optimizer.\n",
    "    optimizer = init_optimizer(model, hparams, verbose)\n",
    "\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        if verbose:\n",
    "            logger.infp('Load optimizer_state_dict from checkpoint into optimizer.')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # initialize loss function #Yu: the core of clamp.\n",
    "    criterion = nn.BCEWithLogitsLoss() # default, allowing `loss_fun` to be optional.\n",
    "    if 'loss_fun' in hparams:\n",
    "        class CustomCE(nn.CrossEntropyLoss):\n",
    "            \"\"\"Cross entropy loss #?Yu\"\"\"\n",
    "            def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "                \"\"\"\n",
    "                param\n",
    "                -------\n",
    "                input: predicted unnormalized logits. This is the raw output (logits, i.e. preactivations, no softmax/sigmoid) from the model, typically of shape [batch_size, batch_size] in contrastive/self-supervised settings.\n",
    "                target: ground truth class indices or class probabilities.\n",
    "\n",
    "                return\n",
    "                -------\n",
    "                for `F.cross_entropy`:\n",
    "                weight: a manual rescaling weight given to each class.\n",
    "                \"\"\"\n",
    "                beta = 1/(input.shape[0]**(1/2)) # scaling factor, normalizes the logits so that their magnitude is independent of batch size, which can help stabilize training.\n",
    "                input = input * (target*2-1) * beta # \n",
    "                target = torch.arange(0, len(input)).to(input.device)\n",
    "\n",
    "                return F.cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)\n",
    "            \n",
    "        class ConLoss(nn.CrossEntropyLoss):\n",
    "            \"\"\"Contrastive Loss\"\"\"\n",
    "            def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "                sigma = 1 # scaling factor. set to 1 here, so it does not affect the result.\n",
    "                bs = target.shape[0] #?Yu\n",
    "                #Yu: remove the below code if not used\n",
    "                #only modify diag that is a negative\n",
    "                # eg makes this from a target of [0, 1, 0]\n",
    "                # tensor([[-1., 1., 1.],\n",
    "                #         [1., 1., 1.],\n",
    "                #         [1., 1., -1.]])\n",
    "                modif = (1-torch.eye(bs)).to(target.device) + (torch.eye(bs).to(target.device)*(target*2-1)) # `torch.eye`: returns a 2-D tensor with ones on the diagonal and zeros elsewhere.`bs` is the number of rows.\n",
    "                input = input*modif/sigma\n",
    "                diag_idx = torch.arange(0, len(input)).to(input.device)\n",
    "\n",
    "                label_smoothing = hparams.get('label_smoothing', 0.0)\n",
    "                if label_smoothing is None:\n",
    "                    label_smoothing = 0.0\n",
    "                \n",
    "                mol2txt = F.cross_entropy(input, diag_idx, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction, label_smoothing=label_smoothing)\n",
    "                text2mol = F.cross_entropy(input.T, diag_idx, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction, label_smoothing=label_smoothing)\n",
    "\n",
    "                return mol2txt + text2mol\n",
    "            \n",
    "        str2loss_fun = {\n",
    "            'BCE': nn.BCEWithLogitsLoss(),\n",
    "            'CE': CustomCE(),\n",
    "            'Con': ConLoss(),\n",
    "        }\n",
    "        assert hparams['loss_fun'] in str2loss_fun, \"loss_fun not implemented\"\n",
    "        criterion = str2loss_fun[hparams['loss_fun']]\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "        \n",
    "    # ================================= Learning rate Scheduler =================================\n",
    "    # lambda function below returns `lr_factor` whatever the input to lambda is.\n",
    "    if 'lr_factor' in hparams:\n",
    "        lr_factor = hparams['lr_factor']\n",
    "    else:\n",
    "        lr_factor = 1 #?Yu: why\n",
    "    scheduler = MultiplicativeLR(optimizer, lr_lambda=lambda _: lr_factor)\n",
    "\n",
    "    #lot_lr_scheduler(torch.optim.lr_scheduler.CosineAnnealingLR, T_max=1000, eta_min=0)\n",
    "    num_steps_per_epoch = len(train_idx)/hparams['batch_size']\n",
    "    class Linwarmup():\n",
    "        def __init__(self, steps=10000):\n",
    "            self.step = 0\n",
    "            self.max_step = steps\n",
    "            self.step_size = 1/steps\n",
    "        def get_lr(self, lr):\n",
    "            if self.step>self.max_step:\n",
    "                return 1\n",
    "            new_lr = self.step * self.step_size\n",
    "            self.step += 1\n",
    "            return new_lr\n",
    "        \n",
    "    #Todo Bug when set to 0\n",
    "    if hparams.get('warmup_step'): #?Yu: why not `if hparams['warmup_step']`?\n",
    "        scheduler2 = torch.optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                                                       lr_lambda=Linwarmup(steps=num_steps_per_epoch + hparams.get('warmup_epochs', 0)).get_lr)\n",
    "    else:\n",
    "        scheduler2 = None\n",
    "    \n",
    "    if lr_factor !=1:\n",
    "        logger.info(f'Scheduler enabled with lr_factor={hparams[\"lr_factor\"]}. Note that this makes different runs difficult to compare.')\n",
    "    else:\n",
    "        logger.info('Scheduler enabled with lr_factor=1. This keeps the interface but results in no reduction.')\n",
    "    # ================================= Batch sampler =================================\n",
    "    # The acutual dataset slicing is actually done manually.\n",
    "    train_sampler = RandomSampler(data_source=train_idx) #?Yu: why not implement train_batcher like 'valid_batcher' and 'test_batcher'?\n",
    "\n",
    "    valid_sampler = SequentialSampler(data_source=valid_idx)\n",
    "    valid_batcher = BatchSampler(\n",
    "        sampler=valid_sampler,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_sampler = SequentialSampler(data_source=test_idx)\n",
    "    test_batcher = BatchSampler(\n",
    "        sampler=test_sampler,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    epoch = checkpoint.get('epoch', 0) #?Yu: what if not checkpoint?\n",
    "    new_train_idx = None #?Yu: why\n",
    "    while epoch < checkpoint.get('epoch', 0) + hparams['epoch_max']:\n",
    "        if hparams.get('train_balanced', False):\n",
    "            logger.info('sampling balanced')\n",
    "            num_pos = InMemory.activity.data[train_idx].sum()\n",
    "            #?Yu_ remove the below comment later\n",
    "            # too large with WeightedRandomSampler\n",
    "            # num_neg = (len(train_idx))-num_pos\n",
    "            remove_those = train_idx[((InMemory.activity.data[train_idx]) == 0)]\n",
    "            remove_those = np.random.choice(remove_those, size=int(len(remove_those)-num_pos)) #?Yu\n",
    "            idx = np.in1d(train_idx, remove_those) #?Yu\n",
    "            new_train_idx = train_idx[~idx] #?Yu\n",
    "            if isinstance(hparams['train_balanced'], int):\n",
    "                max_samples_per_epoch = hparams['train_balanced']\n",
    "                if max_samples_per_epoch > 1:\n",
    "                    logger.info(f'using only {max_samples_per_epoch} for one epoch')\n",
    "                    new_train_idx = np.random.choice(new_train_idx, size=max_samples_per_epoch)\n",
    "            train_sampler = RandomSampler(data_source=new_train_idx)\n",
    "        if hparams.get('train_subsample', 0) > 0: #?Yu: why not `elif`\n",
    "            if hparams['train_subsample']<1:\n",
    "                logger.info(f'subsample training set to {hparams[\"train_subsample\"]*100}%')\n",
    "                hparams['train_subsample'] = int(hparams['train_subsample']*len(train_idx))\n",
    "            logger.info(f'subsample training set to {hparams[\"train_subsample\"]}')\n",
    "            sub_train_idx = np.random.choice(train_idx if new_train_idx is None else new_train_idx, size=int(hparams['train_subsample']))\n",
    "            train_sampler = RandomSampler(data_source=sub_train_idx)\n",
    "        \n",
    "        train_batcher = BatchSampler(\n",
    "            sampler=train_sampler, \n",
    "            batch_size=hparams['batch_size'],\n",
    "            drop_last=False\n",
    "        )\n",
    " \n",
    "        # ================================= Training loop =================================\n",
    "        loss_sum = 0.\n",
    "        preactivations_l = []\n",
    "        topk_l, arocc_l = [], []\n",
    "        activity_idx_l = []\n",
    "        for nb, batch_indices in enumerate(train_batcher): #tqdm(, mininterval=2) #?Yu: what is nb?\n",
    "\n",
    "            # get and unpack batch data\n",
    "            batch_data = Subset(InMemory, indices=train_idx)[batch_indices]\n",
    "            activity_idx, compound_features, assay_features, assay_onehot, activity = batch_data #?Yu: what is no assay_onehot?\n",
    "\n",
    "            # move data to device\n",
    "            #?Yu: remove the below comments if not used\n",
    "            # assignment is not necessary for modules but it is for tensors.\n",
    "            # https://discuss.pytorch.org/t/what-is-the-difference-between-doing-net-cuda-vs-net-to-device/69278/8\n",
    "            if isinstance(compound_features, torch.Tensor):\n",
    "                compound_features = compound_features.to(device)\n",
    "            assay_features = assay_features.to(device) if not isinstance(assay_features[0], str) else assay_features #?Yu: why not using same method that is used for `compound_features`?\n",
    "            assay_onehot = assay_onehot.to(device).float() if not isinstance(assay_onehot[0], str) else assay_onehot #?Yu\n",
    "            activity = activity.to(device)\n",
    "\n",
    "            # forward\n",
    "            #with torch.autocast(\"cuda\", dtype=torch.bfloat16 if bf16 else torch.float32): #?Yu: shall I keep this comment?\n",
    "            if hparams.get('loss_fun') in ('CE', 'Con'): # why in the two cases, `forward_dense` is used?\n",
    "                preactivations = model.forward_dense(compound_features, #?Yu: go to check the difference between 'forward' and 'forward_dense'\n",
    "                                                     assay_onehot if 'Multitask' in hparams['model'] else assay_features) #?Yu: consider whether to remove 'assay_onehot'\n",
    "            else:\n",
    "                preactivations = model(compound_features, \n",
    "                                       assay_onehot if 'Multitask' in hparams['model'] else assay_features)\n",
    "            \n",
    "            # loss\n",
    "            beta = hparams.get('beta', 1)\n",
    "            if beta is None: beta = 1\n",
    "            preactivations = preactivations*1/beta #?Yu\n",
    "            loss = criterion(preactivations, activity)\n",
    "\n",
    "            # zero gradients, backpropagation, update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if hparams.get('optimizer') == 'SAM': #?Yu\n",
    "                def closure():\n",
    "                    preactivations = model(compound_features, assay_onehot if 'Multitask' in hparams['model'] else assay_features) # why compute preactivation again?\n",
    "                    loss = criterion(preactivations, activity)\n",
    "                    loss.backward()\n",
    "                    return loss \n",
    "                optimizer.step(closure)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                if scheduler2: scheduler2.step() #?Yu\n",
    "\n",
    "            # accumulate loss \n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            if hparams.get('loss_fun')=='Con':\n",
    "                ks = [1, 5, 10, 50] #?Yu\n",
    "                tkaccs, arocc = top_k_accuracy(torch.arange(0, len(preactivations)), preactivations, k=[1, 5, 10, 50], ret_arocc=True)\n",
    "                topk_l.append(tkaccs)\n",
    "                arocc_l.append(arocc)\n",
    "            if hparams.get('loss_fun') in ('CE', 'Con'):\n",
    "                #preactivations = preactivations.sum(axis=1) #?Yu: why keep it here?\n",
    "                preactivations =torch.diag(preactivations) # get only diag elements\n",
    "\n",
    "            # accumulate preactivations\n",
    "            # - need to detach; preactivations.requires_grad = True\n",
    "            # - move it to cpu #?Yu\n",
    "            preactivations_l.append(preactivations.detach().cpu())\n",
    "\n",
    "            # accumulate_indices to track order in which the dataset is visited\n",
    "            # - activity_idx is a np.array, not a torch.tensor\n",
    "            activity_idx_l.append(activity_idx)\n",
    "\n",
    "            if nb % EVERY == 0 and verbose: #?Yu: EVERY = 50000, why set it to 50000?\n",
    "                logger.info(f'Epoch{epoch}: Training batch {nb} out of {len(train_batcher) - 1}.')\n",
    "        \n",
    "        # log mean loss over all minibatches\n",
    "        mlflow.log_metric('train_loss', loss_sum / len(train_batcher), step=epoch)\n",
    "        if wandb.run:\n",
    "            wandb.log({\n",
    "                'train/loss': loss_sum / len(train_batcher),\n",
    "                'lr': scheduler2.get_last_lr()[0] if scheduler2 else scheduler.get_last_lr()[0]\n",
    "            }, step=epoch)\n",
    "\n",
    "        # compute metrics for each assay (on the cpu)\n",
    "        preactivations = torch.cat(preactivations_l, dim=0)\n",
    "        probabilities = torch.sigmoid(preactivations).numpy()\n",
    "\n",
    "        activity_idx = np.concatenate(activity_idx_l, axis=0)\n",
    "        # assert np.array_equal(np.sort(activity_idx), train_idx)\n",
    "        # assert not np.array_equal(activity_idx, train_idx)\n",
    "\n",
    "        targets = sparse.csc_matrix(\n",
    "            (\n",
    "                InMemory.activity.data[activity_idx],\n",
    "                (\n",
    "                    InMemory.activity.row[activity_idx],\n",
    "                    InMemory.activity.col[activity_idx]\n",
    "                )\n",
    "            ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.bool\n",
    "        )\n",
    "\n",
    "        scores = sparse.csc_matrix(\n",
    "            (\n",
    "                probabilities, #?Yu: why is probabilities used here?\n",
    "                (\n",
    "                    InMemory.activity.row[activity_idx],\n",
    "                    InMemory.activity.col[activity_idx]\n",
    "                )\n",
    "            ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        #?Yu: `metrics` should be changed according to my implementation.\n",
    "        #md = metrics.swipe_threshold_sparse(targets=targets, scores=scores,verbose=verbose>=2, ret_dict=True) # returns dict for with metric per assay in the form of {metric: {assay_nr: value}} #?Yu: isn't `verbose=verbose>=2` syntax error?\n",
    "        md = swipe_threshold_sparse(targets=targets, scores=scores,verbose=verbose>=2, ret_dict=True)\n",
    "\n",
    "        if hparams.get('loss_fun') == 'Con':\n",
    "            for ii, k in enumerate(ks):#?Yu: `ks` is not defined in this loop.\n",
    "                md[f'top_{k}_acc'] = {0:np.vstack(topk_l)[:-1, ii].mean()} # drop last (might be not full) #?Yu why\n",
    "            md['arocc'] = {0:np.hstack(arocc_l)[:-1].mean()} # drop last (might be not full) #?Yu why\n",
    "\n",
    "        logdic = {f'train_mean_{k}': np.nanmean(list(v.values())) for k,v in md.items() if v}\n",
    "        mlflow.log_metrics(logdic, step=epoch) #?Yu: sort the use of mlflow along the code.\n",
    "        if wandb.run: wandb.log({k.replace('_', '/'):v for k, v in logdic.items()}, step=epoch)\n",
    "        # if verbose: logger.info(logdic) #?Yu: why not print the logdic？\n",
    "\n",
    "        # ================================= Validation loop =================================\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            loss_sum = 0.\n",
    "            preactivations_l = [] #?Yu: will this overwrite the ones in the training loop?\n",
    "            activity_idx_l = []\n",
    "            for nb, batch_indices in enumerate(valid_batcher):\n",
    "\n",
    "                # get and unpack batch data\n",
    "                batch_data = Subset(InMemory, indices=valid_idx)[batch_indices]\n",
    "                activity_idx, compound_features, assay_features, _, activity = batch_data #?Yu why isn't here `assay_onehot`?\n",
    "\n",
    "                # move data to device\n",
    "                # assignment is not necessary for modules but it is for tensors.\n",
    "                # https://discuss.pytorch.org/t/what-is-the-difference-between-doing-net-cuda-vs-net-to-device/69278/8\n",
    "                if isinstance(compound_features, torch.Tensor):\n",
    "                    compound_features = compound_features.to(device)\n",
    "                assay_features = assay_features.to(device) if not isinstance(assay_features[0], str) else assay_features # why is the conditions different between 'assay_features' and 'compound_features'.\n",
    "                activity = activity.to(device)\n",
    "\n",
    "                # forward #?Yu: why the `Multitask` related code is here but not in the training loop?`\n",
    "                if 'Multitask' in hparams['model']:\n",
    "                    assay_features_norm = F.normalize(\n",
    "                        assay_features, p=2, dim=1\n",
    "                    )\n",
    "                    sim_to_train = assay_features_norm @ train_assay_features_norm.T #?Yu\n",
    "                    sim_to_train_weights = F.softmax(sim_to_train * hparams['multitask_temperature'], dim = 1) #?Yu: what does `F.softmax` do?\n",
    "                    preactivations = model(compound_features, sim_to_train_weights)\n",
    "                \n",
    "                elif hparams.get('loss_fun') in ('CE', 'Con'):\n",
    "                    preactivations = model.forward_dense(compound_features, assay_onehot if 'Multitask' in hparams['model'] else assay_features) #?Yu: 'assay_onehot' is not defined in the validation loop.\n",
    "                else:\n",
    "                    preactivations = model(compound_features, assay_features)\n",
    "\n",
    "                # loss\n",
    "                preactivations = preactivations * 1 / hparams.get('beta', 1) #?Yu\n",
    "                #?Yu Why is the below code block commented out in the primary code.\n",
    "                if hparams.get('loss_fun') in ('CE', 'Con'):\n",
    "                    loss = F.binary_cross_entropy_with_logits(preactivations, activity)\n",
    "                else:\n",
    "                    loss = criterion(preactivations, activity)\n",
    "                \n",
    "                # accumulate loss\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "                if hparams.get('loss_fun') in ('CE', 'Con'): # how to calc the below metrics if `loss_fun` is neither 'CE' nor 'Con'?\n",
    "                    ks = [1, 5, 10, 50]\n",
    "                    tkaccs, arocc = top_k_accuracy(torch.arange(0, len(preactivations)), preactivations, k=[1, 5, 10, 50], ret_arocc=True)\n",
    "                    topk_l.append(tkaccs) # already detached numpy #?Yu\n",
    "                    arocc_l.append(arocc)\n",
    "                \n",
    "                # accumulate preactivations\n",
    "                #Yu: remove the below comments if not used\n",
    "                # - need to detach; preactivations.requires_grad is True\n",
    "                # - move it to cpu\n",
    "                if hparams.get('loss_fun') in ('CE', 'Con'): #?Yu: combine this condition with the one above? and similarily, how to calc preactivations if `loss_fun` is neither 'CE' nor 'Con'?\n",
    "                    # preactivations = preactivations.sum(axis=1)\n",
    "                    preactivations = torch.diag(preactivations) #?Yu: `torch.diag`\n",
    "\n",
    "                preactivations_l.append(preactivations.detach().cpu())\n",
    "\n",
    "                # accumulate indices just to double check.\n",
    "                # - activity_idx is a np.array, not a torch.tensor\n",
    "                activity_idx_l.append(activity_idx)\n",
    "\n",
    "                if nb % EVERY == 0 and verbose:\n",
    "                    logger.info(f'Epoch{epoch}: Validation batch {nb} out of {len(valid_batcher) -1}.')\n",
    "            \n",
    "            # log mean loss over all minibatches\n",
    "            valid_loss = loss_sum / len(valid_batcher)\n",
    "            mlflow.log_metrics('valid_loss', valid_loss, step=epoch) #?Yu: sort the use of mlflow along the code.\n",
    "            if wandb.run: \n",
    "                wandb.log({'valid/loss': valid_loss}, step=epoch)\n",
    "            \n",
    "            # compute test auroc and avgp for each assay (on the cpu)\n",
    "            preactivations = torch.cat(preactivations_l, dim=0)\n",
    "            probabilities = torch.sigmoid(preactivations).numpy()\n",
    "\n",
    "            activity_idx = np.concatenate(activity_idx_l, axis=0)\n",
    "            # assert np.array_equal(activity_idx, valid_idx) #?Yu: why is this line commented out in the primary code?\n",
    "\n",
    "            targets = sparse.csc_matrix(\n",
    "                (\n",
    "                    InMemory.activity.data[valid_idx],\n",
    "                    (\n",
    "                        InMemory.activity.row[valid_idx],\n",
    "                        InMemory.activity.col[valid_idx]\n",
    "                    )\n",
    "                ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.bool\n",
    "            )\n",
    "\n",
    "            scores = sparse.csc_matrix(\n",
    "                (\n",
    "                    probabilities,\n",
    "                    (\n",
    "                        InMemory.activity.row[valid_idx],\n",
    "                        InMemory.activity.col[valid_idx]\n",
    "                    )\n",
    "                ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.float32\n",
    "            )\n",
    "\n",
    "            #md = metrics.swipe_threshold_sparse(targets=targets, scores=scores, verbose=verbose>=2, ret_dict=True)\n",
    "            md = swipe_threshold_sparse(targets=targets, scores=scores, verbose=verbose>=2, ret_dict=True)\n",
    "\n",
    "            if hparams.get('loss_fun') == 'Con': #?Yu: what if 'loss_fun' is 'Con'\n",
    "                #?Yu: how about other metrics?\n",
    "                for ii, k in enumerate(ks): #?Yu: `ks` is not defined in this loop.\n",
    "                    md[f'top_{k}_acc'] = {0:np.vstack(topk_l)[:-1, ii].mean()} # drop last (might be not full)\n",
    "                md['arocc'] = {0:np.hstack(arocc_l)[:-1].mean()} # drop last (might be not full)\n",
    "            \n",
    "            # log metrics mean over assays #?Yu: modify here to calc metrics on OR datasets.\n",
    "\n",
    "            logdic = {f'valid_mean_{k}': np.nanmean(list(v.values())) for k,v in md.items() if v} #?Yu: what is logdic?\n",
    "            logdic['valid_loss'] =valid_loss\n",
    "\n",
    "            mlflow.log_metrics(logdic, step=epoch)\n",
    "\n",
    "            if wandb.run: wandb.log({k.replace('_', '/'):v for k, v in logdic.items()}, step=epoch)\n",
    "            # if verbose: logger.info(logdic)\n",
    "\n",
    "            # monitor metric\n",
    "            evaluation_metric = 'valid_mean_davgp'\n",
    "\n",
    "            if evaluation_metric not in logdic:\n",
    "                logger.info('Using -valid_loss because valid_mean_avgp not in logdic.')\n",
    "            log_value = logdic.get(evaluation_metric, -valid_loss) #?Yu: why -valid_loss?\n",
    "            # metric_monitor(logdic['valid_mean_davgp'], epoch)\n",
    "            do_early_stop = early_stopping(-log_value) # smaller is better #?Yu: why -log_value?\n",
    "\n",
    "            # log model checkpoint dir\n",
    "            if wandb.run:\n",
    "                wandb.run.config.update({'model_save_dir':checkpoint_file_path})\n",
    "            \n",
    "            if early_stopping.improved:\n",
    "                logger.info(f'Epoch {epoch}: Save model and optimizer checkpoint with val-davgp: {log_value}.')\n",
    "                torch.save({\n",
    "                    'value': log_value,\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint_file_path)\n",
    "            \n",
    "\n",
    "            if do_early_stop: #?Yu is this set by me or during the training loop?\n",
    "                logger.info(f'Epoch {epoch}: Out of patience. Early stop!')\n",
    "                break\n",
    "\n",
    "            model.train() #?Yu: how this code line connect with the code above.\n",
    "        \n",
    "        epoch +=1\n",
    "    \n",
    "    # ================================= Testing loop =================================\n",
    "    # test with best model\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        epoch -= 1 #?Yu how can this be the best model\n",
    "        logger.info(f'Epoch {epoch}: Restore model from checkpoint.')\n",
    "        # check if checkpoint exists\n",
    "        if not os.path.exists(checkpoint_file_path):\n",
    "            logger.warning(f'Checkpoint file {checkpoint_file_path} does not exist. Test with init model.')\n",
    "        else:\n",
    "            checkpoint = torch.load(checkpoint_file_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        loss_sum = 0.\n",
    "        preactivations_l = []\n",
    "        activity_idx_l = []\n",
    "        for nb, batch_indices in enumerate(test_batcher):\n",
    "\n",
    "            # get and unpack batch data\n",
    "            batch_data = Subset(InMemory, indices=test_idx)[batch_indices]\n",
    "            activity_idx, compound_features, assay_features, _, activity = batch_data #?Yu: why isn't here `assay_onehot`?\n",
    "\n",
    "            # move data to device\n",
    "            # assignment is not necessary for modules but it is for tensors.\n",
    "            # https://discuss.pytorch.org/t/what-is-the-difference-between-doing-net-cuda-vs-net-to-device/69278/8\n",
    "            if isinstance(compound_features, torch.Tensor):\n",
    "                compound_features = compound_features.to(device)\n",
    "            assay_features = assay_features.to(device) if not isinstance(assay_features[0], str) else assay_features\n",
    "            activity = activity.to(device)\n",
    "\n",
    "            # forward\n",
    "            if 'Multitask' in hparams['model']:\n",
    "                assay_features_norm = F.normalize(assay_features, p=2, dim=1) #\n",
    "                sim_to_train = assay_features_norm @ train_assay_features_norm.T\n",
    "                sim_to_train_weights = F.softmax(sim_to_train * hparams['multitask_temperature '], dim=1) #?Yu: what is `sim_to_train_weights`?\n",
    "                preactivations = model(compound_features, sim_to_train_weights)\n",
    "            else:\n",
    "                preactivations = model(compound_features, assay_features) #?Yu: why not `assay_onehot`?\n",
    "            \n",
    "            # loss\n",
    "            #?Yu: why the below code block is commented out in the primary code.\n",
    "            #if hparams.get('loss_fun') in ('CE', 'Con'):\n",
    "            #    loss = F.binary_cross_entropy_with_logits(preactivations, activity)\n",
    "            #else:\n",
    "            loss = criterion(preactivations, activity)\n",
    "\n",
    "            # accumulate loss\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            # accumulate preactivations\n",
    "            # - need to detach; preactivations.requires_grad is True\n",
    "            # - move it to cpu\n",
    "            preactivations_l.append(preactivations)\n",
    "\n",
    "            # accumulate indices just to double check.\n",
    "            # - activity_idx is a np.array, not a torch.tensor\n",
    "            activity_idx_l.append(activity_idx) #?Yu: why not this code line closely after the definition of `activity_idx`.\n",
    "\n",
    "            if nb % EVERY == 0 and verbose:\n",
    "                logger.info(f'Epoch{epoch}: Test batch {nb} out of {len(test_batcher) -1}.')\n",
    "        \n",
    "        # log mean loss over all minibatches\n",
    "        mlflow.log_metric('test_loss', loss_sum / len(test_batcher), step=epoch)\n",
    "        if wandb.run: wandb.log({'test/loss': loss_sum / len(test_batcher)})\n",
    "\n",
    "        # compute test auroc and avgp for each assay (on the cpu) 'WHY??? #?Yu: 'WHY' is in the primary code.\n",
    "        preactivations = torch.cat(preactivations_l, dim=0)\n",
    "        probabilities = torch.sigmoid(preactivations) #?Yu: figure out `sigmoid` function, why use it here.\n",
    "\n",
    "        activity_idx = np.concatenate(activity_idx_l, axis=0)\n",
    "        # assert np.array_equal(activity_idx, test_idx) #Todo WHY??? #?Yu: 'WHY' is in the primary code.\n",
    "\n",
    "        probabilities = probabilities.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        targets = sparse.csc_matrix(\n",
    "            (\n",
    "                InMemory.activity.data[test_idx],\n",
    "                (\n",
    "                    InMemory.activity.row[test_idx],\n",
    "                    InMemory.activity.col[test_idx]\n",
    "                )\n",
    "            ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.bool\n",
    "        )\n",
    "\n",
    "        scores = sparse.csc_matrix(\n",
    "            (\n",
    "                probabilities,\n",
    "                (\n",
    "                    InMemory.activity.row[test_idx],\n",
    "                    InMemory.activity.col[test_idx]\n",
    "                )\n",
    "            ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        #md  = metrics.swipe_threshold_sparse(targets=targets, scores=scores, verbose=verbose>=2, ret_dict=True)\n",
    "        md  = swipe_threshold_sparse(targets=targets, scores=scores, verbose=verbose>=2, ret_dict=True)\n",
    "\n",
    "        if hparams.get('loss_fun') == 'Con':\n",
    "            for ii, k in enumerate(ks): # what is `ii`\n",
    "                md[f'top_{k}_acc'] = {0:np.vstack(topk_l)[:-1, ii].mean()} # drop last (might be not full)\n",
    "            md['arocc'] = {0:np.hstack(arocc_l)[:-1].mean()} # drop last (might be not full)\n",
    "\n",
    "        # log metrics mean over assays\n",
    "\n",
    "        logdic = {f'test_mean_{k}': np.nanmean(list(v.values())) for k,v in md.items() if v} #?Yu: why `:` is not in f''\n",
    "        mlflow.log_metrics(logdic, step=epoch)\n",
    "\n",
    "        if wandb.run: wandb.log({k.replace('_','/'):v for k, v in logdic.items()}, step=epoch)\n",
    "        if verbose:\n",
    "            logger.info(pd.DataFrame.from_dict([logdic]).T) #?Yu: print a dataframe?\n",
    "        \n",
    "        # compute test activity counts and positives\n",
    "        counts, positives = {}, {}\n",
    "        for idx, col in enumerate(targets.T):\n",
    "            if col.nnz == 0:\n",
    "                continue\n",
    "            counts[idx] = col.nnz\n",
    "            positives[idx] = col.sum()\n",
    "\n",
    "        # 'test_mean_bedroc': 0.6988015835969245, 'test_mean_davgp': 0.16930837444561778, 'test_mean_dneg_avgp': 0.17522445272085613, \n",
    "        # 'test/mean/auroc': 0.6709850363704437, 'test/mean/avgp': 0.6411171492554743, 'test/mean/neg/avgp': 0.7034156779109996, \n",
    "        # 'test/mean/argmax/j': 0.4308185\n",
    "        # store test metrics and counts in a parquet file\n",
    "        metrics_df = pd.DataFrame(md)\n",
    "        metrics_df['argmax_j'] = metrics_df['argmax_j'].apply(sigmoid)\n",
    "        #?Yu: why is the below code commented out in the primary code.\n",
    "        # metrics_df['counts'] = counts # for PC_large: ValueError: Length of values (3933) does not match length of index (615)\n",
    "        # metrics_df['positives'] = positves\n",
    "\n",
    "        metrics_df.index.rename('assay_idx', inplace=True)\n",
    "\n",
    "        metrics_df = InMemory.assay_names.merge(metrics_df, left_index=True, right_index=True)\n",
    "        logger.info(f'Writing test metrics to {metrics_file_path}')\n",
    "        metrics_df.to_parquet(metrics_file_path, compression=None, index=True)\n",
    "\n",
    "        with pd.option_context('float_format', \"{:.2f}\".format):\n",
    "            print(metrics_df)\n",
    "            print(metrics_df.mean(0, numeric_only=True))\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "    if not keep: #?Yu: what is keep\n",
    "        logger.info('Delete model checkpoint.')\n",
    "        checkpoint_file_path.unlink() #unlink_ remove file or link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855cab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: [0, 1, 2]\n",
      "Batch 1: [3, 4, 5]\n",
      "Batch 2: [6, 7, 8]\n",
      "Batch 3: [9]\n",
      "The length of train_batcher: 4\n"
     ]
    }
   ],
   "source": [
    "# Yu: comprehend ``for nb, batch_indices in enumerate(train_batcher)`\n",
    "\n",
    "train_batcher = [[0, 1, 2], \n",
    "                 [3, 4, 5], \n",
    "                 [6, 7, 8], \n",
    "                 [9]]\n",
    "for nb, batch_indices in enumerate(train_batcher):\n",
    "    print(f'Batch {nb}: {batch_indices}')\n",
    "\n",
    "print(f'The length of train_batcher: {len(train_batcher)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [[True, False, False, False, False],\n",
    " [False,  True, False, False, False],\n",
    " [False,  True, False, False, False],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, False]]\n",
    "\n",
    "# convert targets to a sparse matrix\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "targets = sparse.csc_matrix(targets, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3287a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[1.,        0.,        0.,        0.,        0.       ],\n",
    " [0.,        1.,        0.,        0.,        0.       ],\n",
    " [0.,        0.9999994, 0.,        0.,        0.       ],\n",
    " [0.,        1.,        0.,        0.,        0.       ],\n",
    " [0.,        1.,        0.,        0.,        0.       ]]\n",
    "\n",
    "scores = sparse.csc_matrix(scores, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90510ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-03 11:23:51.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mswipe_threshold_sparse\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mFound 0 columns with both positive and negative samples.\u001b[0m\n",
      "\u001b[32m2025-08-03 11:23:51.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mswipe_threshold_sparse\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound and skipped 2 columns with only positive or negative samples.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'argmax_j': {}, 'auroc': {}, 'avgp': {}, 'neg_avgp': {}, 'davgp': {}, 'dneg_avgp': {}, 'auprc': {}, 'bedroc': {}}\n"
     ]
    }
   ],
   "source": [
    "bedroc_alpha = 20\n",
    "md = swipe_threshold_sparse(targets=targets, scores=scores, bedroc_alpha=bedroc_alpha, ret_dict=True)\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "        InMemory: InMemoryClamp,\n",
    "        train_idx: np.ndarray,\n",
    "        test_idx: np.ndarray,\n",
    "        hparams: dict, \n",
    "        run_info: mlflow.entities.RunInfo,\n",
    "        device: str = 'cpu',\n",
    "        verbose: bool = False, \n",
    "        model = None\n",
    ") -> None: #?Yu: isn't `metric.df` returned?\n",
    "    \"\"\"\n",
    "    Test a model on `InMemory[test_idx]`if test metrics are not yet to be found under the `actifacts` directory. \n",
    "    If so, interrupt the program.\n",
    "\n",
    "    Params\n",
    "    ----------\n",
    "    InMemory: InMemoryClamp\n",
    "        Dataset instance\n",
    "    train_idx: :class:`numpy.ndarray``\n",
    "        Activity indices of the training split. Only for multitask models. #?Yu: why only for multitask models?\n",
    "    test_idx: :class:`numpy.ndarray``\n",
    "        Activity indices of the test split.\n",
    "    run_info: class:`mlflow.entities.RunInfo`\n",
    "        MLflow's run details (for logging purposes).\n",
    "    device: str\n",
    "        Computing device.\n",
    "    verbose: bool\n",
    "        Be verbose if True.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        logger.info('Start evaluation.')\n",
    "\n",
    "    artifacts_dir = Path('mlruns', run_info.experiment_id, run_info.run_id, 'artifacts')\n",
    "\n",
    "    # for logging new checkpoints\n",
    "    checkpoint_file_path = artifacts_dir / 'checkpoint.pt'\n",
    "    metrics_file_path = artifacts_dir / 'metrics.parquet'\n",
    "\n",
    "    # initialize checkpoint\n",
    "    if model != None:\n",
    "        checkpoint = init_checkpoint(checkpoint_file_path, device)\n",
    "        assert checkpoint, 'No checkpoint found'\n",
    "        assert 'model_state_dict' in checkpoint, 'No model found in checkpoint' #?Yu 'model_state_dict' in checkpoint? how this attribute be gotten?\n",
    "\n",
    "    artifacts_dir, checkpoint_file_path, metrics_file_path = get_log_paths(run_info)\n",
    "\n",
    "    # initialize model\n",
    "    if 'Multitask' in hparams['model']:\n",
    "        _, train_assays = InMemory.get_unique_names(train_idx)\n",
    "        InMemory.setp_assay_onehot(size=train_assays.index.max() + 1)\n",
    "        train_assay_features = InMemory.assay_features[:train_assays.index.max() + 1]\n",
    "        train_assay_features_norm = F.normalize(\n",
    "            torch.from_numpy(train_assay_features), p=2, dim=1\n",
    "        ).to(device)\n",
    "\n",
    "        if model != None:\n",
    "            model = init_model(\n",
    "                compound_feature_size= InMemory.compound_feature_size,\n",
    "                assay_feature_size= InMemory.assay_onehot.size,\n",
    "                hp=hparams, #?Yu: how can the hparams that get the best model be used here?\n",
    "                verbose=verbose\n",
    "            )\n",
    "    else:\n",
    "        if model != None:\n",
    "            model = init_model(\n",
    "                compound_feature_size=InMemory.compound_feature_size,\n",
    "                assay_feature_size=InMemory.assay_features.size,\n",
    "                hp=hparams, verbose=verbose\n",
    "            )\n",
    "        \n",
    "    if verbose:\n",
    "        logger.info('Load model from checkpoint.')\n",
    "    if model != None:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # assignment is not necessary when moving modules, but it is for tensors.\n",
    "    # https://discuss.pytorch.org/t/what-is-the-difference-between-doing-net-cuda-vs-net-to-device/69278/8\n",
    "    # here I only assign for consistency\n",
    "    model = model.to(device)\n",
    "\n",
    "    # initialize loss function\n",
    "    criterion = nn.BCEWithLogitsLoss # why is it enough to use this function instead of `CustomCE` or `ConLoss` during testing?\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    test_sampler = SequentialSampler(data_source=test_idx)\n",
    "    test_batcher = BatchSampler(\n",
    "        sampler=test_sampler,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        drop_last=False #?Yu:\n",
    "    )\n",
    "\n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        loss_sum = 0.\n",
    "        preactivations_l = []\n",
    "        activity_idx_l = []\n",
    "        for nb, batch_indices in enumerate(test_batcher):\n",
    "\n",
    "            # get and unpack batch data\n",
    "            batch_data = Subset(InMemory, indices=test_idx)[batch_indices]\n",
    "            activity_idx, compound_features, assay_features, assay_onehot, activity = batch_data #?Yu: why `assay_onehot` is added here?\n",
    "\n",
    "            # move data to device\n",
    "            if isinstance(compound_features, torch.Tensor):\n",
    "                compound_features = compound_features.to(device)\n",
    "            assay_features = assay_features.to(device) if not isinstance(assay_features[0], str) else assay_features\n",
    "            activity = activity.to(device)\n",
    "\n",
    "            # forward\n",
    "            if 'Multitask' in hparams['model']:\n",
    "                assay_features_norm = F.normalize(assay_features, p=2, dim=1)\n",
    "                sim_to_train = assay_features_norm @ train_assay_features_norm.T\n",
    "                sim_to_train_weights = F.softmax(sim_to_train * hparams['multitask_temperature'], dim=1)\n",
    "                preactivations = model(compound_features, sim_to_train_weights)\n",
    "            else:\n",
    "                preactivations = model(compound_features, assay_features)   \n",
    "            \n",
    "            # loss\n",
    "            loss = criterion(preactivations, activity)\n",
    "\n",
    "            # accumulate loss\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            # accumulate preactivations\n",
    "            # - need to detach; preactivations.requires_grad is True\n",
    "            # - move it to cpu\n",
    "            preactivations_l.append(preactivations.detach().cpu()) #?Yu: why is `detach` used here but not in the `def train_and_test``\n",
    "\n",
    "            # accumulate indices just to double check\n",
    "            # - activity_idx is a np.array, not a torch.tensor\n",
    "            activity_idx_l.append(activity_idx)\n",
    "\n",
    "            if nb % EVERY == 0 and verbose:\n",
    "                logger.info(f'Epoch {epoch}: Test batch {nb} out of {len(test_batcher) - 1}.')\n",
    "\n",
    "        # log mean loss over all minibatches\n",
    "        mlflow.log_metric('test_loss', loss_sum / len(test_batcher), step=epoch)\n",
    "        if wandb.run: wandb.log({'test/loss': loss_sum/len(test_batcher)}, step=epoch)\n",
    "\n",
    "        # compute test auroc and avgp for each assay (on the cpu)\n",
    "        preactivations = torch.cat(preactivations_l, dim=0)\n",
    "        probabilities = torch.sigmoid(preactivations).numpy()\n",
    "\n",
    "        activity_idx = np.concatenate(activity_idx_l, axis=0)\n",
    "        assert np.array_equal(activity_idx, test_idx) #?Yu: this code line is commented out in the `def train_and_test`\n",
    "\n",
    "        targets = sparse.csc_matrix(\n",
    "            (\n",
    "                InMemory.activity.data[test_idx],\n",
    "                (\n",
    "                    InMemory.activity.row[test_idx],\n",
    "                    InMemory.activity.col[test_idx]\n",
    "                )\n",
    "            ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.bool\n",
    "        )\n",
    "\n",
    "        scores = sparse.csc_matrix(\n",
    "            (\n",
    "                probabilities,\n",
    "                (\n",
    "                    InMemory.activity.row[test_idx],\n",
    "                    InMemory.activity.col[test_idx]\n",
    "                )\n",
    "            ), shape=(InMemory.num_compounds, InMemory.num_assays), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        #md = metrics.swipe_threshold_sparse(targets=targets, scores=scores, verbose=verbose>=2, ret_dict=True)\n",
    "        md = swipe_threshold_sparse(targets=targets, scores=scores, verbose=verbose>=2, ret_dict=True)\n",
    "\n",
    "        # log metrics mean over assays\n",
    "        logdic = {f'test_mean_{mdk}': np.mean(list(md[f'{mdk}'].values())) for mdk in md.keys()} #?Yu: why is `mdk` used here? different from the one in the `def train_and_test`\n",
    "        mlflow.log_metrics(logdic, step=epoch)\n",
    "\n",
    "        if wandb.run: wandb.log({k.replace('_', '/'):v for k, v in logdic.items()}, step=epoch)\n",
    "        if verbose: logger.info(logdic)\n",
    "\n",
    "        # compute test activity counts and positives\n",
    "        counts, positives = {}, {}\n",
    "        for idx, col in enumerate(targets.T):#?Yu: why `targets.T`?\n",
    "            if col.nnz == 0:\n",
    "                continue\n",
    "            counts[idx] = col.nnz\n",
    "            positives[idx] = col.sum()\n",
    "        \n",
    "        # store test metrics and counts in a parquet file\n",
    "        metrics_df = pd.DataFrame(md)\n",
    "        metrics_df['argmax_j'] = metrics_df['argmax_j'].apply(sigmoid)\n",
    "        metrics_df['counts'] = counts\n",
    "        metrics_df['positives'] = positives\n",
    "\n",
    "        metrics_df.index.rename('assay_idx', inplace=True)\n",
    "\n",
    "        metrics_df = InMemory.assay_names.merge(metrics_df, left_index=True, right_index=True)\n",
    "        logger.info(f'Writing test metrics to {metrics_file_path}')\n",
    "        metrics_df.to_parquet(metrics_file_path, compression=None, index=True)\n",
    "\n",
    "        if wandb.run:\n",
    "            wandb.log({\"metrics_per_assay\": wandb.Table(data=metrics_df)})\n",
    "        \n",
    "        logger.info(f'Saved best test-metrics to {metrics_file_path}')\n",
    "        logger.info(f'Saved best checkpoint to {checkpoint_file_path}')\n",
    "\n",
    "        model.train() #?Yu: why is this line here?\n",
    "\n",
    "        with pd.option_context('float_format', \"{:.2f}\".format):\n",
    "            print(metrics_df)\n",
    "        \n",
    "        return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138030a1",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67897fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' training pubchem23 without downstream datasets\\npython clamp/train.py     --dataset=./data/pubchem23/     --split=time_a     --assay_mode=clip     --batch_size=8192     --dropout_hidde=0.3 \\\\ #?Yu: this parameter is not implemented in the primary code\\n    --drop_cidx_path=./data/pubchem23/cidx_overlap_moleculenet.npy     --train_subsample=10e6     --wandb --experiment=pretrain\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"example call:\n",
    "python clamp/train.py \\\n",
    "    --dataset=./data/fsmol \\\n",
    "    --split=FSMOL_split \\\n",
    "    --assay_mode=clip \\\n",
    "    --compound_mode=morganc+rdkc \n",
    "\"\"\"\n",
    "\n",
    "\"\"\" training pubchem23 without downstream datasets\n",
    "python clamp/train.py \\\n",
    "    --dataset=./data/pubchem23/ \\\n",
    "    --split=time_a \\\n",
    "    --assay_mode=clip \\\n",
    "    --batch_size=8192 \\\n",
    "    --dropout_hidde=0.3 \\ #?Yu: this parameter is not implemented in the primary code\n",
    "    --drop_cidx_path=./data/pubchem23/cidx_overlap_moleculenet.npy \\\n",
    "    --train_subsample=10e6 \\\n",
    "    --wandb --experiment=pretrain\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d471165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args_override(override_hpjson=True): #?Yu: why set this to True?\n",
    "    parser = argparse.ArgumentParser('Train and test a single run of clamp-Activity model. Overrides arguments from hyperparam-file')\n",
    "    parser.add_argument('-f', type=str) #?Yu \n",
    "    parser.add_argument('--dataset', type=str, default='./data/fsmol', help='Path to a prepared dataset directory') #?Yu: parquet file or npy file or others?\n",
    "    parser.add_argument('--assay_mode', type=str, default='lsa', help='Type of assay features(\"clip\", \"biobert\", or \"lsa\")') #?Yu: why lsa is default? where is lsa implemented?#\n",
    "    parser.add_argument('--compound_mode', type=str, default='morganc+rdkc', help='Type of compound features (default: morganc+rdkc)') \n",
    "    parser.add_argument('--hyperparams', type=str, default='./hparams/default.json', help='Path to hyperparameters to use in training (json, Hyperparams, or logs).')\n",
    "\n",
    "    parser.add_argument('--checkpoint', help='Path to a model-optimizer PyTorch checkpoint from which to resume training.', metavar='')\n",
    "    parser.add_argument('--experiment', type=str, default='debug', help='Name of MLflow experiment where to assign this run.', metavar='')\n",
    "    parser.add_argument('--random', action='store_true', help='Forget about the specified model and run a random baseline.') #?Yu: delete it later if not used?\n",
    "\n",
    "    #?Yu: why the arguments below are commented out in the primary code?\n",
    "    parser.add_argument('--optimizer', type=str, default='AdamW', help='Optimizer to use for training (default: AdamW).')\n",
    "    parser.add_argument('--l2', type=float, default=0.01, help='Weight decay to use for training (default: 0.01).')\n",
    "    parser.add_argument('--loss_fun', type=str, default='BCE', help='Loss function to use for training (default: BCE).')\n",
    "    parser.add_argument('--epoch_max', type=int, default=50, help='Maximum number of epochs to train for (default: 100).')\n",
    "    parser.add_argument('--lr_ini', type=float, default=1e-5, help='Initial learning rate (default: 1e-5).' )\n",
    "\n",
    "    parser.add_argument('--gpu', type=str, default='0', help='GPU number to use (default: 0).', metavar='')\n",
    "    parser.add_argument('--seed', type=int, default=None, help='seed everything with provided seed, default no seed')\n",
    "    \n",
    "    parser.add_argument('--split', type=str, default='time_a_c', help='split-type. Default:time_a_c for time based assay and compound split. Options: time_a, time_c, random:{seed}, or column of activity.parquet triplet.') #?Yu: shall I modify these split options?\n",
    "    parser.add_argument('--support_set_size', type=int, default=0, help='per task how many to add from test- as well as valid- to the train-set (Default:0, i.e. zero-shot).') #?Yu: '0' -> 0\n",
    "    parser.add_argument('--train_only_actives', action='store_true', help='train only with active molecules.')\n",
    "    parser.add_argument('--drop_cidx_path', type=str, default=None, help='Path to a file containing a np of cidx (NOT CIDs) to drop from the dataset.') #?Yu: a np of cidx?\n",
    "\n",
    "    parser.add_argument('--verbose','-v', type=int, default=0, help='verbosity level (default:0)') #?Yu: what does verbosity level mean?\n",
    "    parser.add_argument('--wandb', '-w', action='store_true', help='wandb logging on')\n",
    "    parser.add_argument('--bf16', action='store_true', help='use bfloat16 for training') #?Yu: bfloat16?\n",
    "\n",
    "    args, unknown = parser.parse_known_args() #?Yu: ?\n",
    "    keypairs = dict([unknown[i:i+2] for i in range(0, len(unknown), 1) if unknown[i].startswith('--') and not (unknown[i+1:i+2]+[\"--\"])[0].startswith('--')]) #?Yu: don't understand. delete it?\n",
    "\n",
    "    hparams = get_hparams(path=args.hyperparams, mode='json', verbose=args.verbose)\n",
    "\n",
    "    if override_hpjson:\n",
    "        for k, v in NAME2FORMATTER.items():\n",
    "            if (k not in args):\n",
    "                default = hparams.get(k, None)\n",
    "                parser.add_argument('--'+k, type=v, default=default)\n",
    "                if (k in keypairs):\n",
    "                    logger.info(f'{k} from hparams file will be overwritten')\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    #?Yu: the below args are not in the 'add_argument' function, why?\n",
    "    if args.nonlinearity is None:\n",
    "        args.nonlinearity = 'ReLU'\n",
    "    if args.compound_layer_sizes is None:\n",
    "        logger.info('no compound_layer_sizes provided, setting to hidden_layers')\n",
    "        args.compound_layer_sizes = args.hidden_layers\n",
    "    if args.assay_layer_sizes is None:\n",
    "        logger.info('no assay_layer_sizes provided, setting to hidden_layers')\n",
    "        args.assay_layer_sizes =  args.hidden_layers\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74819c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataset(dataset='./data/fsmol', assay_mode='lsa', compound_mode='morganc+rdkc', split='split', \n",
    "                  verbose=False, support_set_size=0, drop_cidx_path=None, train_only_actives=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Setup the dataset by given a dataset-path.\n",
    "    Loads an InMemoryClamp object containing:\n",
    "    - split: 'split' is the column name in the activity.parquet, 'time_a_c' is in the primary code\n",
    "    - support_set_size: 0, adding {support_set_size} samples from test and from valid to train (per assay/task);\n",
    "    - train_only_actives: False, only uses the active compounds;\n",
    "    - drop_cidx_path: None, path to a npy file containing cidx (NOT CIDs) to drop from the dataset.\n",
    "    \"\"\"\n",
    "    dataset = Path(dataset)\n",
    "    clamp_dl = InMemoryClamp(\n",
    "        root=dataset,\n",
    "        assay_mode=assay_mode,\n",
    "        compound_mode=compound_mode,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # ===== split =====\n",
    "    logger.info(f'loading split info from activity.parquet triplet-list under the column split={split}')\n",
    "    try:\n",
    "        splits = pd.read_parquet(dataset/'activity.parquet')[split]\n",
    "    except KeyError:\n",
    "        raise ValueError(f'no split column {split} in activity.parquet', pd.read_parquet(dataset/'activity.parquet').columns, 'columns available')\n",
    "    train_idx, valid_idx, test_idx =[splits[splits==sp].index.values for sp in ['train', 'valid', 'test']]\n",
    "\n",
    "    # ===== support_set_size =====\n",
    "\n",
    "    # ===== train_only_actives =====\n",
    "\n",
    "    # ===== drop_cidx_path =====\n",
    "\n",
    "    # ===== verbose =====\n",
    "\n",
    "    return clamp_dl, train_idx, valid_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Hyperparameter Preparation\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    # MLflow Experiment Setup\n",
    "    mlflow.set_experiment(args.experiment)\n",
    "\n",
    "    # Seeding (Optional)\n",
    "    if args.seed:\n",
    "        seed_everything(args.seed)\n",
    "        logger.info(f'seeded everything with seed {args.seed}') #?Yu: if not needed, delete it?\n",
    "    \n",
    "    # Dataset Preparation\n",
    "    clamp_dl, train_idx, valid_idx, test_idx = setup_dataset(**args.__dict__)\n",
    "    assert set(train_idx).intersection(set(valid_idx)) == set() # assert no overlap between the splits.\n",
    "    assert set(train_idx).intersection(set(test_idx)) == set()\n",
    "\n",
    "    # Weights & Biases (wandb) Logging\n",
    "    if args.wandb:\n",
    "        runname = args.experiment+args.split[-1]+args.assay_mode[-1]\n",
    "        if args.random:\n",
    "            runname += 'random'\n",
    "        else:\n",
    "            runname = str(runname)+''\n",
    "            runname += str(args.model) #?Yu: what could `args.model` be?\n",
    "        runname += ''.join([chr(random.randrange(97, 97 +26)) for _ in range(3)]) # to add some randomness to the run name\n",
    "        wandb.init(project='clipGPCR', entity='yu', name=runname, config=args.__dict__)\n",
    "\n",
    "    # Device Setup\n",
    "    device = set_device(gpu=args.gpu, verbose=args.verbose)\n",
    "\n",
    "    # Metrics DataFrame Initialization\n",
    "    metrics_df = pd.DataFrame()\n",
    "\n",
    "    # Training/Testing Run (with MLflow Logging)\n",
    "    try:\n",
    "        with mlflow.start_run(): # begins a new experiment run.\n",
    "            mlflowi = mlflow.active_run().info # provides metadata (like run id, experiment id) for this run.\n",
    "        \n",
    "        # Checkpoint Resume Logging\n",
    "        if args.checkpoint is not None:\n",
    "            mlflow.set_tag(\n",
    "                'mlflow.note.content',\n",
    "                f'Resumed training from {args.checkpoint}.'\n",
    "            )\n",
    "        \n",
    "        # Assay Mode Consistency and Logging\n",
    "        if 'assay_mode' in hparams:\n",
    "            if hparams['assay_mode'] != args.assay_mode:\n",
    "                # Warn if there's a mismatch.\n",
    "                logger.warning(f'Assay features are \"{args.assay_mode}\" in command line but \\\"{hparams[\"assay_mode\"]}\\\" in hyperparameter file.')\n",
    "                logger.warning(f'Command line \"{args.assay_mode}\" is the prevailing option.')\n",
    "                hparams['assay_mode'] = args.assay_mode\n",
    "        else:\n",
    "            mlflow.log_param('assay_mode', args.assay_mode)\n",
    "        mlflow.log_params(hparams) # Logs all hyperparamters to MLflow for easy reference and reproducibility.\n",
    "\n",
    "        # Comment out the below code block because the random baseline is seemed unnecessary for my current plan.\n",
    "        #if args.random:\n",
    "        #    mlflow.set_tag(\n",
    "        #        'mlflow.note.content',\n",
    "        #        'Ignore the displayed parameters. Metrics correspond to predictions randomly drawn from U(0, 1).'\n",
    "        #    )\n",
    "        #    utils.random(\n",
    "        #        clamp_dl,\n",
    "        #        test_idx=test_idx,\n",
    "        #        run_info=mlflowi,\n",
    "        #        verbose=args.verbose)\n",
    "        #else:\n",
    "        #metrics_df = utils.train_and_test(\n",
    "        metrics_df = train_and_test(\n",
    "            clamp_dl, \n",
    "            train_idx=train_idx,\n",
    "            valid_idx=valid_idx,\n",
    "            test_idx=test_idx,\n",
    "            hparams=hparams,\n",
    "            run_info=mlflowi,\n",
    "            checkpoint_file=args.checkpoint,\n",
    "            device=device,\n",
    "            bf16=args.bf16,\n",
    "            verbose=args.verbose)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        logger.error('Training manually interrupted. Trying to test with last checkpoint.')\n",
    "        # If the training is manually interrupted, it still tries to evaluate (test) the model using the last checkpoint, and logs results to the same MLflow run.\n",
    "        #?Yu: delete the below code if not used.\n",
    "        #metrics_df = utils.test(\n",
    "        metrics_df = test(\n",
    "            clamp_dl,\n",
    "            train_idx=train_idx,\n",
    "            test_idx=test_idx,\n",
    "            hparams=hparams,\n",
    "            run_info=mlflowi,\n",
    "            device=device,\n",
    "            verbose=args.verbose,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parse_args_override()\n",
    "\n",
    "    run_id = str(time()).split('.')[0]\n",
    "    fn_postfix = f'{args.experiment}_{run_id}'\n",
    "\n",
    "    if args.verbose>=1:\n",
    "        logger.info('Run args:', os.getcwd()+__file__, args.__dict__)\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clamp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
