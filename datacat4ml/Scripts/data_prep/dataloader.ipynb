{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b4212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conda env: datacat (Python 3.8.20)\n",
    "\"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14e05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/miniconda3/envs/datacat/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from scipy import sparse\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try: # only if Graph-Model is used\n",
    "    import dgl\n",
    "except: pass\n",
    "\n",
    "import os\n",
    "from datacat4ml.const import SPLIT_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b41b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_indices_and_data(m, i):\n",
    "    \"\"\"Get the indices and data of a sparse matrix.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    m: a sparse matrix in CSR format\n",
    "    i: the index of the row for which to extract the non-zero elements. #? non-zero?\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple: (indices, data)\n",
    "        col_indices: the column indices of the non-zero data in row i. (in csr format, only the non-zero elements are stored)\n",
    "        data: the values of the non-zero elements in row i.\n",
    "    \"\"\"\n",
    "    # `m.data`: the non-zero values of the sparse matrix\n",
    "    # `m.indices`: the column indices of the non-zero values\n",
    "    # `m.indptr`: which maps the elements of `data` and `indices` to the rows of the sparse matrix. Explanation: https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr\n",
    "    col_indices = m.indices[m.indptr[i]:m.indptr[i+1]] \n",
    "    data = m.data[m.indptr[i]:m.indptr[i+1]]\n",
    "    return col_indices, data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0232e192",
   "metadata": {},
   "source": [
    "# `dataloader.py/class InMemoryClamp(Dataset)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0078f7",
   "metadata": {},
   "source": [
    "`class InMemoryClamp` is called \n",
    "\n",
    "1) as `biobert` in the `utils.py / def train_and_test()` as below:\n",
    "- `biobert.get_unique_names(train_idx)`\n",
    "- `biobert.setup_assay_onehot(size=train_assays.index.max() + 1)`\n",
    "- `biobert.assay_features[:train_assays.index.max() + 1]`\n",
    "- `biobert.compound_features_size,`\n",
    "    `biobert.assay_onehot.size,`\n",
    "- `biobert.assay_features_size,`\n",
    "- `biobert.activity.data[train_idx].sum()`\n",
    "- `biobert.activity.row[activity_idx]`\n",
    "- `biobert.activity.col[activity_idx]`\n",
    "- `batch_data = Subset(biobert, indices=train_idx)[batch_indices]`\n",
    "- `biobert.num_compounds`\n",
    "- `biobert.num_assays`\n",
    "- `biobert.assay_names.merge(metrics_df, left_index=True, right_index=True)`\n",
    "\n",
    "2) as `biobert` in the `overlap.py / check_overlap()` as below:\n",
    "- `biobert, train_idx, valid_idx, test_idx = setup_dataset(dataset=dset_path, assay_mode='', compound_mode='morganc+rdkc',split=split, verbose=False)`\n",
    "- `biobert.activity_df.compound_idx.unique()`\n",
    "\n",
    "3) as `im_biobert` in `dataloader.py / if __name__ == '__main__'`\n",
    "- `im_biobert.meta_assays = True`\n",
    "- `im_biobert.collate`\n",
    "\n",
    "4) as `~biobert.models.models.DotProduct` in `scaled.py`\n",
    "\n",
    "5) `from biobert.models import scaled` in `test_scaled.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a15488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryClamp(Dataset):\n",
    "    \"\"\"\n",
    "    Subclass of :class:`torch.utils.data.Dataset` holding BioBert activity data, \n",
    "    that is, activity triplets, and compound and assay feature vectors.\n",
    "\n",
    "    :class:`InMemoryClamp` supports two different indexing (and iteration) styles. \n",
    "    The default style is to itreate over `(compound, assay, activity)` COO triplets, however they are sorted.\n",
    "    The \"meta-assays\" style consists in interating over unique compounds using a CSR sparse structure,\n",
    "    and averaging the feature vectors of the positive and negative assays of each compound. \n",
    "\n",
    "    By inheriting from :class:`torch.utils.data.Dataset`, this class must implement at least two methods:\n",
    "    - :meth:`__len__` to return the size of the dataset.\n",
    "    - :meth:`__getitem__` to retrieve a single data point from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: Union[str, Path],\n",
    "            assay_mode: str,\n",
    "            compound_mode: str = None, \n",
    "            train_size: float = 0.6, \n",
    "            aid_max: int = None, #? Yu: could be removed\n",
    "            cid_max: int = None, #? Yu: could be removed\n",
    "            verbose: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the dataset class.\n",
    "\n",
    "        - The data is loaded in memory with the :meth:`_load_dataset` method.\n",
    "        - Splits are created separately along compounds and along assays with the :meth:`_find_splits` method. Compound and assay splits can be interwoven with the :meth:`subset` method.\n",
    "\n",
    "        Params:\n",
    "        root: str or :class:`pathlib.Path`\n",
    "            Path to a directory of ready BioBert files.\n",
    "        assay_mode: str\n",
    "            Type of assay features (\"biobert-last\", \"biobert-two-last\", or \"lsa\").\n",
    "        train_size: float (between 0 and 1)\n",
    "            Fraction of compounds and assays assigned to training data.\n",
    "        verbose: bool\n",
    "            Be verbose if True.\n",
    "        \"\"\"\n",
    "        self.root = Path(root)\n",
    "        self.assay_mode = assay_mode\n",
    "        self.compound_mode = compound_mode\n",
    "        self.train_size = train_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self._load_dataset()\n",
    "        self.find_splits()\n",
    "\n",
    "        self.meta_assays = False\n",
    "        self.assay_onehot = None\n",
    "\n",
    "    def _load_dataset(self) -> None:\n",
    "        \"\"\"\n",
    "        Load prepared dataset from the `root` directory:\n",
    "\n",
    "        - `activity`: Parquet file containing `(compound, assay, activity)` triplets. Compounds and assays are represented by indices, \n",
    "        and thus the file is directly loaded into a :class:`scipy.sparse.coo_matrix` with rows corresponding to compounds and columns corresponding to assays.\n",
    "\n",
    "        - `compound_names`: Parquet file containing the mapping between the compound index used in `activity` and the corresponding compound name.\n",
    "        It is loaded into a :class:`pandas.DataFrame`.\n",
    "\n",
    "        - `assay_names`: Parquet file containing the mapping between the assay index used in `activity` and the corresponding assay name. \n",
    "        It is loaded into a :class:`pandas.DataFrame`.\n",
    "\n",
    "        - `compound_features`: npz file containing the compound features array, where the feature vector for the compound indexed by `idx` is stored in the `idx`-th row. \n",
    "        It is loaded into a :class:`scipy.sparse.csr_matrix`.\n",
    "\n",
    "        - `assay_features`: npy file containing the assay features array, where the feature vector for the assay indexed by `idx` is stored in the `idx`-th row.\n",
    "        It is loaded into a :class:`numpy.ndarray`.\n",
    "\n",
    "        Compute the additional basic dataset attributes `num_compounds`, `num_assays`, `compound_feature_size`, `assay_feature_size`.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            logger.info(f'Load dataset from \"{self.root} with {self.assay_mode}\" assay features.')\n",
    "\n",
    "        #======= Load compound data =======\n",
    "        with open(self.root / 'compound_names.parquet', 'rb') as f:\n",
    "            self.compound_names = pd.read_parquet(f)\n",
    "        self.num_compounds = len(self.compound_names)\n",
    "\n",
    "        compound_modes = self.compound_mode.split('||') if self.compound_mode is not None else 1 #? Yu: replace `||` with `+` ?        if len(compound_modes) >1:\n",
    "        if len(compound_modes) > 1:\n",
    "            logger.info('Multiple compound modes are concatenated ')\n",
    "            self.compound_features = np.concatenate([self._load_compound(cm) for cm in compound_modes], axis=1)\n",
    "        else:\n",
    "            self.compound_features = self._load_compound(self.compound_mode)\n",
    "        # compound_feature_size\n",
    "        if 'graph' in self.compound_mode and (not 'graphormer' in self.compound_mode):\n",
    "            self.compound_features_size = self.compound_features[0].ndata['h'].shape[1] # in_edge_feats. #? Yu\n",
    "        elif isinstance(self.compound_features, pd.DataFrame):\n",
    "            self.compound_features_size = 40000 #? Yu\n",
    "        else:\n",
    "            if len(self.compound_features.shape)>1:\n",
    "                self.compound_features_size = self.compound_features.shape[1]\n",
    "            else:\n",
    "                self.compound_features_size = 1\n",
    "\n",
    "        #======== Load assay data ========\n",
    "        with open(self.root / 'assay_info.parquet', 'rb') as f:\n",
    "            self.assay_names = pd.read_parquet(f)\n",
    "        self.num_assays = len(self.assay_names)\n",
    "\n",
    "        assay_modes = self.assay_mode.split('||')\n",
    "        if len(assay_modes)>1:\n",
    "            logger.info('Multiple assay modes are concatenated')\n",
    "            self.assay_features = np.concatenate([self._load_assay(am) for am in assay_modes], axis=1)\n",
    "        else:\n",
    "            self.assay_features = self._load_assay(self.assay_mode)\n",
    "\n",
    "        # assay_feature_size\n",
    "        if (self.assay_features is None):\n",
    "            self.assay_features_size = 512 #wild guess also 512#? Yu\n",
    "        elif len(self.assay_features.shape)==1:\n",
    "            # its only a list, so probably text\n",
    "            self.assay_features_size = 768 #? Yu\n",
    "        else:\n",
    "            self.assay_features_size = self.assay_features.shape[1]\n",
    "        \n",
    "        #======= Load activity data =======\n",
    "        with open(self.root / 'activity.parquet', 'rb') as f:\n",
    "            activity_df = pd.read_parquet(f)\n",
    "            self.activity_df = activity_df\n",
    "        \n",
    "        # ? Yu: will the :meth:`sparse.coo_matrix` only keep the non-zero values? If so, only the active compounds (where activity  is not 0) will be kept?\n",
    "        self.activity = sparse.coo_matrix(\n",
    "            (\n",
    "                activity_df['activity'],# activity is the value\n",
    "                (activity_df['compound_idx'], activity_df['assay_idx']) # compound in row, assay in column.\n",
    "            ),\n",
    "            shape=(self.num_compounds, self.num_assays),\n",
    "        )\n",
    "    \n",
    "    def _load_compound(self, compound_mode=None):\n",
    "        cmpfn = f'compound_features{\"_\"+compound_mode if compound_mode else \"\"}'\n",
    "        #?Yu: if 'graph' is not used, remove the below code\n",
    "        if 'graph' in compound_mode and (not 'graphormer' in compound_mode):\n",
    "            logger.info(f'graph in compound mode: loading '+cmpfn)\n",
    "            import dgl\n",
    "            from dgl.data.utils import load_graphs\n",
    "            compound_features = load_graphs(str(self.root/(cmpfn+\".bin\")))[0]\n",
    "            compound_features = np.array(compound_features)\n",
    "        elif compound_mode == 'smiles':\n",
    "            compound_features = pd.read_parquet(self.root/('compound_smiles.parquet'))['CanonicalSMILES'].values\n",
    "        else:\n",
    "            try: #tries to open npz files else npy\n",
    "                with open(self.root/(cmpfn+\".npz\"), 'rb') as f:\n",
    "                    compound_features = sparse.load_npz(f)\n",
    "            except:\n",
    "                logger.info(f'loading '+cmpfn+'.npz failed, using .npy instead')\n",
    "                try:\n",
    "                    compound_features = np.load(self.root/(cmpfn+\".npy\"))\n",
    "                except:\n",
    "                    logger.info(f'loading '+cmpfn+'.npy failed, trying to compute it on the fly')\n",
    "                    compound_features = pd.read_parquet(self.root/('compound_smiles.parquet'))\n",
    "        return compound_features\n",
    "    \n",
    "    def _load_assay(self, assay_mode='lsa') -> None: #? Yu: 'lsa'\n",
    "        \"\"\" loads assay \"\"\"\n",
    "        if assay_mode =='':\n",
    "            print('no assay features')\n",
    "            return None\n",
    "        \n",
    "        #? Yu: if the below assay modes are not used, remove them.\n",
    "        if assay_mode == 'biobert-last':\n",
    "            with open(self.root/('assay_features_dmis-lab_biobert-large-cased-v1.1_last_layer.npy'), 'rb') as f:\n",
    "                return np.load(f, allow_pickle=True)\n",
    "        elif assay_mode == 'biobert-two-last':\n",
    "            with open(self.root/('assay_features_dmis-lab_biobert-large-cased-v1.1_penultimate_and_last_layer.npy'), 'rb') as f:\n",
    "                return  np.load(f, allow_pickle=True)\n",
    "        \n",
    "        # load the prepared assay features\n",
    "        try: # tries to open npz file else npy\n",
    "            with open(self.root/(f'assay_features_{assay_mode}.npz'), 'rb') as f:\n",
    "                return sparse.load_npz(f)\n",
    "        except:\n",
    "            with open(self.root/(f'assay_features_{assay_mode}.npy'), 'rb') as f:\n",
    "                return np.load(f, allow_pickle=True)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _find_splits(self) -> None:\n",
    "        \"\"\"\n",
    "        We assume that during the preparation of the PubChem data, compounds(assays) have been indexed \n",
    "        so that a larger compound(assay) index corresponds to a compound(assay) incorporated to PubChem later in time.\n",
    "        This function finds the compound(assay) index cut-points to create three chronological disjoint splits.\n",
    "\n",
    "        The oldest `train_size` fraction of compounds(assays) are assigned to training. \n",
    "        From the remaining compounds(assays), the oldest half are assigned to vailidation, and the newest half are assigned to test.\n",
    "        Only the index cut points are stored.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            logger.info(f'Find split cut-points for compound and assay indices (train_size={self.train_size}).')\n",
    "\n",
    "        first_cut, second_cut = self._chunk(self.num_compounds, self.train_size)\n",
    "        self.compound_cut = {'train': first_cut, 'valid': second_cut}\n",
    "\n",
    "        first_cut, second_cut = self._chunk(self.num_assays, self.train_size)\n",
    "        self.assay_cut = {'train': first_cut, 'valid': second_cut}\n",
    "\n",
    "    def _chunk(n:int, first_cut_ratio:float) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Find the two cut points required to chunk a sequence of `n` items into three parts, \n",
    "        the first having `first_cut_ratio` of the items, \n",
    "        the second and the third having approximately the half of the remaining items.\n",
    "\n",
    "        Params\n",
    "        -------\n",
    "        n: int\n",
    "            Length of the sequence to chunk.\n",
    "        first_cut_ratio: float\n",
    "            Portion of items in the first chunk. This is the `train_size`\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int, int\n",
    "            Positions where the first and second cut occurs.\n",
    "        \"\"\"\n",
    "        first_cut = int(round(first_cut_ratio * n))\n",
    "        second_cut = first_cut + int(round((n - first_cut) / 2))\n",
    "\n",
    "        return first_cut, second_cut\n",
    "\n",
    "    def subset(\n",
    "            self, \n",
    "            c_low: Optional[int] = None,\n",
    "            c_high: Optional[int] = None,\n",
    "            a_low: Optional[int] = None,\n",
    "            a_high: Optional[int] = None,\n",
    "    ) -> np.ndarray:\n",
    "        if c_low is None: # sef the compound low index to 0\n",
    "            c_low = 0\n",
    "        if c_high is None: # set the compound high index to the number of compounds\n",
    "            c_high = self.num_compounds\n",
    "        if a_low is None: # set the assay low index to 0\n",
    "            a_low = 0\n",
    "        if a_high is None: # set the assay high index to the number of assays\n",
    "            a_high = self.num_assays\n",
    "\n",
    "        if self.verbose:\n",
    "            logger.info(f'Find activity triplets where {c_low} <= compound_idx <= {c_high} and {a_low} <= assay_idx <= {a_high}.')\n",
    "        \n",
    "        activity_bool = np.logical_and.reduce( # take multiple Boolean conditions and combines them using logical AND across all conditions.\n",
    "            (\n",
    "                self.activity.row >= c_low,\n",
    "                self.activity.row < c_high,\n",
    "                self.activity.col >= a_low,\n",
    "                self.activity.col < a_high\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return np.flatnonzero(activity_bool) # applies the logical condition to the COO matrix and returns the indices that satisfy the condition.\n",
    "\n",
    "    def get_unique_names(\n",
    "            self, \n",
    "            activity_idx: Union[int, Iterable[int], slice]\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the unique compound and assay names within the `activity` triplets  indexed by `activity_idx` in default, COO style.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        activity_idx: int, iterable of int, slice\n",
    "            Index to one or multiple `activity` triplets.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        compound_names: :class:`pandas.DataFrame`\n",
    "        assay_names: :class:`pandas.DataFrame`\n",
    "        \"\"\"\n",
    "\n",
    "        compound_idx = self.activity.row[activity_idx]\n",
    "        assay_idx = self.activity.col[activity_idx]\n",
    "\n",
    "        if isinstance(compound_idx, np.ndarray) and isinstance(assay_idx, np.ndarray):\n",
    "            compound_idx = pd.unique(compound_idx)\n",
    "            assay_idx = pd.unique(assay_idx)\n",
    "        \n",
    "        elif isinstance(compound_idx, (int, np.integer)) and isinstance(assay_idx, (int, np.integer)):\n",
    "            pass # a single index means a single compound and assay, so no need to do anything.\n",
    "\n",
    "        else:\n",
    "            raise ValueError('activity_idx must be an int, iterable of int, or slice.')\n",
    "\n",
    "        compound_names = self.compound_names.iloc[compound_idx]\n",
    "        assay_names = self.assay_names.iloc[assay_idx]\n",
    "\n",
    "        return compound_names.sort_index(), assay_names.sort_index() # sort the names alphabetically\n",
    "\n",
    "    def getitem(\n",
    "            self,\n",
    "            activity_idx: Union[int, Iterable[int], slice],\n",
    "            ret_np=False\n",
    "    ) -> Tuple[Any, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        activity_idx: int, iterable of int, slice\n",
    "            Specifies the indices of the activity triplets to retrieve.\n",
    "        ret_np: bool\n",
    "            Determines the format of the returned data. If True, returns numpy arrays; If False, returns PyTorch tensors.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of :class:`torch.Tensor`\n",
    "        - `activity_idx`: the original indices provided as input. This will enable to reconstruct the order in which the dataset has been visited.\n",
    "        - `compound_features`: shape(len(activity_idx), compound_feature_size)\n",
    "        - `assay_features`: shape(len(activity_idx), assay_feature_size)\n",
    "        - `activity`: shape(len(activity_idx), ).\n",
    "        \"\"\"\n",
    "        compound_idx = self.activity.row[activity_idx]\n",
    "        assay_idx = self.activity.col[activity_idx]\n",
    "        activity = self.activity.data[activity_idx]\n",
    "\n",
    "        # ===== get compound_features =====\n",
    "        if isinstance(self.compound_features, pd.DataFrame):\n",
    "            compound_smiles = self.compound_features.iloc[compound_idx]['CanonicalSMILES'].values\n",
    "            from datacat4ml.Scripts.data_prep.data_featurize.compound_featurize.encode_compound  import convert_smiles_to_fp\n",
    "            if self.compound_mode == 'MxFP':\n",
    "                fptype = 'maccs+morganc+topologicaltorsion+erg+atompair+pattern+rdkc+mhfp+rdkd'\n",
    "            else:\n",
    "                fptype = self.compound_mode\n",
    "            # Todo: fp_size as input parameter\n",
    "            fp_size = 40000 #? Yu\n",
    "            compound_features = convert_smiles_to_fp(compound_smiles, fp_size=fp_size, which=fptype, radius=2, njobs=1).astype(np.float32)\n",
    "        else:\n",
    "            compound_features = self.compound_features[compound_idx]\n",
    "            if isinstance(compound_features, sparse.csr_matrix):\n",
    "                compound_features = compound_features.toarray()\n",
    "        \n",
    "\n",
    "        # ===== get assay_features =====\n",
    "        assay_features = self.assay_features[assay_idx]\n",
    "        if isinstance(assay_features, sparse.csr_matrix):\n",
    "            assay_features = assay_features.toarray()\n",
    "        \n",
    "        #? Yu: if not used, remove the below code\n",
    "        try:\n",
    "            assay_onehot = self.assay_onehot[assay_idx].toarray()\n",
    "        except (TypeError, ValueError):\n",
    "            assay_onehot = np.zeros_like(assay_features)\n",
    "        \n",
    "        # ===== Handle single indices =====\n",
    "        # If `activity_idx`is a single integer or a list with only one element, the retrieved feature vectors are reshaped into 1D arrays to maintain the consistency of the output format.\n",
    "        if isinstance(activity_idx, (int, np.integer)):\n",
    "            compound_features = compound_features.reshape(-1) \n",
    "            assay_features = assay_features.reshape(-1) \n",
    "            assay_onehot = assay_onehot.reshape(-1) \n",
    "            activity = [activity]\n",
    "        elif isinstance(activity_idx, list):\n",
    "            if len(activity_idx) == 1:\n",
    "                compound_features = compound_features.reshape(-1)\n",
    "                assay_features = assay_features.reshape(-1)\n",
    "                assay_onehot = assay_onehot.reshape(-1)\n",
    "        activity = np.array(activity)\n",
    "\n",
    "        # ===== Return =====\n",
    "        # return the data as Numpy arrays.\n",
    "        if ret_np:\n",
    "            return(\n",
    "                activity_idx,\n",
    "                compound_features, #already float32\n",
    "                assay_features if not isinstance(assay_features[0], str) else assay_features, # already float32\n",
    "                assay_onehot if not isinstance(assay_onehot[0], str) else assay_onehot, # already float32\n",
    "                (float(activity)) # torch.nn.BCEWithLogitsLoss needs this to be float too...\n",
    "            )\n",
    "\n",
    "        # return the data as PyTorch tensors.\n",
    "        if self.compound_mode == 'smiles':\n",
    "            comp_feat = compound_features\n",
    "        elif isinstance(compound_features, np.ndarray):\n",
    "            comp_feat = torch.from_numpy(compound_features)\n",
    "        elif not isinstance(compound_features[0], dgl.DGLGraph):\n",
    "            comp_feat = dgl.batch(compound_features)\n",
    "        else:\n",
    "            comp_feat = compound_features\n",
    "\n",
    "        return  (\n",
    "            activity_idx, \n",
    "            comp_feat, # alread float32\n",
    "            torch.from_numpy(assay_features) if not isinstance(assay_features[0], str) else assay_features, # already float32\n",
    "            torch.from_numpy(assay_onehot.astype(int)) if not isinstance(assay_onehot[0], str) else assay_onehot, # already float32\n",
    "            torch.from_numpy(activity).float() # torch.nn.BCEWithLogitsLoss needs this to be float too...\n",
    "        )\n",
    "\n",
    "    def getitem_meta_assay(\n",
    "            self,\n",
    "            compound_idx: Union[int, List[int], slice]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        For a given compound (or list), retrieve the data in the `meta-assay` style, \n",
    "        which involves summarizing assay feature vectors (positive and negative) for each compound.\n",
    "        \n",
    "        Params\n",
    "        -------\n",
    "        compound_idx: int, iterable of int, slice\n",
    "            Index to one or multiple compounds.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of :class:`torch.Tensor`\n",
    "        - `compound_features`: shape(N, compound_feature_size)\n",
    "        - `assay_features`: shape(N, assay_feature_size)\n",
    "        - `activity`: shape(N, )\n",
    "        \"\"\"\n",
    "        \n",
    "        # extract the data for the specified compounds\n",
    "        activity_slice = self.activity.tocsr()[compound_idx] \n",
    "\n",
    "        # find non-empty rows\n",
    "        # `activity_slic.indptr`: pointer to the start of each row in the sparse matrix.\n",
    "        # `np.diff(activity_slice.indptr)`: measures the number of elements in each row.\n",
    "        # `np.where(...!=0)`: finds rows that contain non-zero elements. (i.e., rows with at least one assay-related to the compound)`\n",
    "        non_empty_row_idx = np.where(np.diff(activity_slice.indptr)!=0)[0] #?\n",
    "\n",
    "        # initialize containers for results\n",
    "        compound_features_l = [] # list of compound features\n",
    "        assay_positive_features_l, assay_negative_features_l = [], [] # averaged features of positive assays, and negative assays\n",
    "        activity_l = [] # activity lables\n",
    "\n",
    "        # process each non-empty row\n",
    "        for row_idx in non_empty_row_idx:\n",
    "            positive_l, negative_l  = [], []\n",
    "            for col_idx, activity in get_sparse_indices_and_data(activity_slice, row_idx):\n",
    "                if activity == 0:\n",
    "                    negative_l.append(self.assay_features[col_idx]) \n",
    "                else:\n",
    "                    positive_l.append(self.assay_features[col_idx])\n",
    "            \n",
    "            if len(negative_l) > 0:\n",
    "                compound_features_l.append(self.compound_features[row_idx])\n",
    "                negative = np.vstack(negative_l).mean(axis=0)\n",
    "                assay_negative_features_l.append(negative)\n",
    "                activity_l.append(0)\n",
    "            \n",
    "            if len(positive_l) > 0:\n",
    "                compound_features_l.append(self.compound_features[row_idx])\n",
    "                positive = np.vstack(positive_l).mean(axis=0)\n",
    "                assay_positive_features_l.append(positive)\n",
    "                activity_l.append(1)\n",
    "\n",
    "        compound_features = sparse.vstack(compound_features_l).toarray()\n",
    "        assay_features_l = np.vstack(\n",
    "            assay_negative_features_l + assay_positive_features_l # '+' is used to concatenate the two lists\n",
    "        )\n",
    "\n",
    "        activity = np.array(activity_l)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(compound_features), # already float32\n",
    "            torch.from_numpy(assay_features_l), # already float32\n",
    "            torch.from_numpy(activity).float() # torch.nn.BCEWithLogitsLoss needs this to be float too...\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch_as_list:list) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Necessary for :meth:`getitem_meta_assay` if using a :class:`torch.utils.data.DataLoader`.\n",
    "        Not necessaryif using :class:`torch.utils.data.BatchSampler`, as I typically do.\n",
    "\n",
    "        Params\n",
    "        -------\n",
    "        batch_as_list: list\n",
    "            Result of :meth:`getitem_meta_assay` for a mini-batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of :class:`torch.Tensor`\n",
    "            Data for a mini-batch.\n",
    "        \"\"\"\n",
    "        compound_features_t, assay_features_t, activty_t = zip(*batch_as_list)\n",
    "        return(\n",
    "            torch.cat(compound_features_t, dim=0),\n",
    "            torch.cat(assay_features_t, dim=0),\n",
    "            torch.cat(activty_t, dim=0)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx: Union[int, Iterable[int], slice]) -> Tuple:\n",
    "        \"\"\"\n",
    "        Index or slice `activity` by `idx`. The indexing mode depends on the value of `meta_assays`. \n",
    "        If False(default), the indexing is over COO triplets.\n",
    "        If True, the indexing is over unique compounds.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.meta_assays:\n",
    "            return self.getitem_meta_assay(idx)\n",
    "        else:\n",
    "            return self.getitem(idx)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "\n",
    "        - If `meta_assays` is False (default), length is defined as the number of `(compound, assay, activity)` COO triplets.\n",
    "        - If `meta_assays` is True, length is defined as the number of the unique compounds.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.meta_assays:\n",
    "            return self.num_compounds\n",
    "        else:\n",
    "            return self.activity.nnz # the number of non-zero elements in the sparse matrix\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'InMemoryClamp\\n' \\\n",
    "               f'\\troot=\"{self.root}\"\\n' \\\n",
    "               f'\\tassay_mode=\"{self.assay_mode}\"\\n' \\\n",
    "               f'\\ttrain_size={self.train_size}\\n' \\\n",
    "               f'\\tactivity.shape={self.activity.shape}\\n' \\\n",
    "               f'\\tactivity.nnz={self.activity.nnz}\\n' \\\n",
    "               f'\\tmeta_assays={self.meta_assays}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ccd7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /storage/homefs/yc24j783/datacat4ml/datacat4ml/Data/data_prep/data_split/fsmol_alike/MHDsFold\n"
     ]
    }
   ],
   "source": [
    "self_root = f'{SPLIT_DATA_DIR}/fsmol_alike/MHDsFold'\n",
    "print(f'Loading dataset from {self_root}')\n",
    "\n",
    "with open(os.path.join(self_root, 'compound_names.parquet'), 'rb') as f:\n",
    "            self_compound_names = pd.read_parquet(f)\n",
    "self_num_compounds = len(self_compound_names)\n",
    "\n",
    "with open(os.path.join(self_root, 'assay_info.parquet'), 'rb') as f:\n",
    "            self_assay_names = pd.read_parquet(f)\n",
    "self_num_assays = len(self_assay_names)\n",
    "\n",
    "with open(os.path.join(self_root, 'activity.parquet'), 'rb') as f:\n",
    "            activity_df = pd.read_parquet(f)\n",
    "            self_activity_df = activity_df\n",
    "            \n",
    "# ? Yu: will the :meth:`sparse.coo_matrix` only keep the non-zero values? If so, only the active compounds (where activity  is not 0) will be kept?\n",
    "activity = sparse.coo_matrix(\n",
    "    (\n",
    "        activity_df['activity'],# activity is the value\n",
    "        (activity_df['compound_idx'], activity_df['assay_idx']) # compound in row, assay in column.\n",
    "    ),\n",
    "    shape=(self_num_compounds, self_num_assays),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0a514",
   "metadata": {},
   "source": [
    "# `utils.py / def train_and_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c38f5d",
   "metadata": {},
   "source": [
    "# `train.py / def main(args)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b0bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
