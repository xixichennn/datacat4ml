{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: pyg (Python3.9.16)\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.cluster import SpectralClustering\n",
    "#from OpioML.Scripts.data_prep.data_split.cliff import ActivityCliffs, get_tanimoto_matrix\n",
    "\n",
    "from datacat4ml.utils import mkdirs, get_df_name\n",
    "from datacat4ml.const import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the categorized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_spliter(smiles: List[str], bioactivity: List[float], active: List[float],\n",
    "               in_log10: bool=True, similarity: float=0.9, potency_fold: int=10,\n",
    "               use_clustering: bool=True, n_clusters: int=5, test_size: float=0.2, \n",
    "               task: str='cls'):\n",
    "    \"\"\"\n",
    "    Split the data into train and test sets according to activity cliffs and compound charateristics.\n",
    "\n",
    "    :param smiles: List of SMILES strings\n",
    "    :param bioactivity: List of bioactivity values\n",
    "    :param active: List of active/inactive labels\n",
    "    :param in_log10: Whether the bioactivity values are in log10\n",
    "    :param similarity: Threshold value to determine structural similarity\n",
    "    :param potency_fold: Threshold value to determine potency difference\n",
    "    :param test_size: Test set size\n",
    "    :param use_clustering: Whether to use clustering to split the data\n",
    "    :param n_clusters: Number of clusters to use if clustering is used\n",
    "\n",
    "    :return: A dataframe \n",
    "    \"\"\"\n",
    "\n",
    "    if len(smiles) < 50:\n",
    "        print(f\"The number of data points in this file is lower than 50, which is not enough to build a good machine learning model. Skipping this file.\")\n",
    "        return None\n",
    "\n",
    "    if not in_log10:\n",
    "        bioactivity = (-np.log10(bioactivity)).tolist()\n",
    "    \n",
    "    # get activity cliffs\n",
    "    cliffs = ActivityCliffs(smiles, bioactivity)\n",
    "    cliff_mols = cliffs.get_cliff_molecules(return_smiles=False, similarity=similarity, potency_fold=potency_fold)\n",
    "\n",
    "    if use_clustering:\n",
    "        # # cluster the dabaset into 5 clusters based on tanimoto distance matrix\n",
    "        spectral = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=RANDOM_SEED)\n",
    "        clusters = spectral.fit(get_tanimoto_matrix(smiles)).labels_ # get the cluster labels for each molecule\n",
    "\n",
    "        train_idx, test_idx = [], []\n",
    "        for cluster in range(n_clusters):\n",
    "            num_in_cluster = len(np.where(clusters == cluster)[0])\n",
    "            print(f\"Cluster {cluster}: {num_in_cluster} data points\")\n",
    "\n",
    "            # get the indices of molecules in the current cluster\n",
    "            cluster_idx = np.where(clusters == cluster)[0] # `[0]` is need to convert the tuple to a list\n",
    "            clust_cliff_mols = [cliff_mols[i] for i in cluster_idx]\n",
    "            \n",
    "            # can only split data stratified by cliff molecules if there are more than 1 cliff molecules in the cluster, else just split randomly\n",
    "            if sum(clust_cliff_mols) > 2:\n",
    "                try: # try to avoid the Value ERROR when the number of class in the training set is less than 2\n",
    "                    clust_train_idx, clust_test_idx = train_test_split(cluster_idx, test_size=test_size, \n",
    "                                                                        stratify=clust_cliff_mols, # ensure that the train/test split has the same proportion of cliff molecules\n",
    "                                                                        random_state=RANDOM_SEED, shuffle=True)\n",
    "                except ValueError:\n",
    "                    # This block will be executed if an error occurs in the try block\n",
    "                    print(\"An error occurred while stratifying data based on cliff molecules. Skipping stratification.\")\n",
    "                    clust_train_idx, clust_test_idx = train_test_split(cluster_idx, test_size=test_size, \n",
    "                                                                        random_state=RANDOM_SEED, shuffle=True)\n",
    "            else:\n",
    "                clust_train_idx, clust_test_idx = train_test_split(cluster_idx, test_size=test_size, \n",
    "                                                                    random_state=RANDOM_SEED, shuffle=True)\n",
    "            train_idx.extend(clust_train_idx)\n",
    "            test_idx.extend(clust_test_idx)\n",
    "    else:\n",
    "        # don't use clustering before spliting. This is to avoid potential cheating by using clustering to split the data, \n",
    "        # which make the structual diversity of the train and test sets the same\n",
    "        if sum(cliff_mols) > 2:\n",
    "            train_idx, test_idx = train_test_split(range(len(smiles)), test_size=test_size, \n",
    "                                            stratify=cliff_mols, # ensure that the train/test split has the same proportion of cliff molecules\n",
    "                                            random_state=RANDOM_SEED, shuffle=True)\n",
    "        else:\n",
    "            train_idx, test_idx = train_test_split(range(len(smiles)), test_size=test_size, \n",
    "                                            random_state=RANDOM_SEED, shuffle=True)\n",
    "    \n",
    "    train_test = []\n",
    "    for i in range(len(smiles)):\n",
    "        if i in train_idx:\n",
    "            train_test.append('train')\n",
    "        elif i in test_idx:\n",
    "            train_test.append('test')\n",
    "        else:\n",
    "            raise ValueError('Index not in train or test set')\n",
    "        \n",
    "    if task == 'cls':\n",
    "        return pd.DataFrame({'smiles': smiles,\n",
    "                            'exp_mean [nM]': (10**abs(np.array(bioactivity))).tolist(),\n",
    "                            'pstandard_value_mean': bioactivity,\n",
    "                            'cliff_mol': cliff_mols,\n",
    "                            'y(active)': active,\n",
    "                            'split': train_test})\n",
    "\n",
    "        \n",
    "    elif task == 'reg':\n",
    "        return pd.DataFrame({'smiles': smiles,\n",
    "                             'exp_mean [nM]': (10**abs(np.array(bioactivity))).tolist(),\n",
    "                             'y(pstandard_value_mean)':bioactivity,\n",
    "                             'cliff_mol': cliff_mols,\n",
    "                             'active':active,\n",
    "                             'split':train_test})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(filepath = FETCH_DATA_DIR, task:str = 'cls', confidence_score:int = 8,\n",
    "               use_clustering: bool=True):\n",
    "    \n",
    "    # access the final csv files obtained from data curation\n",
    "    folder_path = os.path.join(filepath, 'curated', task, 'confidence_score'+'_'+str(confidence_score))\n",
    "    files = os.listdir(folder_path)\n",
    "    final_files = [file for file in files if file.endswith('_final.csv')]\n",
    "\n",
    "    # make new directory to store the featurized data\n",
    "    new_path = os.path.join(filepath, 'splited', task, 'confidence_score'+'_'+str(confidence_score), 'use_clustering'+'_'+str(use_clustering))\n",
    "    mkdirs(new_path)\n",
    "\n",
    "    for final_file in final_files:\n",
    "        print (f\"final_file is: {final_file}\\n\")\n",
    "\n",
    "        df = pd.read_csv(os.path.join(folder_path, final_file))\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "        # split the data into train and test sets\n",
    "        data_splited_df = data_spliter(df['CuratedSmiles'].tolist(), df['pstandard_value_mean'].tolist(), df['active'].tolist(), \n",
    "                          task=task, use_clustering=use_clustering)\n",
    "        \n",
    "\n",
    "        # save data_splited_df as a csv file if it is not None\n",
    "        if data_splited_df is not None:\n",
    "            data_splited_df.to_csv(os.path.join(new_path, final_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_clusterings = [True, False]\n",
    "\n",
    "for filepath in File_paths:\n",
    "    print (f\"filepath is: {filepath}\\n\")\n",
    "\n",
    "    for task in Tasks:\n",
    "        print (f\"task is: {task}\\n\")\n",
    "\n",
    "        for confidence_score in Confidence_scores:\n",
    "            print (f\"confidence_score is: {confidence_score}\\n\")\n",
    "\n",
    "            for use_clustering in use_clusterings:\n",
    "                print (f\"use_clustering is: {use_clustering}\\n\")\n",
    "                \n",
    "                split_data(filepath, task, confidence_score, use_clustering=use_clustering)\n",
    "                print(f'Done!\\n====================================\\n')\n",
    "#It took around 1 hour to run the above code locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
