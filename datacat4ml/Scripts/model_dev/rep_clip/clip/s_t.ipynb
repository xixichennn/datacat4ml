{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: pyg (Python 3.9.16)\n",
    "import gzip\n",
    "import html\n",
    "import os\n",
    "from functools import lru_cache \n",
    "\n",
    "import ftfy\n",
    "import regex as re\n",
    "from pathlib import Path\n",
    "\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def default_bpe():\n",
    "    \"\"\"\n",
    "    The default BPE vocabulary file.\n",
    "    \"\"\"\n",
    "    current_dir= os.getcwd()\n",
    "    return os.path.join(current_dir, 'bpe_simple_vocab_16e6.txt.gz')\n",
    "\n",
    "# Hash the below function, because `__file__` is not available in Jupyter notebooks\n",
    "#@lru_cache()\n",
    "#def default_bpe():\n",
    "#    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1)) + list(range(ord(\"¡\"), ord(\"¬\")+1)) + list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = default_bpe()):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'</w>' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[602, 10099, 6206, 536, 8768, 22833, 41835, 20777, 530, 5150, 8890, 44160, 601, 512, 24658, 539, 645, 909, 18283, 268, 16434, 943, 2877, 4146, 638, 11807, 3750, 587, 1451]\n",
      "agonist activity at delta opioid receptor expressed in cho cells assessed as inhibition of forskolin - stimulated camp production by alphascreen assay \n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "# Encode a text string\n",
    "encoded_tokens = tokenizer.encode(\"Agonist activity at delta opioid receptor expressed in CHO cells assessed as inhibition of forskolin-stimulated cAMP production by ALPHAscreen assay\")\n",
    "print(encoded_tokens)\n",
    "\n",
    "# Decode the token indices back to text\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21287, 41451, 536, 2751, 22833, 41835, 8768, 272, 739, 14371, 638, 1996, 314, 274, 327, 316, 570, 679, 1062, 40985, 7176, 19915, 537, 530, 5150, 5533, 4937, 3684, 542, 536, 320, 18565, 539, 271, 269, 279, 11370]\n",
      "binding affinity at human opioid receptor delta 1 was determined by using [ 3 h ] diprenorphine radioligand in cho cell membranes at a concentration of 0 . 8 nm \n"
     ]
    }
   ],
   "source": [
    "# Encode a text string\n",
    "encoded_tokens = tokenizer.encode(\"Binding affinity at human opioid receptor delta 1 was determined by using [3H]diprenorphine radioligand in CHO cell membranes at a concentration of 0.8 nM\")\n",
    "print(encoded_tokens)\n",
    "\n",
    "# Decode the token indices back to text\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37931, 539, 314, 274, 327, 316, 2092, 633, 22833, 41835, 8768, 272, 530, 18537, 9619, 4812, 34088]\n",
      "displacement of [ 3 h ] ek from opioid receptor delta 1 in guinea pig brain membrane \n"
     ]
    }
   ],
   "source": [
    "# Encode a text string\n",
    "encoded_tokens = tokenizer.encode(\"Displacement of [3H]EK from Opioid receptor delta 1 in guinea pig brain membrane\")\n",
    "print(encoded_tokens)\n",
    "\n",
    "# Decode the token indices back to text\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
