{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SMILESTokenizer:\n",
    "\n",
    "    \"\"\"\n",
    "    Shared by Markus.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\!|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        self.vocab = {}\n",
    "        self.inv_vocab = {}\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.max_len = None\n",
    "    def tokenize(self, smiles):\n",
    "        \"\"\"Tokenizes a SMILES string using the predefined regular expression.\"\"\"\n",
    "        return re.findall(self.pattern, smiles)\n",
    "    def build_vocab(self, smiles_list):\n",
    "        \"\"\"Builds vocabulary from a list of SMILES strings.\"\"\"\n",
    "        all_tokens = set()\n",
    "        for smiles in smiles_list:\n",
    "            tokens = self.tokenize(smiles)\n",
    "            all_tokens.update(tokens)\n",
    "        tokens = [self.pad_token, self.unk_token, self.start_token, self.end_token]\n",
    "        all_tokens = sorted(all_tokens)\n",
    "        all_tokens = tokens + all_tokens\n",
    "        self.vocab = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "        self.inv_vocab = {idx: token for token, idx in self.vocab.items()}\n",
    "    def encode(self, smiles, max_len=None):\n",
    "        \"\"\"Encodes a SMILES string into a list of token indices, optionally padding to max_len.\"\"\"\n",
    "        tokens = self.tokenize(smiles)\n",
    "        tokens = [self.start_token] + tokens + [self.end_token]\n",
    "        token_ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        if max_len:\n",
    "            token_ids = token_ids[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(token_ids))\n",
    "        return token_ids\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decodes a list of token indices back into a SMILES string.\"\"\"\n",
    "        tokens = [self.inv_vocab.get(token_id, self.unk_token) for token_id in token_ids]\n",
    "        tokens = [token for token in tokens if token not in [self.start_token, self.end_token, self.pad_token]]\n",
    "        return ''.join(tokens)\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Returns the size of the vocabulary.\"\"\"\n",
    "        return len(self.vocab)\n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        \"\"\"Pads a sequence to the maximum length.\"\"\"\n",
    "        return sequence[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SMILESEncoder(nn.Module):\n",
    "    def __init__(self, tokenizer, embed_dim, max_len=128):\n",
    "        super(SMILESEncoder, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size(), embed_dim)\n",
    "        \n",
    "        # Transformer encoder for SMILES\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8),\n",
    "            num_layers=6\n",
    "        )\n",
    "\n",
    "    def forward(self, smiles):\n",
    "        # Tokenize and encode the SMILES strings\n",
    "        encoded = [self.tokenizer.encode(s, max_len=self.max_len) for s in smiles]\n",
    "        padded = torch.tensor(encoded).to(next(self.parameters()).device)\n",
    "\n",
    "        # Pass through embedding layer\n",
    "        embeddings = self.embedding(padded)\n",
    "\n",
    "        # Process embeddings through the transformer\n",
    "        embeddings = embeddings.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, embed_dim)\n",
    "        transformed = self.transformer(embeddings)\n",
    "\n",
    "        # Pooling: Use the mean of the sequence outputs\n",
    "        pooled_output = transformed.mean(dim=0)\n",
    "        return pooled_output\n",
    "\n",
    "class SMILESCLIP(nn.Module):\n",
    "    def __init__(self, smiles_encoder, text_encoder, projection_dim):\n",
    "        super(SMILESCLIP, self).__init__()\n",
    "        self.smiles_encoder = smiles_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "        # Projection heads\n",
    "        self.smiles_projection = nn.Linear(smiles_encoder.embed_dim, projection_dim)\n",
    "        self.text_projection = nn.Linear(text_encoder.config.hidden_size, projection_dim)\n",
    "\n",
    "    def forward(self, smiles, text):\n",
    "        # Encode SMILES and text\n",
    "        smiles_features = self.smiles_encoder(smiles)\n",
    "        text_features = self.text_encoder(text)[\"pooler_output\"]\n",
    "\n",
    "        # Project features to the same dimensional space\n",
    "        smiles_embeddings = self.smiles_projection(smiles_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Normalize embeddings\n",
    "        smiles_embeddings = nn.functional.normalize(smiles_embeddings, p=2, dim=1)\n",
    "        text_embeddings = nn.functional.normalize(text_embeddings, p=2, dim=1)\n",
    "\n",
    "        return smiles_embeddings, text_embeddings\n",
    "\n",
    "# Initialize tokenizer\n",
    "smiles_tokenizer = SMILESTokenizer()\n",
    "\n",
    "# Build vocabulary (example SMILES dataset needed)\n",
    "example_smiles_list = [\"CCO\", \"C1=CC=CC=C1\", \"O=C(O)C(O)\"]\n",
    "smiles_tokenizer.build_vocab(example_smiles_list)\n",
    "\n",
    "# Initialize models\n",
    "smiles_encoder = SMILESEncoder(smiles_tokenizer, embed_dim=512)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Combine into a CLIP-like model\n",
    "model = SMILESCLIP(smiles_encoder, text_encoder, projection_dim=512).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Example training loop\n",
    "def train_step(batch, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    smiles, text, labels = batch  # Example batch structure\n",
    "\n",
    "    # Forward pass\n",
    "    smiles_embeddings, text_embeddings = model(smiles, text)\n",
    "\n",
    "    # Compute logits\n",
    "    logits_per_smiles = smiles_embeddings @ text_embeddings.T\n",
    "    logits_per_text = logits_per_smiles.T\n",
    "\n",
    "    # Compute loss\n",
    "    ground_truth = torch.arange(len(smiles)).to(logits_per_smiles.device)\n",
    "    loss = (criterion(logits_per_smiles, ground_truth) + criterion(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Example usage\n",
    "batch = ([\"CCO\", \"C1=CC=CC=C1\"], [\"ethanol\", \"benzene\"], None)  # Dummy batch\n",
    "loss = train_step(batch, model, optimizer, criterion)\n",
    "print(f\"Training loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
