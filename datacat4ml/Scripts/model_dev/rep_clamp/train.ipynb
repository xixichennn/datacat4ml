{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/miniconda3/envs/pyg/lib/python3.9/site-packages/mlflow/protos/service_pb2.py:11: UserWarning: google.protobuf.service module is deprecated. RPC implementations should provide code generator plugins which generate code specific to the RPC implementation. service.py will be removed in Jan 2025\n",
      "  from google.protobuf import service as _service\n"
     ]
    }
   ],
   "source": [
    "#################   Yu's ##########################\n",
    "#--> Yu's < from clamp.dataset import InMemoryClamp\n",
    "#--> Yu's < from clamp import utils\n",
    "#--> Yu's < from clamp.utils import set_device\n",
    "\n",
    "#################   Yu's ##########################\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "import argparse\n",
    "import wandb\n",
    "from time import time\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"example call:\n",
    "python clamp/train.py \\\n",
    "    --dataset=./data/fsmol \\\n",
    "    --split=FSMOL_split \\\n",
    "    --assay_mode=clip \\\n",
    "    --compound_mode=morganc+rdkc \n",
    "\"\"\"\n",
    "\n",
    "\"\"\" training pubchem23 without downstream datasets\n",
    "python clamp/train.py \\\n",
    "    --dataset=./data/pubchem23/ \\\n",
    "    --split=time_a \\\n",
    "    --assay_mode=clip \\\n",
    "    --batch_size=8192 \\\n",
    "    --dropout_hidde=0.3 \\\n",
    "    --drop_cidx_path=./data/pubchem23/cidx_overlap_moleculenet.npy \\\n",
    "    --train_subsample=10e6 \\\n",
    "    --wandb \\\n",
    "    --experiment=pretrain\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args_override(override_hpjson=True):\n",
    "    parser = argparse.ArgumentParser('Train and test a single run of clamp-Activity model. Overrides arguments from hyperparam-file')\n",
    "    parser.add_argument('-f', type=str)\n",
    "    #--> Yu's < default='./data/fsmol/'\n",
    "    parser.add_argument('--dataset', type=str, default='./data/fsmol/', help='Path to a prepared dataset directory.')\n",
    "    #--> Yu's < assay_features (clip trained by GPCR assay corups)\n",
    "    parser.add_argument('--assay_mode', type=str, default='lsa', help='Type of assay features (\"clip\", \"biobert\" or \"lsa\").')\n",
    "    #--> Yu's < default='morganc+rdkc' --> SMILESTokenizer\n",
    "    parser.add_argument('--compound_mode', type=str, default='morganc+rdkc', help='Type of compound features (default: morganc+rdkc)')\n",
    "    #--> Yu's < default='.hparams/default.json'\n",
    "    parser.add_argument('--hyperparams',type=str, default='./hparams/default.json', help='Path to hyperparameters to use in training (json, Hyperparams, or logs).')\n",
    "    \n",
    "    parser.add_argument('--checkpoint', help='Path to a model-optimizer PyTorch checkpoint from which to resume training.', metavar='')\n",
    "    parser.add_argument('--experiment', type=str, default='debug', help='Name of MLflow experiment where to assign this run.', metavar='')\n",
    "    parser.add_argument('--random', action='store_true', help='Forget about the specified model and run a random baseline.')\n",
    "\n",
    "    #parser.add_argument('--optimizer', type=str, default='AdamW', help='Optimizer to use for training (default: AdamW).')\n",
    "    #parser.add_argument('--l2', type=float, default=0.01, help='Weight decay to use for training (default: 0.01).')\n",
    "    #parser.add_argument('--loss_fun', type=str, default='BCE', help='Loss function to use for training (default: BCE).')\n",
    "    #parser.add_argument('--epoch_max', type=int, default=50, help='Maximum number of epochs to train for (default: 100).')\n",
    "    #parser.add_argument('--lr_ini', type=float, default=1e-5, help='Initial learning rate (default: 1e-5).')\n",
    "    \n",
    "    parser.add_argument('--gpu', type=str, default=\"0\", help='GPU number to use. Default: 0', metavar='')\n",
    "    parser.add_argument('--seed', type=int, default=None, help='seed everything with provided seed, default no seed')\n",
    "\n",
    "    parser.add_argument('--split',type=str, default='time_a_c',help=\"split-type default: time_a_c for time based assay and compound split, other options time_a, time_c, random_{seed}, or column of activity.parquet triplet\")\n",
    "    parser.add_argument('--support_set_size',type=int, default='0',help=\"per task how many to add from test- as well as valid- to the train-set default=0 = zero-shot\")\n",
    "    parser.add_argument('--train_only_actives', action='store_true', help='train only with active molecules')\n",
    "    parser.add_argument('--drop_cidx_path', type=str, default=None, help='Path to a file containing a np of cidx (NOT CIDs) to drop from the dataset.')\n",
    "\n",
    "    parser.add_argument('--verbose','-v', type=int, default=0, help='verbosity level default=0')\n",
    "    parser.add_argument('--wandb','-w', action='store_true', help='wandb logging on')\n",
    "    parser.add_argument('--bf16', action='store_true', help='use bfloat16 for training')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    keypairs = dict([unknown[i:i+2] for i in range(0, len(unknown), 1) if unknown[i].startswith(\"--\") and not (unknown[i+1:i+2]+[\"--\"])[0].startswith(\"--\")])\n",
    "\n",
    "    hparams = utils.get_hparams(path=args.hyperparams, mode='json', verbose=args.verbose)\n",
    "    \n",
    "    if override_hpjson:\n",
    "        from clamp.utils import NAME2FORMATTER\n",
    "        for k,v in NAME2FORMATTER.items():\n",
    "            if (k not in args):\n",
    "                default = hparams.get(k, None)\n",
    "                parser.add_argument('--'+k, type=v, default=default)\n",
    "                if (k in keypairs):\n",
    "                    logger.info(f'{k} from hparam file will be overwritten')\n",
    "\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if args.nonlinearity is None:\n",
    "        args.nonlinearity = 'ReLU'\n",
    "    if args.compound_layer_sizes is None:\n",
    "        logger.info('no compound_layer_sizes provided, setting to hidden_layers')\n",
    "        args.compound_layer_sizes = args.hidden_layers\n",
    "    if args.assay_layer_sizes is None:\n",
    "        logger.info('no assay_layer_sizes provided, setting to hidden_layers')\n",
    "        args.assay_layer_sizes =  args.hidden_layers\n",
    "\n",
    "    return args\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
