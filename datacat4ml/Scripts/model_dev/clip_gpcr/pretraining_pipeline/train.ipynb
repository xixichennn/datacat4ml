{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: datacat(Python 3.8.20)\n",
    "#--> from clamp.dataset import InMemoryClamp\n",
    "#--> from clamp import utils\n",
    "#--> from clamp.utils import set_device\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "import argparse\n",
    "import wandb\n",
    "from time import time\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `clamp/clamp/utils.py`\n",
    "def seed_everything(seed=70135): \n",
    "    \"\"\"does what it says ;) - from https://gist.github.com/KirillVladimirov/005ec7f762293d2321385580d3dbe335\"\"\"\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import os\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed) # set the seed for hash-based operations\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed) # if using GPU\n",
    "    torch.backends.cudnn.deterministic = True # if using GPU\n",
    "\n",
    "def set_device(gpu=0):\n",
    "    \"Set device to gpu or cpu.\"\n",
    "    if gpu=='any':\n",
    "        gpu = 0 # The GPU numbre on device is ususally starting from 0\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f'cuda:{gpu}')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `prep_fsmol.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `def parse_args_override`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(): \n",
    "    parser = argparse.ArgumentParser('Train and test a single run of clip-gpcr model. Overrides arguments from hyperparam-file')\n",
    "    parser.add_argument('-f', type=str) # ? what does 'f' mean?\n",
    "    parser.add_argument('--dataset', type=str, default='./data/fsmol/', help='Path to a prepared dataset directory.') # <--\n",
    "    parser.add_argument('--assay_mode', type=str, default='lsa', help='Type of assay features (\"clip\", \"biobert\", or \"lsa\").') # -->\n",
    "    parser.add_argument('--compound_mode', type=str, default='morganc+rdkc', help='Type of compound features (default:morgan+rdkc)') # <-->\n",
    "    parser.add_argument('--hparams', type=str, default='./hparams/clip-gpcr.json', help='Path to a hyperparameter to use in training clip-gpcr (json, yml)') # <-->\n",
    "\n",
    "    parser.add_argument('--checkpoint', help='Path to a checkpoint file to load model weights from.', metavar='') \n",
    "    parser.add_argument('--experiment', type=str, default='debug', help='Name of MLflow experiment where to assign this run.', metavar='') \n",
    "    parser.add_argument('--random', action='store_true', help='Forget about the specified model and run a random baseline.') #?\n",
    "\n",
    "    parser.add_argument('--gpu', type=str, default=\"0\", help='GPU number to use. Default: 0', metavar='')\n",
    "    parser.add_argument('--seed', type=int, default=None, help='seed everything with provided seed, default no seed')\n",
    "    \n",
    "    parser.add_argument('--split', type=str, default='time_a_c', help='split-type default: time_a_c for time based assay and compound split, other options: time_a, time_c, random_{seed}m or column of activity.parquet triplet') # <-->\n",
    "    parser.add_argument('--support_set_size', type=int, default='0', help='per task how many to add from test- as well as valid- to the train-set default=0 = zero-shot') #?\n",
    "    parser.add_argument('--train_only_actives', action='store_true', help='train only with active molecules')\n",
    "    parser.add_argument('--drop_cidx_path', type=str, default=None, help='Path to a file containing a np of cidx (NOT CIDs) to drop from the dataset.')\n",
    "\n",
    "    parser.add_argument('--verbose', '-v', type=int, default=0, help='verbosity level default=0')\n",
    "    parser.add_argument('--wandb','-w', action='store_true', help='Log to wandb')\n",
    "    parser.add_argument('--bf16', action='store_true', help='use bfloat16 for training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `def setup_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = './data/pubchem23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/pubchem23')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Path(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `def main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.experiment = 'debug'\n",
    "        self.seed = None\n",
    "        self.wandb = False\n",
    "        self.split = 'time_a_c' # <--\n",
    "        self.assay_mode = 'lsa' # <--\n",
    "        self.gpu = '0'\n",
    "        self.verbose = 0\n",
    "        self.checkpoint = None\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment': 'debug',\n",
       " 'seed': None,\n",
       " 'wandb': False,\n",
       " 'split': 'time_a_c',\n",
       " 'assay_mode': 'lsa',\n",
       " 'gpu': '0',\n",
       " 'verbose': 0,\n",
       " 'checkpoint': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = args.__dict__\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     seed_everything(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m      5\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeeded everything with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m clamp_dl, train_idx, valid_idx, test_idx \u001b[38;5;241m=\u001b[39m \u001b[43msetup_dataset\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ensure that there is no overlap between the splits.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(train_idx)\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mset\u001b[39m(valid_idx)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(args.experiment)\n",
    "\n",
    "if args.seed:\n",
    "    seed_everything(args.seed)\n",
    "    logger.info(f'Seeded everything with {args.seed}')\n",
    "\n",
    "clamp_dl, train_idx, valid_idx, test_idx = setup_dataset(**args.__dict__)\n",
    "# ensure that there is no overlap between the splits.\n",
    "assert set(train_idx).intersection(set(valid_idx)) == set()\n",
    "assert set(train_idx).intersection(set(test_idx)) == set()\n",
    "\n",
    "if args.wandb:\n",
    "    runname = args.experiment + args.split[-1]+ args.assay_mode[-1]\n",
    "    \n",
    "    runname += ''.join([chr(random.randrange(97, 97 + 26)) for _ in range(3)]) # random 3 letter suffix\n",
    "    wandb.init(project='clip-gpcr', entity='yu', name=runname, config=args.__dict__)\n",
    "\n",
    "device = set_device(gpu=args.gpu)\n",
    "\n",
    "metrics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massay_mode\u001b[39m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m.\u001b[39massay_mode) \n\u001b[1;32m     25\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_params(hparams) \u001b[38;5;66;03m# log all hyperparameters to mlflow.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_and_test(\n\u001b[1;32m     28\u001b[0m         clamp_dl,\n\u001b[1;32m     29\u001b[0m         train_idx\u001b[38;5;241m=\u001b[39mtrain_idx,\n\u001b[1;32m     30\u001b[0m         valid_idx\u001b[38;5;241m=\u001b[39mvalid_idx,\n\u001b[1;32m     31\u001b[0m         test_idx\u001b[38;5;241m=\u001b[39mtest_idx,\n\u001b[1;32m     32\u001b[0m         hparams\u001b[38;5;241m=\u001b[39mhparams,\n\u001b[1;32m     33\u001b[0m         run_info\u001b[38;5;241m=\u001b[39mmlflowi,\n\u001b[1;32m     34\u001b[0m         checkpoint_file\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mcheckpoint,\n\u001b[1;32m     35\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     36\u001b[0m         bf16\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbf16,\n\u001b[1;32m     37\u001b[0m         verbose\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining manually interrupted. Trying to test with last checkpoint.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # start a new MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # retrieve the run info\n",
    "        mlflowi = mlflow.active_run().info\n",
    "\n",
    "    if args.checkpoint is not None:\n",
    "        # set a tag in the MLflow run to indicate that we are resuming training\n",
    "        mlflow.set_tag(\n",
    "            'mlflow.note.content',\n",
    "            f'Resumed training from {args.checkpoint}.'\n",
    "        )\n",
    "\n",
    "    if 'assay_mode' in hparams:\n",
    "        if hparams['assay_mode'] != args.assay_mode:\n",
    "            # 2 reasons to check the above:\n",
    "            # a previous hyperparameter set could be already saved\n",
    "            # the command-line argument could be merged with a experiment parameters from a tool like MLflow\n",
    "\n",
    "            logger.warning(f'Assay features are {args.assay_mode} in command line but \\\"{hparams[\"assay_mode\"]}\\\" in hyperparameter file.')\n",
    "            logger.warning('Command line {args.assay_mode} is the prevailing option.')\n",
    "            hparams['assay_mode'] = args.assay_mode\n",
    "    else: # if not in hparams, log it using mlflow\n",
    "        mlflow.log_param('assay_mode', args.assay_mode) \n",
    "    mlflow.log_params(hparams) # log all hyperparameters to mlflow.\n",
    "\n",
    "    metrics_df = utils.train_and_test(\n",
    "        clamp_dl,\n",
    "        train_idx=train_idx,\n",
    "        valid_idx=valid_idx,\n",
    "        test_idx=test_idx,\n",
    "        hparams=hparams,\n",
    "        run_info=mlflowi,\n",
    "        checkpoint_file=args.checkpoint,\n",
    "        device=device,\n",
    "        bf16=args.bf16,\n",
    "        verbose=args.verbose,\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.error('Training manually interrupted. Trying to test with last checkpoint.')\n",
    "    metrics_df = utils.test(\n",
    "        clamp_dl,\n",
    "        train_idx=train_idx,\n",
    "        test_idx=test_idx,\n",
    "        hparams=hparams,\n",
    "        run_info=mlflowi,\n",
    "        device=device,\n",
    "        verbose=args.verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `if __name__ == '__main__:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parse_args_override()\n",
    "run_id = str(time()).split('.')[0]\n",
    "fn_postfix = f'{args.experiment}_{run_id}' \n",
    "\n",
    "if args.verbose>=1:\n",
    "    logger.info('Run args: ', os.getcwd()+__file__, args.__dict__)\n",
    "\n",
    "# main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
