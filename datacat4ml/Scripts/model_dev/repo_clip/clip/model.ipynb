{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from datacat4ml.const import *\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SMILESTokenizer:\n",
    "\n",
    "    \"\"\"\n",
    "    Shared by Markus.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\!|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        self.vocab = {}\n",
    "        self.inv_vocab = {}\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.max_len = None\n",
    "    def tokenize(self, smiles):\n",
    "        \"\"\"Tokenizes a SMILES string using the predefined regular expression.\"\"\"\n",
    "        return re.findall(self.pattern, smiles)\n",
    "    def build_vocab(self, smiles_list):\n",
    "        \"\"\"Builds vocabulary from a list of SMILES strings.\"\"\"\n",
    "        all_tokens = set()\n",
    "        for smiles in smiles_list:\n",
    "            tokens = self.tokenize(smiles)\n",
    "            all_tokens.update(tokens)\n",
    "        tokens = [self.pad_token, self.unk_token, self.start_token, self.end_token]\n",
    "        all_tokens = sorted(all_tokens)\n",
    "        all_tokens = tokens + all_tokens\n",
    "        self.vocab = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "        self.inv_vocab = {idx: token for token, idx in self.vocab.items()}\n",
    "    def encode(self, smiles, max_len=None):\n",
    "        \"\"\"Encodes a SMILES string into a list of token indices, optionally padding to max_len.\"\"\"\n",
    "        tokens = self.tokenize(smiles)\n",
    "        tokens = [self.start_token] + tokens + [self.end_token]\n",
    "        token_ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        if max_len:\n",
    "            token_ids = token_ids[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(token_ids))\n",
    "        return token_ids\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decodes a list of token indices back into a SMILES string.\"\"\"\n",
    "        tokens = [self.inv_vocab.get(token_id, self.unk_token) for token_id in token_ids]\n",
    "        tokens = [token for token in tokens if token not in [self.start_token, self.end_token, self.pad_token]]\n",
    "        return ''.join(tokens)\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Returns the size of the vocabulary.\"\"\"\n",
    "        return len(self.vocab)\n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        \"\"\"Pads a sequence to the maximum length.\"\"\"\n",
    "        return sequence[:max_len] + [self.vocab[self.pad_token]] * max(0, max_len - len(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP/clip/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "    \n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPGPCR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # molecule\n",
    "                 smiles_vocab_size: int,\n",
    "\n",
    "                 # assay text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        # Initialize the SMILES tokenizer\n",
    "        self.smiles_tokenizer = SMILESTokenizer()\n",
    "        self.smiles_vocab_size = smiles_vocab_size\n",
    "        self.smiles_embedding = nn.Embedding(smiles_vocab_size, transformer_width)\n",
    "        self.smiles_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width = transformer_width,\n",
    "            layers = transformer_layers,\n",
    "            heads = transformer_heads,\n",
    "            attn_mask= self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self. vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) *np.log(1 / 0.07))\n",
    "\n",
    "        self.initizalize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "        nn.init.normal_(self.smiles_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.smiles_projection, std=0.01)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self. transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=attn_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "        \n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "    \n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the smiles tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"_inf\"))\n",
    "        mask.triu_(1) # zero out the lower diagonal\n",
    "        return mask\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.smiles_embedding.weight.dtype\n",
    "    \n",
    "    def encode_smiles(self, smiles):\n",
    "\n",
    "        token_ids = [self.smiles_tokenizer.encode(s) for s in smiles]\n",
    "        token_ids = torch.tensor(token_ids).to(self.dtype)\n",
    "        x = self.smiles_embedding(token_ids)\n",
    "        x = x.mean(dim=1) # average the embeddings\n",
    "        x = x @ self.smiles_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, smiles, text):\n",
    "        smiles_features = self.encode_smiles(smiles)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalize features\n",
    "        smiles_features = smiles_features / smiles_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_smiles = logit_scale * smiles_features @ text_features.t()\n",
    "        logits_per_text = logits_per_smiles.t()\n",
    "\n",
    "        return logits_per_smiles, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weights(model: nn.Module):\n",
    "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp16(l):\n",
    "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            l.weight.data = l.weight.data.half()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data = l.bias.data.half()\n",
    "\n",
    "        if isinstance(l, nn.MultiheadAttention):\n",
    "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                tensor = getattr(l, attr)\n",
    "                if tensor is not None:\n",
    "                    tensor.data = tensor.data.half()\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\"]:\n",
    "            if hasattr(l, name):\n",
    "                attr = getattr(l, name)\n",
    "                if attr is not None:\n",
    "                    attr.data = attr.data.half()\n",
    "\n",
    "    model.apply(_convert_weights_to_fp16)\n",
    "\n",
    "def build_model(state_dict:dict):\n",
    "    # Extract relevant parameters from the state_dict\n",
    "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
    "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n",
    "\n",
    "    # SMILES specific parameters\n",
    "    smiles_vocab_size = state_dict[\"smiles_embedding.weight\"].shape[0]\n",
    "\n",
    "    # Initialize the modified CLIP model with SMILES and text handling\n",
    "    model = CLIPGPCR(\n",
    "        embed_dim,\n",
    "        smiles_vocab_size,\n",
    "        context_length,\n",
    "        vocab_size,\n",
    "        transformer_width,\n",
    "        transformer_heads,\n",
    "        transformer_layers\n",
    "    )\n",
    "\n",
    "    for key in [\"smiles_vocab_size\", \"context_length\", \"vocab_size\"]: # \"input_resolution\" --> \"smiles_vocab_size\"\n",
    "        if key in state_dict:\n",
    "            del state_dict[key]\n",
    "    \n",
    "    convert_weights(model)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    return model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CLIPGPCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import CLIPGPCR\n",
    "# import text tokenizer\n",
    "from datacat4ml.Scripts.model_dev.repo_clip.clip.simple_tokenizer import SimpleTokenizer\n",
    "# import smiles tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpcr_assay_descs.txt\", \"r\") as f:\n",
    "    gpcr_texts = f.readlines()\n",
    "\n",
    "with open(\"gpcr_smis.txt\", \"r\") as f:\n",
    "    gpcr_smiles = f.readlines()\n",
    "\n",
    "# load the training data into a pandas dataframe\n",
    "datapath = os.path.join(CURA_GPCR_DATASETS_DIR, \"cls\")\n",
    "# load all files with a suffix of '_curated.csv'\n",
    "files = [f for f in os.listdir(datapath) if f.endswith(\"_curated.csv\")]\n",
    "# load all the files into a single dataframe adn drop column 'Unnamed: 0'\n",
    "df = pd.concat([pd.read_csv(os.path.join(datapath, f)) for f in files], ignore_index=True).drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assay_id</th>\n",
       "      <th>assay_chembl_id</th>\n",
       "      <th>tid</th>\n",
       "      <th>target_chembl_id</th>\n",
       "      <th>standard_type</th>\n",
       "      <th>standard_relation</th>\n",
       "      <th>standard_value</th>\n",
       "      <th>standard_units</th>\n",
       "      <th>pchembl_value</th>\n",
       "      <th>assay_type</th>\n",
       "      <th>...</th>\n",
       "      <th>pStandard_value</th>\n",
       "      <th>max_num_atoms</th>\n",
       "      <th>max_molecular_weight</th>\n",
       "      <th>activity_string</th>\n",
       "      <th>activity</th>\n",
       "      <th>threshold</th>\n",
       "      <th>target</th>\n",
       "      <th>effect</th>\n",
       "      <th>assay</th>\n",
       "      <th>std_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200520</td>\n",
       "      <td>CHEMBL801203</td>\n",
       "      <td>11261</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>Ki</td>\n",
       "      <td>=</td>\n",
       "      <td>2660.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>5.58</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>5.575118363368933</td>\n",
       "      <td>64</td>\n",
       "      <td>882.116</td>\n",
       "      <td>weak active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.565998</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200520</td>\n",
       "      <td>CHEMBL801203</td>\n",
       "      <td>11261</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>Ki</td>\n",
       "      <td>=</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>5.28</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>5.279840696594043</td>\n",
       "      <td>64</td>\n",
       "      <td>882.116</td>\n",
       "      <td>inactive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.565998</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200520</td>\n",
       "      <td>CHEMBL801203</td>\n",
       "      <td>11261</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>Ki</td>\n",
       "      <td>=</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>5.43</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>5.425968732272281</td>\n",
       "      <td>64</td>\n",
       "      <td>882.116</td>\n",
       "      <td>inactive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.565998</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200506</td>\n",
       "      <td>CHEMBL803851</td>\n",
       "      <td>11261</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>Ki</td>\n",
       "      <td>=</td>\n",
       "      <td>2574.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>5.59</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>5.589391457431632</td>\n",
       "      <td>64</td>\n",
       "      <td>882.116</td>\n",
       "      <td>weak active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.565998</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200506</td>\n",
       "      <td>CHEMBL803851</td>\n",
       "      <td>11261</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>Ki</td>\n",
       "      <td>=</td>\n",
       "      <td>3614.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>5.44</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>5.442011851775087</td>\n",
       "      <td>64</td>\n",
       "      <td>882.116</td>\n",
       "      <td>inactive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.565998</td>\n",
       "      <td>CHEMBL2028</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289750</th>\n",
       "      <td>1804125</td>\n",
       "      <td>CHEMBL4276417</td>\n",
       "      <td>101356</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>EC50</td>\n",
       "      <td>=</td>\n",
       "      <td>2747.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>5.56</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>5.561141340579438</td>\n",
       "      <td>65</td>\n",
       "      <td>897.095</td>\n",
       "      <td>weak active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.474955</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289751</th>\n",
       "      <td>1804125</td>\n",
       "      <td>CHEMBL4276417</td>\n",
       "      <td>101356</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>EC50</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65</td>\n",
       "      <td>897.095</td>\n",
       "      <td>inactive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.474955</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289752</th>\n",
       "      <td>1804125</td>\n",
       "      <td>CHEMBL4276417</td>\n",
       "      <td>101356</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>EC50</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65</td>\n",
       "      <td>897.095</td>\n",
       "      <td>inactive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.474955</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289753</th>\n",
       "      <td>1804125</td>\n",
       "      <td>CHEMBL4276417</td>\n",
       "      <td>101356</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>EC50</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>nM</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65</td>\n",
       "      <td>897.095</td>\n",
       "      <td>inactive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.474955</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289754</th>\n",
       "      <td>1804125</td>\n",
       "      <td>CHEMBL4276417</td>\n",
       "      <td>101356</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>EC50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>897.095</td>\n",
       "      <td>inactive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.474955</td>\n",
       "      <td>CHEMBL5852</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>EC50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289755 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        assay_id assay_chembl_id     tid target_chembl_id standard_type  \\\n",
       "0         200520    CHEMBL801203   11261       CHEMBL2028            Ki   \n",
       "1         200520    CHEMBL801203   11261       CHEMBL2028            Ki   \n",
       "2         200520    CHEMBL801203   11261       CHEMBL2028            Ki   \n",
       "3         200506    CHEMBL803851   11261       CHEMBL2028            Ki   \n",
       "4         200506    CHEMBL803851   11261       CHEMBL2028            Ki   \n",
       "...          ...             ...     ...              ...           ...   \n",
       "289750   1804125   CHEMBL4276417  101356       CHEMBL5852          EC50   \n",
       "289751   1804125   CHEMBL4276417  101356       CHEMBL5852          EC50   \n",
       "289752   1804125   CHEMBL4276417  101356       CHEMBL5852          EC50   \n",
       "289753   1804125   CHEMBL4276417  101356       CHEMBL5852          EC50   \n",
       "289754   1804125   CHEMBL4276417  101356       CHEMBL5852          EC50   \n",
       "\n",
       "       standard_relation standard_value standard_units pchembl_value  \\\n",
       "0                      =         2660.0             nM          5.58   \n",
       "1                      =         5250.0             nM          5.28   \n",
       "2                      =         3750.0             nM          5.43   \n",
       "3                      =         2574.0             nM          5.59   \n",
       "4                      =         3614.0             nM          5.44   \n",
       "...                  ...            ...            ...           ...   \n",
       "289750                 =         2747.0             nM          5.56   \n",
       "289751                 >       100000.0             nM          None   \n",
       "289752                 >       100000.0             nM          None   \n",
       "289753                 >       100000.0             nM          None   \n",
       "289754              None           None           None          None   \n",
       "\n",
       "       assay_type  ...    pStandard_value max_num_atoms max_molecular_weight  \\\n",
       "0               B  ...  5.575118363368933            64              882.116   \n",
       "1               B  ...  5.279840696594043            64              882.116   \n",
       "2               B  ...  5.425968732272281            64              882.116   \n",
       "3               B  ...  5.589391457431632            64              882.116   \n",
       "4               B  ...  5.442011851775087            64              882.116   \n",
       "...           ...  ...                ...           ...                  ...   \n",
       "289750          F  ...  5.561141340579438            65              897.095   \n",
       "289751          F  ...                4.0            65              897.095   \n",
       "289752          F  ...                4.0            65              897.095   \n",
       "289753          F  ...                4.0            65              897.095   \n",
       "289754          F  ...               None            65              897.095   \n",
       "\n",
       "       activity_string activity threshold      target effect assay std_type  \n",
       "0          weak active      1.0  5.565998  CHEMBL2028   None  None       Ki  \n",
       "1             inactive      0.0  5.565998  CHEMBL2028   None  None       Ki  \n",
       "2             inactive      0.0  5.565998  CHEMBL2028   None  None       Ki  \n",
       "3          weak active      1.0  5.565998  CHEMBL2028   None  None       Ki  \n",
       "4             inactive      0.0  5.565998  CHEMBL2028   None  None       Ki  \n",
       "...                ...      ...       ...         ...    ...   ...      ...  \n",
       "289750     weak active      1.0  5.474955  CHEMBL5852   None  None     EC50  \n",
       "289751        inactive      0.0  5.474955  CHEMBL5852   None  None     EC50  \n",
       "289752        inactive      0.0  5.474955  CHEMBL5852   None  None     EC50  \n",
       "289753        inactive      0.0  5.474955  CHEMBL5852   None  None     EC50  \n",
       "289754        inactive      0.0  5.474955  CHEMBL5852   None  None     EC50  \n",
       "\n",
       "[289755 rows x 44 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "text_tokenizer = SimpleTokenizer()\n",
    "smiles_tokenizer = SMILESTokenizer()\n",
    "smiles_tokenizer.build_vocab(gpcr_smiles) # make sure to build the vocabulary from your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'method' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m CLIPGPCR(\n\u001b[1;32m      3\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      4\u001b[0m     smiles_vocab_size\u001b[38;5;241m=\u001b[39msmiles_tokenizer\u001b[38;5;241m.\u001b[39mvocab_size(),\n\u001b[1;32m      5\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m77\u001b[39m,\n\u001b[0;32m----> 6\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      7\u001b[0m     transformer_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      8\u001b[0m     transformer_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      9\u001b[0m     transformer_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# set up the optimizer and loss function\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'method' has no len()"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = CLIPGPCR(\n",
    "    embed_dim=512,\n",
    "    smiles_vocab_size=smiles_tokenizer.vocab_size(),\n",
    "    context_length=77,\n",
    "    vocab_size=len(text_tokenizer.encode),\n",
    "    transformer_width=512,\n",
    "    transformer_heads=8,\n",
    "    transformer_layers=12\n",
    ")\n",
    "\n",
    "# set up the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for smiles, text in dataloader:  # Replace with your actual dataloader\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Tokenize inputs\n",
    "        smiles_ids = [smiles_tokenizer.encode(s) for s in smiles]\n",
    "        text_ids = text_tokenizer.encode(text)\n",
    "\n",
    "        smiles_ids = torch.tensor(smiles_ids).to(device)\n",
    "        text_ids = torch.tensor(text_ids).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_smiles, logits_per_text = model(smiles_ids, text_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        labels = torch.arange(len(smiles)).to(device)\n",
    "        loss = (criterion(logits_per_smiles, labels) + criterion(logits_per_text, labels)) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"clip_smiles_text_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP/clip/clip.py\n",
    "\n",
    "The module merely loads the pre-trained clip models, without incorporating the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "from datacat4ml.Scripts.model_dev.repo_clip.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
