{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda environment: pyg (Python 3.9.16)\n",
    "\"\"\" CLIP Model\n",
    "\n",
    "Adapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n",
    "\"\"\"\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################   Yu's ##########################\n",
    "\n",
    "#--> from .hf_model import HFTextEncoder\n",
    "#--> from .modified_resnet import ModifiedResNet\n",
    "#--> from .timm_model import TimmModel\n",
    "#--> from .transformer import LayerNormFp32, LayerNorm, QuickGELU, Attention, VisionTransformer, TextTransformer,\\\n",
    "#-->     text_global_pool\n",
    "from datacat4ml.Scripts.model_dev.rep_open_clip.src.open_clip.transformer import TextTransformer\n",
    "#--> from .utils import to_2tuple\n",
    "#################   Yu's ##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `class CLIPTextCfg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CLIPTextCfg:\n",
    "    context_length: int = 77\n",
    "    vocab_size: int = 49408\n",
    "    hf_tokenizer_name: Optional[str] = None\n",
    "    tokenizer_kwargs: Optional[dict] = None\n",
    "\n",
    "    width: int = 512\n",
    "    heads: int = 8\n",
    "    layers: int = 12\n",
    "    mlp_ratio: float = 4.0\n",
    "    ls_init_value: Optional[float] = None  # layer scale initial value\n",
    "    embed_cls: bool = False\n",
    "    pad_id: int = 0\n",
    "    no_causal_mask: bool = False  # disable causal masking\n",
    "    final_ln_after_pool: bool = False  # apply final LayerNorm after pooling\n",
    "    pool_type: str = 'argmax'\n",
    "    proj_bias: bool = False\n",
    "    proj_type: str = 'linear'  # control final text projection, 'none' forces no projection\n",
    "    output_tokens: bool = False\n",
    "    act_kwargs: dict = None\n",
    "    norm_kwargs: dict = None\n",
    "\n",
    "    # HuggingFace specific text tower config\n",
    "    hf_model_name: Optional[str] = None\n",
    "    hf_model_pretrained: bool = True\n",
    "    hf_proj_type: str = 'mlp'\n",
    "    hf_pooler_type: str = 'mean_pooler'  # attentional pooling for HF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_text_tower(\n",
    "        embed_dim: int,\n",
    "        text_cfg: CLIPTextCfg,\n",
    "        quick_gelu: bool = False,\n",
    "        cast_dtype: Optional[torch.dtype] = None,\n",
    "):\n",
    "    if isinstance(text_cfg, dict):\n",
    "        text_cfg = CLIPTextCfg(**text_cfg)\n",
    "\n",
    "    if text_cfg.hf_model_name:\n",
    "        text = HFTextEncoder(\n",
    "            text_cfg.hf_model_name,\n",
    "            output_dim=embed_dim,\n",
    "            proj_type=text_cfg.hf_proj_type,\n",
    "            pooler_type=text_cfg.hf_pooler_type,\n",
    "            pretrained=text_cfg.hf_model_pretrained,\n",
    "            output_tokens=text_cfg.output_tokens,\n",
    "        )\n",
    "    else:\n",
    "        act_layer = QuickGELU if quick_gelu else nn.GELU\n",
    "        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm\n",
    "        if text_cfg.norm_kwargs:\n",
    "            norm_layer = partial(norm_layer, **text_cfg.norm_kwargs)\n",
    "        if text_cfg.act_kwargs is not None:\n",
    "            act_layer = partial(act_layer, **text_cfg.act_kwargs)\n",
    "\n",
    "        text = TextTransformer(\n",
    "            context_length=text_cfg.context_length,\n",
    "            vocab_size=text_cfg.vocab_size,\n",
    "            width=text_cfg.width,\n",
    "            heads=text_cfg.heads,\n",
    "            layers=text_cfg.layers,\n",
    "            mlp_ratio=text_cfg.mlp_ratio,\n",
    "            ls_init_value=text_cfg.ls_init_value,\n",
    "            output_dim=embed_dim,\n",
    "            embed_cls=text_cfg.embed_cls,\n",
    "            no_causal_mask=text_cfg.no_causal_mask,\n",
    "            pad_id=text_cfg.pad_id,\n",
    "            pool_type=text_cfg.pool_type,\n",
    "            proj_type=text_cfg.proj_type,\n",
    "            proj_bias=text_cfg.proj_bias,\n",
    "            output_tokens=text_cfg.output_tokens,\n",
    "            act_layer=act_layer,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `class CLIPCpdCfg` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `class CLIP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    output_dict: torch.jit.Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            vision_cfg: CLIPVisionCfg,\n",
    "            text_cfg: CLIPTextCfg,\n",
    "            quick_gelu: bool = False,\n",
    "            init_logit_scale: float = np.log(1 / 0.07),\n",
    "            init_logit_bias: Optional[float] = None,\n",
    "            nonscalar_logit_scale: bool = False,\n",
    "            cast_dtype: Optional[torch.dtype] = None,\n",
    "            output_dict: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_dict = output_dict\n",
    "\n",
    "        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n",
    "\n",
    "        text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n",
    "        self.transformer = text.transformer\n",
    "        self.context_length = text.context_length\n",
    "        self.vocab_size = text.vocab_size\n",
    "        self.token_embedding = text.token_embedding\n",
    "        self.positional_embedding = text.positional_embedding\n",
    "        self.ln_final = text.ln_final\n",
    "        self.text_projection = text.text_projection\n",
    "        self.text_pool_type = text.pool_type\n",
    "        self.register_buffer('attn_mask', text.attn_mask, persistent=False)\n",
    "\n",
    "        lshape = [1] if nonscalar_logit_scale else []\n",
    "        self.logit_scale = nn.Parameter(torch.ones(lshape) * init_logit_scale)\n",
    "        if init_logit_bias is not None:\n",
    "            self.logit_bias = nn.Parameter(torch.ones(lshape) * init_logit_bias)\n",
    "        else:\n",
    "            self.logit_bias = None\n",
    "\n",
    "    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n",
    "        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n",
    "        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        self.visual.set_grad_checkpointing(enable)\n",
    "        self.transformer.grad_checkpointing = enable\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        # for timm optimizers, 1d params like logit_scale, logit_bias, ln/bn scale, biases are excluded by default\n",
    "        no_wd = {'positional_embedding'}\n",
    "        if hasattr(self.visual, 'no_weight_decay'):\n",
    "            for n in self.visual.no_weight_decay():\n",
    "                no_wd.add('visual.' + n)\n",
    "        return no_wd\n",
    "\n",
    "    def encode_image(self, image, normalize: bool = False):\n",
    "        features = self.visual(image)\n",
    "        return F.normalize(features, dim=-1) if normalize else features\n",
    "\n",
    "    def encode_text(self, text, normalize: bool = False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x, _ = text_global_pool(x, text, self.text_pool_type)\n",
    "        if self.text_projection is not None:\n",
    "            if isinstance(self.text_projection, nn.Linear):\n",
    "                x = self.text_projection(x)\n",
    "            else:\n",
    "                x = x @ self.text_projection\n",
    "\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def get_logits(self, image, text):\n",
    "        image_features = self.encode_image(image, normalize=True)\n",
    "        text_features = self.encode_text(text, normalize=True)\n",
    "        image_logits = self.logit_scale.exp() * image_features @ text_features.T\n",
    "        if self.logit_bias is not None:\n",
    "            image_logits += self.logit_bias\n",
    "        text_logits = image_logits.T\n",
    "        return image_logits, text_logits\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            image: Optional[torch.Tensor] = None,\n",
    "            text: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        image_features = self.encode_image(image, normalize=True) if image is not None else None\n",
    "        text_features = self.encode_text(text, normalize=True) if text is not None else None\n",
    "\n",
    "        if self.output_dict:\n",
    "            out_dict = {\n",
    "                \"image_features\": image_features,\n",
    "                \"text_features\": text_features,\n",
    "                \"logit_scale\": self.logit_scale.exp()\n",
    "            }\n",
    "            if self.logit_bias is not None:\n",
    "                out_dict['logit_bias'] = self.logit_bias\n",
    "            return out_dict\n",
    "\n",
    "        if self.logit_bias is not None:\n",
    "            return image_features, text_features, self.logit_scale.exp(), self.logit_bias\n",
    "        \n",
    "        return image_features, text_features, self.logit_scale.exp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
