{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: pyg (Python 3.9.16)\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from typing import Callable, List, Optional, Sequence, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################   Yu's ##########################\n",
    "\n",
    "#--> from .utils import to_2tuple\n",
    "#--> from .pos_embed import get_2d_sincos_pos_embed\n",
    "\n",
    "#--> delete the above two lines because thoes function are used in the VisionTransformer\n",
    "#################   Yu's ##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        return x.to(orig_type)\n",
    "    \n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model: int,\n",
    "            n_head: int,\n",
    "            mlp_ratio: float = 4.0,\n",
    "            ls_init_value: float = None,\n",
    "            act_layer: Callable = nn.GELU,\n",
    "            norm_layer: Callable = LayerNorm,\n",
    "            is_cross_attention: bool = False,\n",
    "            batch_first: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head, batch_first=batch_first)\n",
    "        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n",
    "        if is_cross_attention:\n",
    "            self.ln_1_kv = norm_layer(d_model)\n",
    "\n",
    "        self.ln_2 = norm_layer(d_model)\n",
    "        mlp_width = int(d_model * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, mlp_width)),\n",
    "            (\"gelu\", act_layer()),\n",
    "            (\"c_proj\", nn.Linear(mlp_width, d_model))\n",
    "        ]))\n",
    "        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n",
    "\n",
    "    def attention(\n",
    "            self,\n",
    "            q_x: torch.Tensor,\n",
    "            k_x: Optional[torch.Tensor] = None,\n",
    "            v_x: Optional[torch.Tensor] = None,\n",
    "            attn_mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        k_x = k_x if k_x is not None else q_x\n",
    "        v_x = v_x if v_x is not None else q_x\n",
    "\n",
    "        attn_mask = attn_mask.to(q_x.dtype) if attn_mask is not None else None\n",
    "        return self.attn(\n",
    "            q_x, k_x, v_x, need_weights=False, attn_mask=attn_mask\n",
    "        )[0]\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            q_x: torch.Tensor,\n",
    "            k_x: Optional[torch.Tensor] = None,\n",
    "            v_x: Optional[torch.Tensor] = None,\n",
    "            attn_mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        k_x = self.ln_1_kv(k_x) if hasattr(self, \"ln_1_kv\") and k_x is not None else None\n",
    "        v_x = self.ln_1_kv(v_x) if hasattr(self, \"ln_1_kv\") and v_x is not None else None\n",
    "        x = q_x + self.ls_1(self.attention(q_x=self.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n",
    "        x = x + self.ls_2(self.mlp(self.ln_2(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            width: int,\n",
    "            layers: int,\n",
    "            heads: int,\n",
    "            mlp_ratio: float = 4.0,\n",
    "            ls_init_value: float = None,\n",
    "            act_layer: Callable = nn.GELU,\n",
    "            norm_layer: Callable = LayerNorm,\n",
    "            batch_first: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.batch_first = batch_first\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        self.resblocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(\n",
    "                width,\n",
    "                heads,\n",
    "                mlp_ratio,\n",
    "                ls_init_value=ls_init_value,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                batch_first=batch_first,\n",
    "            )\n",
    "            for _ in range(layers)\n",
    "        ])\n",
    "\n",
    "    def get_cast_dtype(self) -> torch.dtype:\n",
    "        if hasattr(self.resblocks[0].mlp.c_fc, 'int8_original_dtype'):\n",
    "            return self.resblocks[0].mlp.c_fc.int8_original_dtype\n",
    "        return self.resblocks[0].mlp.c_fc.weight.dtype\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1).contiguous()    # NLD -> LND\n",
    "        for r in self.resblocks:\n",
    "            if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                # TODO: handle kwargs https://github.com/pytorch/pytorch/issues/79887#issuecomment-1161758372\n",
    "                x = checkpoint(r, x, None, None, attn_mask)\n",
    "            else:\n",
    "                x = r(x, attn_mask=attn_mask)\n",
    "        if not self.batch_first:\n",
    "            x = x.transpose(0, 1)    # LND -> NLD\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TextTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_token(token, batch_size: int):\n",
    "    \"\"\"\n",
    "    - Expands a given token tensor to match the batch size.\n",
    "    - Takes a token tensor of shape (1, 1, -1) and expands it to shape (batch_size, 1, -1).\n",
    "    \"\"\"\n",
    "    return token.view(1, 1, -1).expand(batch_size, -1, -1)\n",
    "\n",
    "def text_global_pool(x, text: Optional[torch.Tensor] = None, pool_type: str = 'argmax'):\n",
    "    \"\"\"\n",
    "    \"pool\" refers to the process of aggregating or summarizing the token embeddings (outputs of the transformer) into a single vector representation. \n",
    "    This is typically done to reduce the dimensionality and produce a fixed-size output, which can then be used for downstream tasks such as classification or retrieval.\n",
    "    - Pools the text embeddings based on the specified pooling type.\n",
    "    - Supports different pooling types: 'first', 'last', 'argmax', and 'none'.\n",
    "    - For 'first', it takes the first token; \n",
    "        for 'last', the last token; \n",
    "        for 'argmax', the token with the highest value; \n",
    "        and for 'none', it returns the embeddings as is.\n",
    "    \"\"\"\n",
    "    if pool_type == 'first':\n",
    "        pooled, tokens = x[:, 0], x[:, 1:]\n",
    "    elif pool_type == 'last':\n",
    "        pooled, tokens = x[:, -1], x[:, :-1]\n",
    "    elif pool_type == 'argmax':\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        assert text is not None\n",
    "        pooled, tokens = x[torch.arange(x.shape[0]), text.argmax(dim=-1)], x\n",
    "    else:\n",
    "        pooled = tokens = x\n",
    "\n",
    "    return pooled, tokens\n",
    "\n",
    "class TextTransformer(nn.Module):\n",
    "    output_tokens: torch.jit.Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            context_length: int = 77,\n",
    "            vocab_size: int = 49408,\n",
    "            width: int = 512,\n",
    "            heads: int = 8,\n",
    "            layers: int = 12,\n",
    "            mlp_ratio: float = 4.0,\n",
    "            ls_init_value: float = None,\n",
    "            output_dim: Optional[int] = 512,\n",
    "            embed_cls: bool = False,\n",
    "            no_causal_mask: bool = False,\n",
    "            pad_id: int = 0,\n",
    "            pool_type: str = 'argmax',\n",
    "            proj_type: str = 'linear',\n",
    "            proj_bias: bool = False,\n",
    "            act_layer: Callable = nn.GELU,\n",
    "            norm_layer: Callable = LayerNorm,\n",
    "            output_tokens: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert pool_type in ('first', 'last', 'argmax', 'none')\n",
    "        self.output_tokens = output_tokens\n",
    "        self.num_pos = self.context_length = context_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.width = width\n",
    "        self.output_dim = output_dim\n",
    "        self.heads = heads\n",
    "        self.pad_id = pad_id\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, width)\n",
    "        if embed_cls:\n",
    "            self.cls_emb = nn.Parameter(torch.empty(width))\n",
    "            self.num_pos += 1\n",
    "        else:\n",
    "            self.cls_emb = None\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.num_pos, width))\n",
    "        self.transformer = Transformer(\n",
    "            width=width,\n",
    "            layers=layers,\n",
    "            heads=heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            ls_init_value=ls_init_value,\n",
    "            act_layer=act_layer,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        self.ln_final = norm_layer(width)\n",
    "\n",
    "        if no_causal_mask:\n",
    "            self.attn_mask = None\n",
    "        else:\n",
    "            self.register_buffer('attn_mask', self.build_causal_mask(), persistent=False)\n",
    "\n",
    "        if proj_type == 'none' or not output_dim:\n",
    "            self.text_projection = None\n",
    "        else:\n",
    "            if proj_bias:\n",
    "                self.text_projection = nn.Linear(width, output_dim)\n",
    "            else:\n",
    "                self.text_projection = nn.Parameter(torch.empty(width, output_dim))\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "        if self.cls_emb is not None:\n",
    "            nn.init.normal_(self.cls_emb, std=0.01)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            if isinstance(self.text_projection, nn.Linear):\n",
    "                nn.init.normal_(self.text_projection.weight, std=self.transformer.width ** -0.5)\n",
    "                if self.text_projection.bias is not None:\n",
    "                    nn.init.zeros_(self.text_projection.bias)\n",
    "            else:\n",
    "                nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        self.transformer.grad_checkpointing = enable\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        # for timm optimizers, 1d params like logit_scale, logit_bias, ln/bn scale, biases are excluded by default\n",
    "        no_wd = {'positional_embedding'}\n",
    "        if self.cls_emb is not None:\n",
    "            no_wd.add('cls_emb')\n",
    "        return no_wd\n",
    "\n",
    "    def build_causal_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.num_pos, self.num_pos)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    def build_cls_mask(self, text, cast_dtype: torch.dtype):\n",
    "        cls_mask = (text != self.pad_id).unsqueeze(1)\n",
    "        cls_mask = F.pad(cls_mask, (1, 0, cls_mask.shape[2], 0), value=True)\n",
    "        additive_mask = torch.empty(cls_mask.shape, dtype=cast_dtype, device=cls_mask.device)\n",
    "        additive_mask.fill_(0)\n",
    "        additive_mask.masked_fill_(~cls_mask, float(\"-inf\"))\n",
    "        additive_mask = torch.repeat_interleave(additive_mask, self.heads, 0)\n",
    "        return additive_mask\n",
    "\n",
    "    def forward(self, text):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "        seq_len = text.shape[1]\n",
    "\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "        attn_mask = self.attn_mask\n",
    "        if self.cls_emb is not None:\n",
    "            seq_len += 1\n",
    "            x = torch.cat([x, _expand_token(self.cls_emb, x.shape[0])], dim=1)\n",
    "            cls_mask = self.build_cls_mask(text, cast_dtype)\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = attn_mask[None, :seq_len, :seq_len] + cls_mask[:, :seq_len, :seq_len]\n",
    "\n",
    "        x = x + self.positional_embedding[:seq_len].to(cast_dtype)\n",
    "        x = self.transformer(x, attn_mask=attn_mask)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        if self.cls_emb is not None:\n",
    "            # presence of appended cls embed (CoCa) overrides pool_type, always take last token\n",
    "            pooled, tokens = text_global_pool(x, pool_type='last')\n",
    "            pooled = self.ln_final(pooled)  # final LN applied after pooling in this case\n",
    "        else:\n",
    "            x = self.ln_final(x)\n",
    "            pooled, tokens = text_global_pool(x, text, pool_type=self.pool_type)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            if isinstance(self.text_projection, nn.Linear):\n",
    "                pooled = self.text_projection(pooled)\n",
    "            else:\n",
    "                pooled = pooled @ self.text_projection\n",
    "\n",
    "        if self.output_tokens:\n",
    "            return pooled, tokens\n",
    "\n",
    "        return pooled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
