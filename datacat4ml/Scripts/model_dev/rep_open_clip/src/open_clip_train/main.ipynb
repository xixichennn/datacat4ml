{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: pyg (Python 3.9.16import copy\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "\n",
    "try:\n",
    "    import torch.utils.tensorboard as tensorboard\n",
    "except ImportError:\n",
    "    tensorboard = None\n",
    "\n",
    "try:\n",
    "    import horovod.torch as hvd\n",
    "except ImportError:\n",
    "    hvd = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################   Yu's ##########################\n",
    "\n",
    "#--> from open_clip import create_model_and_transforms, trace_model, get_tokenizer, create_loss\n",
    "#--> from open_clip_train.data import get_data\n",
    "from datacat4ml.Scripts.model_dev.rep_open_clip.src.open_clip_train.distributed import is_master, init_distributed_device, broadcast_object\n",
    "from datacat4ml.Scripts.model_dev.rep_open_clip.src.open_clip_train.logger import setup_logging\n",
    "from datacat4ml.Scripts.model_dev.rep_open_clip.src.open_clip_train.params import parse_args\n",
    "#--> from open_clip_train.scheduler import cosine_lr, const_lr, const_lr_cooldown\n",
    "#--> from open_clip_train.train import train_one_epoch, evaluate\n",
    "#--> from open_clip_train.file_utils import pt_load, check_exists, start_sync_process, remote_sync\n",
    "\n",
    "#################   Yu's ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_CHECKPOINT_NAME = \"epoch_latest.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed=42, rank=0):\n",
    "    torch.manual_seed(seed + rank)\n",
    "    np.random.seed(seed + rank)\n",
    "    random.seed(seed + rank)\n",
    "\n",
    "def natural_key(string_):\n",
    "    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]\n",
    "\n",
    "def get_latest_checkpoint(path: str, remote : bool):\n",
    "    # as writen, this glob recurses, so can pick up checkpoints across multiple sub-folders\n",
    "    if remote:\n",
    "        result = subprocess.run([\"aws\", \"s3\", \"ls\", path + \"/\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(result)\n",
    "        if result.returncode == 1:\n",
    "            return None\n",
    "        checkpoints = [os.path.join(path, x.split(' ')[-1]) for x in result.stdout.decode().split('\\n')[:-1]]\n",
    "    else:\n",
    "        checkpoints = glob.glob(path + '**/*.pt', recursive=True)\n",
    "    if checkpoints:\n",
    "        checkpoints = sorted(checkpoints, key=natural_key)\n",
    "        return checkpoints[-1]\n",
    "    return None\n",
    "\n",
    "def copy_codebase(args):\n",
    "    \"\"\"\n",
    "    Create a backup of the current codebase (source files) into a specified directory. \n",
    "    It is often used in machine learning experiments to ensure reproducibility by saving a snapshot of the code that was used to produce specific results\n",
    "    \"\"\"\n",
    "    from shutil import copytree, ignore_patterns\n",
    "    new_code_path = os.path.join(args.logs, args.name, \"code\")\n",
    "    if os.path.exists(new_code_path):\n",
    "        print(\n",
    "            f\"Error. Experiment already exists at {new_code_path}. Use --name to specify a new experiment.\"\n",
    "        )\n",
    "        return -1\n",
    "    print(f\"Copying codebase to {new_code_path}\")\n",
    "    current_code_path = os.path.realpath(__file__)\n",
    "    for _ in range(3):\n",
    "        current_code_path = os.path.dirname(current_code_path)\n",
    "    copytree(current_code_path, new_code_path, ignore=ignore_patterns('log', 'logs', 'wandb'))\n",
    "    print(\"Done copying code.\")\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `def main(args):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"\n",
    "    Yu: Be careful the differnce between `args` and `params` in the original code\n",
    "    \"\"\"\n",
    "    args = parse_args(args)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # This enables tf32 on Ampere GPUs which is only 8% slower than\n",
    "        # float16 and almost as accurate as float32\n",
    "        # This was a default in pytorch until 1.12\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # Fully initialize distributed device environment\n",
    "    device = init_distributed_device(args)\n",
    "\n",
    "    # Get the name of the experiments\n",
    "    if args.name is None:\n",
    "        # sanitize model name for filesystem / uri use, easier if we don't use / in name as a rule?\n",
    "        smi_model_name_safe = args.model_name.replace(\"/\", \"_\")\n",
    "        date_str = datetime.now().strftime(\"%Y_%m_%d-%_H%M_%S\")\n",
    "        if args.distributed: #--> Yu's: no argument 'distributed' defined in `params.py``\n",
    "            # sync data_str from master to all ranks\n",
    "            date_str = broadcast_object(date_str, device)\n",
    "        args.name = '-'.join([\n",
    "            date_str,\n",
    "            f\"smi_model_{smi_model_name_safe}\",\n",
    "            f\"lr_{args.lr}\",\n",
    "            f\"b_{args.batch_size}\", #--> Yu's: be careful, it is 'batch-size' in `params.py`\n",
    "            f\"j_{args.workers}\",\n",
    "            f\"p_{args.precision}\",\n",
    "        ])\n",
    "\n",
    "    resume_latest = args.resume == \"latest\"\n",
    "    log_base_path  = os.path.join(args.logs, args.name)\n",
    "    args.log_path = None #--> Yu's: no argument 'log_path' defined in `params.py`\n",
    "    if is_master(args, local=args.log_local): #--> Yu's: be careful, it is 'log-local' in `params.py`\n",
    "        os.makedirs(log_base_path, exist_ok=True)\n",
    "        log_filename = f'out-{args.rank}' if args.log_local else 'out.log' #--> Yu's: no argument 'rank' defined; and it is 'log-local' in `params.py`\n",
    "        args.log_path = os.path.join(log_base_path, log_filename)\n",
    "        if os.path.exists(args.log_path) and not resume_latest:\n",
    "            print(\n",
    "                \"Error. Experiment already exists. Use --name {} to specify a new experiment.\"\n",
    "            )\n",
    "            return -1   \n",
    "\n",
    "    # Setup text logger\n",
    "    args.log_level = logging.DEBUG if args.debug else logging.INFO #--> Yu's: no argument 'log_level' defined in `params.py`\n",
    "    setup_logging(args.log_path, args.log_level)\n",
    "\n",
    "    # Setup wandb, tensorboard, checkpoint logging\n",
    "    args.wandb = 'wandb' in args.report_to or 'all' in args.report_to\n",
    "    args.tensorboard = 'tensorboard' in args.report_to or 'all' in args.report_to\n",
    "    args.checkpoint_path = os.path.join(log_base_path, \"checkpoints\")\n",
    "    if is_master(args):\n",
    "        args.tensorboard_path = os.path.join(log_base_path, \"tensorboard\") if args.tensorboard else ''\n",
    "        for dirname in [args.tensorboard_path, args.checkpoint_path]:\n",
    "            if dirname:\n",
    "                os.makedirs(dirname, exist_ok=True)\n",
    "    else:\n",
    "        args.tensorboard_path = ''\n",
    "\n",
    "    if resume_latest:\n",
    "        resume_from = None\n",
    "        checkpoint_path = args.checkpoint_path\n",
    "\n",
    "        #################   Yu's ##########################\n",
    "        # Delete the code snippet below if the argument 'remote_sync' is not used\n",
    "        # If using remote_sync, need to check the remote instead of the local checkpoints folder.\n",
    "        #if args.remote_sync is not None:\n",
    "        #    checkpoint_path = os.path.join(args.remote_sync, args.name, \"checkpoints\")\n",
    "        #    if args.save_most_recent:\n",
    "        #        print('Error. Cannot use save-most-recent with remote_sync and resume latest.')\n",
    "        #        return -1\n",
    "        #    if args.remote_sync_protocol != 's3':\n",
    "        #        print('Error. Sync protocol not supported when using resume latest.')\n",
    "        #        return -1\n",
    "        #################   Yu's ##########################\n",
    "        if is_master(args):\n",
    "            # Checking for existing checkpoint via master rank only. It is possible for\n",
    "            # different rank processes to see different files if a shared file-system is under\n",
    "            # stress, however it's very difficult to fully work around such situations.\n",
    "            if args.save_most_recent:\n",
    "                # if --save-most-recent flag is set, look for latest at a fixed filename\n",
    "                resume_from = os.path.join(checkpoint_path, LATEST_CHECKPOINT_NAME)\n",
    "                if not os.path.exists(resume_from):\n",
    "                    # If no latest checkpoint has been saved yet, don't try to resume\n",
    "                    resume_from = None\n",
    "            else:\n",
    "                # otherwise, list checkpoint dir contents and pick the newest checkpoint\n",
    "                resume_from = get_latest_checkpoint(checkpoint_path, remote=args.remote_sync is not None)\n",
    "            if resume_from:\n",
    "                logging.info(f'Found latest resume checkpoint at {resume_from}.')\n",
    "            else:\n",
    "                logging.info(f'No latest resume checkpoint found in {checkpoint_path}.')\n",
    "        if args.distributed:\n",
    "            # sync found checkpoint path to all ranks\n",
    "            resume_from = broadcast_object(args, resume_from)\n",
    "        args.resume = resume_from\n",
    "\n",
    "    if args.copy_codebase:\n",
    "        copy_codebase(args)\n",
    "\n",
    "    if args.precision == 'fp16':\n",
    "        logging.warning(\n",
    "            'It is recommended to use AMP mixed-precision instead of FP16. '\n",
    "            'FP16 support needs further verification and tuning, especially for train.')\n",
    "\n",
    "    if args.horovod:\n",
    "        logging.info(\n",
    "            f'Running in horovod mode with multiple processes / nodes. Device: {args.device}.'\n",
    "            f'Process (global: {args.rank}, local {args.local_rank}), total {args.world_size}.')\n",
    "    elif args.distributed:\n",
    "        logging.info(\n",
    "            f'Running in distributed mode with multiple processes. Device: {args.device}.'\n",
    "            f'Process (global: {args.rank}, local {args.local_rank}), total {args.world_size}.')\n",
    "    else:\n",
    "        logging.info(f'Running with a single process. Device {args.device}.')\n",
    "\n",
    "    dist_model = None\n",
    "    args.distill = args.distill_model is not None and args.distill_pretrained is not None\n",
    "    if args.distill:\n",
    "        #FIXME: support distillation with grad accum.\n",
    "        assert args.accum_freq == 1\n",
    "        #FIXME: support distillation with coca.\n",
    "        assert 'coca' not in args.model.lower()\n",
    "\n",
    "    #################   Yu's ##########################\n",
    "    # the `image` related arguments should be replaced by `smiles` related arguments\n",
    "    if isinstance(args.force_image_size, (tuple, list)) and len(args.force_image_size) == 1:\n",
    "        # arg is nargs, single (square) image size list -> int\n",
    "        args.force_image_size = args.force_image_size[0]\n",
    "    #################   Yu's ##########################\n",
    "    random_seed(args.seed, 0)\n",
    "    model_kwargs = {}\n",
    "    if args.siglip:\n",
    "        model_kwargs['init_logit_scale'] = np.log(10)  # different from CLIP\n",
    "        model_kwargs['init_logit_bias'] = -10\n",
    "    \n",
    "    #################   Yu's ##########################\n",
    "    #--> need to modify the function `create_model_and_transforms` to be with SMILES encoder\n",
    "    model, preprocess_train, preprocess_val = create_model_and_transforms(\n",
    "        args.model,\n",
    "        args.pretrained,\n",
    "        precision=args.precision,\n",
    "        device=device,\n",
    "        jit=args.torchscript,\n",
    "        force_quick_gelu=args.force_quick_gelu,\n",
    "        force_custom_text=args.force_custom_text,\n",
    "        force_patch_dropout=args.force_patch_dropout,\n",
    "        force_image_size=args.force_image_size,\n",
    "        image_mean=args.image_mean,\n",
    "        image_std=args.image_std,\n",
    "        image_interpolation=args.image_interpolation,\n",
    "        image_resize_mode=args.image_resize_mode,  # only effective for inference\n",
    "        aug_cfg=args.aug_cfg,\n",
    "        pretrained_image=args.pretrained_image,\n",
    "        output_dict=True,\n",
    "        cache_dir=args.cache_dir,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    #################   Yu's ##########################\n",
    "\n",
    "    if args.distill:\n",
    "        # FIXME: currently assumes the model you're distilling from has the same tokenizer & transforms.\n",
    "        dist_model, _, _ = create_model_and_transforms(\n",
    "            args.distill_model, \n",
    "            args.distill_pretrained,\n",
    "            device=device,\n",
    "            precision=args.precision,\n",
    "            output_dict=True,\n",
    "            cache_dir=args.cache_dir,\n",
    "        )\n",
    "    if args.use_bnb_linear is not None:\n",
    "        print('=> using a layer from bitsandbytes.\\n'\n",
    "              '   this is an experimental feature which requires two extra pip installs\\n'\n",
    "              '   pip install bitsandbytes triton'\n",
    "              '   please make sure to use triton 2.0.0')\n",
    "        import bitsandbytes as bnb\n",
    "        from open_clip.utils import replace_linear\n",
    "        print(f'=> replacing linear layers with {args.use_bnb_linear}')\n",
    "        linear_replacement_cls = getattr(bnb.nn.triton_based_modules, args.use_bnb_linear)\n",
    "        replace_linear(model, linear_replacement_cls)\n",
    "        model = model.to(device)\n",
    "\n",
    "    random_seed(args.seed, args.rank)\n",
    "\n",
    "    if args.trace:\n",
    "        model = trace_model(model, batch_size=args.batch_size, device=device)\n",
    "\n",
    "    #################   Yu's ##########################\n",
    "    #--> need to modify the function `lock_image_tower` to be with SMILES encoder\n",
    "    #--> if args.lock_image:\n",
    "    #-->     # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n",
    "    #-->     model.lock_image_tower(\n",
    "    #-->         unlocked_groups=args.lock_image_unlocked_groups,\n",
    "    #-->         freeze_bn_stats=args.lock_image_freeze_bn_stats)\n",
    "    #################   Yu's ##########################\n",
    "    if args.lock_text:\n",
    "        model.lock_text_tower(\n",
    "            unlocked_layers=args.lock_text_unlocked_layers,\n",
    "            freeze_layer_norm=args.lock_text_freeze_layer_norm)\n",
    "    \n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing()\n",
    "\n",
    "    if is_master(args):\n",
    "        logging.info(\"Model:\")\n",
    "        logging.info(f\"{str(model)}\")\n",
    "        logging.info(\"Params:\")\n",
    "        params_file = os.path.join(args.logs, args.name, \"params.txt\")\n",
    "        with open(params_file, \"w\") as f:\n",
    "            for name in sorted(vars(args)):\n",
    "                val = getattr(args, name)\n",
    "                logging.info(f\"  {name}: {val}\")\n",
    "                f.write(f\"{name}: {val}\\n\")\n",
    "\n",
    "    if args.distributed and not args.horovod:\n",
    "        if args.use_bn_sync:\n",
    "            model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        ddp_args = {}\n",
    "        if args.ddp_static_graph:\n",
    "            # this doesn't exist in older PyTorch, arg only added if enabled\n",
    "            ddp_args['static_graph'] = True\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[device], **ddp_args)\n",
    "    \n",
    "        if args.distill:\n",
    "            dist_model = torch.nn.parallel.DistributedDataParallel(dist_model, device_ids=[device], **ddp_args)\n",
    "    \n",
    "    ####\n",
    "    # ....\n",
    "    ####\n",
    "\n",
    "    # initialize datasets\n",
    "    tokenizer = get_tokenizer(args.model, cache_dir=args.cache_dir)\n",
    "    data = get_data(\n",
    "        args,\n",
    "        (preprocess_train, preprocess_val),\n",
    "        epoch=start_epoch,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    assert len(data), 'At least one train or eval dataset must be specified.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run `main`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sys.argv[0]`: The name of the script\n",
    "- `sys.argv[1:]`: contains the actual arguments passed to the script, i.e. everything after the script name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train-data TRAIN_DATA]\n",
      "                             [--val-data VAL_DATA]\n",
      "                             [--train-num-samples TRAIN_NUM_SAMPLES]\n",
      "                             [--val-num-samples VAL_NUM_SAMPLES]\n",
      "                             [--dataset-type {webdataset,csv,synthetic,auto}]\n",
      "                             [--csv-separator CSV_SEPARATOR]\n",
      "                             [--csv-img-key CSV_IMG_KEY]\n",
      "                             [--csv-caption-key CSV_CAPTION_KEY]\n",
      "                             [--imagenet-val IMAGENET_VAL]\n",
      "                             [--cache-dir CACHE_DIR] [--logs LOGS]\n",
      "                             [--log-local] [--name NAME] [--workers WORKERS]\n",
      "                             [--batch-size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--epochs-cooldown EPOCHS_COOLDOWN] [--lr LR]\n",
      "                             [--beta1 BETA1] [--beta2 BETA2] [--eps EPS]\n",
      "                             [--wd WD] [--momentum MOMENTUM] [--warmup WARMUP]\n",
      "                             [--opt OPT] [--use-bn-sync] [--skip-scheduler]\n",
      "                             [--lr-scheduler LR_SCHEDULER]\n",
      "                             [--lr-cooldown-end LR_COOLDOWN_END]\n",
      "                             [--lr-cooldown-power LR_COOLDOWN_POWER]\n",
      "                             [--save-frequency SAVE_FREQUENCY]\n",
      "                             [--save-most-recent]\n",
      "                             [--zeroshot-frequency ZEROSHOT_FREQUENCY]\n",
      "                             [--val-frequency VAL_FREQUENCY] [--resume RESUME]\n",
      "                             [--precision {amp,amp_bf16,amp_bfloat16,bf16,fp16,pure_bf16,pure_fp16,fp32}]\n",
      "                             [--pretrained PRETRAINED]\n",
      "                             [--aug-cfg [AUG_CFG ...]] [--grad-checkpointing]\n",
      "                             [--local-loss] [--gather-with-grad]\n",
      "                             [--force-quick-gelu]\n",
      "                             [--force-patch-dropout FORCE_PATCH_DROPOUT]\n",
      "                             [--force-custom-text] [--torchscript]\n",
      "                             [--torchcompile] [--trace]\n",
      "                             [--accum-freq ACCUM_FREQ] [--device DEVICE]\n",
      "                             [--dist-url DIST_URL]\n",
      "                             [--dist-backend DIST_BACKEND]\n",
      "                             [--report-to REPORT_TO]\n",
      "                             [--wandb-notes WANDB_NOTES]\n",
      "                             [--wandb-project-name WANDB_PROJECT_NAME]\n",
      "                             [--debug] [--copy-codebase] [--horovod]\n",
      "                             [--ddp-static-graph] [--no-set-device-rank]\n",
      "                             [--seed SEED] [--grad-clip-norm GRAD_CLIP_NORM]\n",
      "                             [--lock-text]\n",
      "                             [--lock-text-unlocked-layers LOCK_TEXT_UNLOCKED_LAYERS]\n",
      "                             [--lock-text-freeze-layer-norm]\n",
      "                             [--log-every-n-steps LOG_EVERY_N_STEPS]\n",
      "                             [--remote-sync REMOTE_SYNC]\n",
      "                             [--remote-sync-frequency REMOTE_SYNC_FREQUENCY]\n",
      "                             [--remote-sync-protocol {s3,fsspec}]\n",
      "                             [--delete-previous-checkpoint]\n",
      "                             [--distill-model DISTILL_MODEL]\n",
      "                             [--distill-pretrained DISTILL_PRETRAINED]\n",
      "                             [--use-bnb-linear USE_BNB_LINEAR] [--siglip]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/storage/homefs/yc24j783/.local/share/jupyter/runtime/kernel-v37af05cdc230e10d70653743b89e0d93c8c8541e7.json could match --force-quick-gelu, --force-patch-dropout, --force-custom-text\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main(sys.argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
