{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner module import\n",
    "import sys\n",
    "sys.path.append(\"/storage/homefs/yc24j783/datacat4ml/datacat4ml\")\n",
    "from const import FETCH_DATA_DIR, FETCH_FIG_DIR, FEATURIZE_DATA_DIR, FEATURIZE_FIG_DIR\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assay_id</th>\n",
       "      <th>assay_chembl_id</th>\n",
       "      <th>tid</th>\n",
       "      <th>target_chembl_id</th>\n",
       "      <th>standard_type</th>\n",
       "      <th>pchembl_value</th>\n",
       "      <th>assay_type</th>\n",
       "      <th>assay_category</th>\n",
       "      <th>assay_organism</th>\n",
       "      <th>assay_tax_id</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_type</th>\n",
       "      <th>aidx</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>molregno</th>\n",
       "      <th>compound_chembl_id</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>assay_info_hash</th>\n",
       "      <th>ecfp4</th>\n",
       "      <th>map4c</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147162</td>\n",
       "      <td>CHEMBL753852</td>\n",
       "      <td>136</td>\n",
       "      <td>CHEMBL236</td>\n",
       "      <td>Ki</td>\n",
       "      <td>6.96</td>\n",
       "      <td>B</td>\n",
       "      <td>None</td>\n",
       "      <td>Cavia porcellus</td>\n",
       "      <td>10141</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>CLD0</td>\n",
       "      <td>8</td>\n",
       "      <td>1798744</td>\n",
       "      <td>CHEMBL3350133</td>\n",
       "      <td>CN1CC[C@]23c4c5ccc(O)c4O[C@H]2[C@@H](NC(=O)CNC...</td>\n",
       "      <td>d5fdf976cc6fd98f7656c177bcab9fc2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2421162, 248707, 1555374, 3026370, 1608673, 1...</td>\n",
       "      <td>intermediate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   assay_id assay_chembl_id  tid target_chembl_id standard_type  \\\n",
       "0    147162    CHEMBL753852  136        CHEMBL236            Ki   \n",
       "\n",
       "   pchembl_value assay_type assay_category   assay_organism assay_tax_id  ...  \\\n",
       "0           6.96          B           None  Cavia porcellus        10141  ...   \n",
       "\n",
       "  relationship_type  aidx confidence_score molregno compound_chembl_id  \\\n",
       "0                 H  CLD0                8  1798744      CHEMBL3350133   \n",
       "\n",
       "                                    canonical_smiles  \\\n",
       "0  CN1CC[C@]23c4c5ccc(O)c4O[C@H]2[C@@H](NC(=O)CNC...   \n",
       "\n",
       "                    assay_info_hash  \\\n",
       "0  d5fdf976cc6fd98f7656c177bcab9fc2   \n",
       "\n",
       "                                               ecfp4  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               map4c      activity  \n",
       "0  [2421162, 248707, 1555374, 3026370, 1608673, 1...  intermediate  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load featurized data\n",
    "ki_maxcur_df = pd.read_pickle(os.path.join(FEATURIZE_DATA_DIR, 'ki_maxcur_8_fp.pkl'))\n",
    "ki_maxcur_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    d5fdf976cc6fd98f7656c177bcab9fc2\n",
      "Name: assay_info_hash, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ki_maxcur_df['assay_info_hash'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpcr: 1642196    121\n",
      "596087     112\n",
      "1468240    110\n",
      "1613477    107\n",
      "447747     106\n",
      "          ... \n",
      "1688606      1\n",
      "700881       1\n",
      "1688609      1\n",
      "1688610      1\n",
      "3051         1\n",
      "Name: assay_id, Length: 14223, dtype: int64\n",
      "gpcr shape: (139416, 31)\n",
      "mor: 1642108    91\n",
      "1641066    90\n",
      "1536477    79\n",
      "540743     65\n",
      "443967     54\n",
      "           ..\n",
      "439345      1\n",
      "939900      1\n",
      "447457      1\n",
      "878643      1\n",
      "1869947     1\n",
      "Name: assay_id, Length: 459, dtype: int64\n",
      "mor shape: (4487, 31)\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpcr: {ki_maxcur_df['assay_id'].value_counts()}\")\n",
    "print(f\"gpcr shape: {ki_maxcur_df.shape}\")\n",
    "mor_ki_maxcur_df = ki_maxcur_df[ki_maxcur_df['target_chembl_id'] == 'CHEMBL233']\n",
    "print(f\"mor: {mor_ki_maxcur_df['assay_id'].value_counts()}\")\n",
    "print(f\"mor shape: {mor_ki_maxcur_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize SMILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from OpenNMT-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C N 1 C C [C@] 2 3 c 4 c 5 c c c ( O ) c 4 O [C@H] 2 [C@@H] ( N C ( = O ) C N C ( = O ) C C C ( = O ) N C C ( = O ) N [C@H] 2 C C [C@@] 4 ( O ) [C@H] 6 C c 7 c c c ( O ) c 8 c 7 [C@@] 4 ( C C N 6 C ) [C@H] 2 O 8 ) C C [C@@] 3 ( O ) [C@H] 1 C 5',\n",
       " 'C O c 1 c c c ( N C ( = O ) c 2 c c c ( - c 3 c c c ( - c 4 n o c ( C ) n 4 ) c c 3 C ) c c 2 ) c c 1 N 1 C C N ( C ) C C 1',\n",
       " 'C O c 1 c c c ( N C ( = O ) c 2 c c c ( - c 3 c c c ( - c 4 n o c ( C ) n 4 ) c c 3 C ) c c 2 ) c c 1 N 1 C C N ( C ) C C 1',\n",
       " 'C O c 1 c c c ( N C ( = O ) c 2 c c c ( - c 3 c c c ( - c 4 n o c ( C ) n 4 ) c c 3 C ) c c 2 ) c c 1 N 1 C C N ( C ) C C 1',\n",
       " 'C C C c 1 n c ( C C ) c ( C ( = O ) N C ) n 1 C c 1 c c c ( - c 2 c c c c c 2 S ( = O ) ( = O ) N c 2 o n c ( C ) c 2 C ) c ( C ) c 1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smis = ki_maxcur_df['canonical_smiles'].values\n",
    "tokenized_smis = [smi_tokenizer(smi) for smi in smis]\n",
    "tokenized_smis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From ChemBerta Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemberta_tokenizer(smi:str, max_smi_len: int=200, padding: bool=True, truncation: bool=True, \n",
    "                        auto_tokenizer: str = 'seyonec/PubChem10M_SMILES_BPE_450k'):\n",
    "    \"\"\"\n",
    "    Tokenize SMILES for a ChemBerta Transformer\n",
    "    \n",
    "    :param smi: (str)SMILES string\n",
    "    :param max_smi_len: (int) maximum SMILES length\n",
    "    :param padding: (bool) padding\n",
    "    :param truncation: (bool) allow truncation (you will need this for heterogenous SMILES strings)\n",
    "    :param auto_tokenizer: (str) tokenizer name provided by HuggingFace\n",
    "    \"\"\"\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(auto_tokenizer)\n",
    "    tokens = tokenizer(smi, return_tensors='pt', padding=padding, truncation=truncation, max_length=max_smi_len)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 13:36:24.730795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a7b49e1a3943a2860b226ddad182c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6230dd4e442444fbc71d421aca7b1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/515 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027f40805b59493e99636f280845d7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/165k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ee3ea9b1484c8cb9cdaca49020854b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/101k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5b0115175c4472a33697c5431c977f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smis = ki_maxcur_df['canonical_smiles'].values\n",
    "tokenized_smis = [chemberta_tokenizer(smi) for smi in smis]\n",
    "tokenized_smis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize assay-related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[147162, 957, 1732, 1360, 65644]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assay_ids = ki_maxcur_df['assay_id'].values.tolist()\n",
    "assay_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of assay_descs is <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Displacement of [3H]EK from Opioid receptor delta 1 in guinea pig brain membrane',\n",
       " 'Affinity for 5-hydroxytryptamine 1A receptor subtype',\n",
       " 'Affinity for 5-hydroxytryptamine 1D receptor subtype',\n",
       " 'Affinity for 5-hydroxytryptamine 1B receptor subtype',\n",
       " 'Binding affinity towards human ETA receptor expressed in CHO-K1 cells in the presence of 0.05 nM [125I]-labeled endothelin 1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assay_descs = ki_maxcur_df['assay_desc'].values.tolist()\n",
    "print(f\"The data type of assay_descs is {type(assay_descs)}\")\n",
    "# Create a temporary file to store assay descriptions\n",
    "with open('assay_descs.txt', 'w') as f:\n",
    "    for desc in assay_descs:\n",
    "        f.write(desc + '\\n')\n",
    "assay_descs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# customize pre-tokenizer and decoder\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "# train tokenizer\n",
    "trainer = trainers.BpeTrainer(vocab_size=9000, min_frequency=2, limit_alphabet=55, special_tokens=['affinity', 'displacement', '3H', '125I', 'camp', 'gtp', 'calcium', 'ca2+', 'IP1', 'IP3', 'arrest', 'agonist'])\n",
    "tokenizer.train(['assay_descs.txt'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assay_desc_tokenizer(sentence):\n",
    "    '''Tokenize a sentense, optimized for assay description'''\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    my_list = [item for item in encoded.tokens] \n",
    "    my_list = ' '.join(my_list)\n",
    "\n",
    "    return my_list\n",
    "\n",
    "def enzyme_sentence_tokenizer(sentence):\n",
    "    '''\n",
    "    Tokenize a sentenze, optimized for enzyme-like descriptions & names\n",
    "    '''\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    my_list = [item for item in encoded.tokens if 'Ä ' != item]\n",
    "    my_list = [item.replace('Ä ', '_') for item in my_list]\n",
    "    my_list = ' '.join(my_list)\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Displacement _of _[ 3H ] EK _from _Opioid _receptor _delta _1 _in _guinea _pig _brain _membrane',\n",
       " '_Affinity _for _5 - hydroxytryptamine _1 A _receptor _subtype',\n",
       " '_Affinity _for _5 - hydroxytryptamine _1 D _receptor _subtype',\n",
       " '_Affinity _for _5 - hydroxytryptamine _1 B _receptor _subtype',\n",
       " '_Binding affinity _towards _human _ETA _receptor _expressed _in _CHO - K 1 _cells _in _the _presence _of _0 . 05 _nM _[ 125I ]- labeled _endothelin _1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_assay_descs = [enzyme_sentence_tokenizer(assay_desc) for assay_desc in assay_descs]\n",
    "tokenized_assay_descs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identifiers in assay table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the tokenized SMILES and tokenized assay-related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES + assay_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES + assay-desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES + assay-related info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training, validation, and testing sets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
