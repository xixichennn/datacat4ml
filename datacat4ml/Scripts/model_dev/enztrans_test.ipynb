{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner module import\n",
    "import sys\n",
    "sys.path.append(\"/storage/homefs/yc24j783/datacat4ml/datacat4ml\")\n",
    "from const import FETCH_DATA_DIR, FETCH_FIG_DIR, FEATURIZE_DATA_DIR, FEATURIZE_FIG_DIR\n",
    "# print current work directory\n",
    "workdir = os.getcwd()\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load featurized data\n",
    "ki_mor_1_df = pd.read_pickle(os.path.join(FEATURIZE_DATA_DIR, 'ki_maxcur', 'ki_target_CHEMBL233_1_fp.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of ki_mor_1_df is (298, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assay_id</th>\n",
       "      <th>assay_chembl_id</th>\n",
       "      <th>tid</th>\n",
       "      <th>target_chembl_id</th>\n",
       "      <th>standard_type</th>\n",
       "      <th>pchembl_value</th>\n",
       "      <th>assay_type</th>\n",
       "      <th>assay_category</th>\n",
       "      <th>assay_organism</th>\n",
       "      <th>assay_tax_id</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_type</th>\n",
       "      <th>aidx</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>molregno</th>\n",
       "      <th>compound_chembl_id</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>assay_info_hash</th>\n",
       "      <th>ecfp4</th>\n",
       "      <th>map4c</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148100</td>\n",
       "      <td>CHEMBL751582</td>\n",
       "      <td>129</td>\n",
       "      <td>CHEMBL233</td>\n",
       "      <td>Ki</td>\n",
       "      <td>4.97</td>\n",
       "      <td>B</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>CLD0</td>\n",
       "      <td>8</td>\n",
       "      <td>176210</td>\n",
       "      <td>CHEMBL108417</td>\n",
       "      <td>CC(c1ccccc1)N1CC[C@H]1[C@@H](N)c1cccc(Cl)c1</td>\n",
       "      <td>901e030c965234752af5ce2b09ef2dea</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9923915, 4152229, 2453811, 1604050, 9383906, ...</td>\n",
       "      <td>inactive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   assay_id assay_chembl_id  tid target_chembl_id standard_type  \\\n",
       "0    148100    CHEMBL751582  129        CHEMBL233            Ki   \n",
       "\n",
       "   pchembl_value assay_type assay_category assay_organism assay_tax_id  ...  \\\n",
       "0           4.97          B           None           None         None  ...   \n",
       "\n",
       "  relationship_type  aidx confidence_score molregno compound_chembl_id  \\\n",
       "0                 H  CLD0                8   176210       CHEMBL108417   \n",
       "\n",
       "                              canonical_smiles  \\\n",
       "0  CC(c1ccccc1)N1CC[C@H]1[C@@H](N)c1cccc(Cl)c1   \n",
       "\n",
       "                    assay_info_hash  \\\n",
       "0  901e030c965234752af5ce2b09ef2dea   \n",
       "\n",
       "                                               ecfp4  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               map4c  activity  \n",
       "0  [9923915, 4152229, 2453811, 1604050, 9383906, ...  inactive  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The shape of ki_mor_1_df is {ki_mor_1_df.shape}')\n",
    "ki_mor_1_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assay_id\n",
       "148100    33\n",
       "148222    22\n",
       "148248    21\n",
       "148089    20\n",
       "495338    19\n",
       "458378    18\n",
       "148088    17\n",
       "148097    17\n",
       "878883    15\n",
       "149492    12\n",
       "563750    11\n",
       "767030    10\n",
       "611744     9\n",
       "148229     7\n",
       "148226     7\n",
       "559849     6\n",
       "753181     6\n",
       "753180     6\n",
       "753179     6\n",
       "148102     6\n",
       "138858     5\n",
       "448535     4\n",
       "148841     4\n",
       "753173     3\n",
       "148093     2\n",
       "148234     2\n",
       "221913     1\n",
       "149326     1\n",
       "148236     1\n",
       "148246     1\n",
       "148095     1\n",
       "148244     1\n",
       "148245     1\n",
       "148243     1\n",
       "148094     1\n",
       "148247     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ki_mor_1_df['assay_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assay_desc\n",
       "Displacement of [3H]DAMGO from mu opioid receptor expressed in CHO cells                                                                                    44\n",
       "Binding affinity at human opioid receptor mu 1 was determined by using [3H]diprenorphine radioligand in CHO cell membranes at a concentration of 0.12 nM    33\n",
       "In vitro binding affinity to human Opioid receptor mu 1 on CHO cell membranes using [3H]diprenorphine displacement.                                         22\n",
       "Inhibitory activity against Opioid receptor mu 1 in chinese Hamster Ovary (CHO) cells membranes was determined using [3H]-DAMGO radioligand                 21\n",
       "Ability to displace [3H]DAMGO from human recombinant Opioid receptor mu 1 in CHO cells                                                                      20\n",
       "Displacement of [3H]naloxone from monocloned mu opioid receptor expressed in CHO cells                                                                      19\n",
       "Displacement of [3H]DAMGO from human recombinant Opioid receptor mu 1 on CHO cell membranes.                                                                17\n",
       "Binding affinity against human Opioid receptor mu 1 on CHO cell membranes was determined by [3H]DAMGO displacement.                                         17\n",
       "Displacement of [3H]Naloxone from mu opioid receptor expressed in CHO cells after 1 hr                                                                      15\n",
       "Binding affinity towards human mu-opioid receptors in CHO (Chinese hamster ovary) cell lines                                                                12\n",
       "Displacement of [3H]naloxone from mu opioid receptor expressed in CHO cells after 1 hr                                                                      10\n",
       "Binding affinity towards recombinant human Opioid receptor mu 1 transfected in to CHO cells for the displacement of [3H]DAMGO (mu)                           7\n",
       "Binding affinity towards cloned human Opioid receptor mu 1 in CHO cell membranes.                                                                            7\n",
       "Displacement of [125I]-IBOxyA from MOR-1 expressed in CHO cells                                                                                              6\n",
       "Displacement of [125I]-IBNalA from MOR-1 expressed in CHO cells                                                                                              6\n",
       "Displacement of [125I]-IBNtxA from MOR-1 expressed in CHO cells                                                                                              6\n",
       "Binding affinity for human opioid receptor mu 1 using [3H]- DAMGO as radioligand transfected into CHO cells                                                  6\n",
       "Binding affinity towards human opioid Mu receptor transfected into Chinese hamster ovary (CHO) cells using [3H]DAMGO as radioligand                          5\n",
       "Affinity towards human Opioid receptor mu 1 on CHO cell membranes using [3H]DAMGO displacement.                                                              4\n",
       "Displacement of [3H]DAMGO from cloned mu opioid receptor expressed in CHO cell membrane                                                                      4\n",
       "Displacement of [3H]-DAMGO from MOR-1 expressed in CHO cells after 150 mins                                                                                  3\n",
       "Binding affinity against cloned human Opioid receptor mu 1 transfected onto CHO cells using [3H]DAMGO                                                        2\n",
       "Compound was evaluated for its binding affinity against CHO cells transfected with cloned human Opioid receptor mu 1 by displacing [3H]DAMGO                 2\n",
       "Binding affinity towards human mu opioid receptor in CHO cells using [3H]- DAMGO as radioligand                                                              1\n",
       "Inhibition of [3H]- diprenorphine binding to Opioid receptor mu 1 (83 fmol/mg protein) stably expressed in membranes from CHO cells                          1\n",
       "Compound was evaluated for its binding affinity to CHO cells expressing cloned human Opioid receptor mu 1 by displacing [3H]-DAMGO                           1\n",
       "Inhibition of binding of [3H]diprenorphine to cloned human Opioid receptor mu 1 expressed in CHO cell membrane;Range is in between (580-680)                 1\n",
       "Inhibition of binding of [3H]diprenorphine to cloned human Opioid receptor mu 1 expressed in CHO cell membrane;Range is in between (6.8-43)                  1\n",
       "Inhibition of binding of [3H]diprenorphine to cloned human Opioid receptor mu 1 expressed in CHO cell membrane;Range is in between (1.8-2.2)                 1\n",
       "Binding affinity against cloned human opioid mu receptors transfected onto CHO cells using [3H]DAMGO                                                         1\n",
       "Inhibition of binding of [3H]diprenorphine to cloned human Opioid receptor mu 1 expressed in CHO cell membrane;Range is in between (1.3-3.7)                 1\n",
       "Binding affinity against cloned human opioid Opioid receptor mu 1 transfected onto CHO cells using [3H]DAMGO                                                 1\n",
       "Inhibition of binding of [3H]diprenorphine to cloned human Opioid receptor mu 1 expressed in CHO cell membrane;Range is in between (110-150)                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ki_mor_1_df['assay_desc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenizer(smi:str):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES string using regular expression.\n",
    "\n",
    "    The pros of this method is that it is simple and fast.\n",
    "\n",
    "    :param smi: SMILES string\n",
    "\n",
    "    :return: list of tokens\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a new column 'regex_token_smi' to store the tokenized SMILES\n",
    "ki_mor_1_df['regex_token_smi'] = ki_mor_1_df['canonical_smiles'].apply(regex_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemberta_tokenizer(smi: str, max_smi_len: int = 200, padding: bool = True, truncation: bool = True,\n",
    "               auto_tokenizer: str = 'seyonec/PubChem10M_SMILES_BPE_450k'):\n",
    "        \"\"\" Tokenize SMILES for a ChemBerta Transformer\n",
    "\n",
    "        The pros of this method compared to the the one based on regular expression is that\n",
    "        it leverages pre-trained knowledge for handling complex SMILES syntax like aromatic rings, nested branches, etc.\n",
    "\n",
    "        :param max_smiles_length: (int) Maximal allowable SMILES string length\n",
    "        :param padding: (bool) allow padding\n",
    "        :param truncation: (bool) allow truncation (you will need this for heterogeneous SMILES strings)\n",
    "        :param auto_tokenizer: (str) name of the auto tokenizer provided by HuggingFace\n",
    "\n",
    "        :return: Dict['input_ids': tensor, 'attention_mask': tensor], tensors are of shape N x max_smiles_length\n",
    "        \"\"\"\n",
    "\n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        chemical_tokenizer = AutoTokenizer.from_pretrained(auto_tokenizer)\n",
    "        tokens = chemical_tokenizer(smi, padding=padding, truncation=truncation,max_length=max_smi_len,\n",
    "                                   return_tensors='pt') # `return_tensors='pt'` means to convert the output to PyTorch tensors. Removing this argument will return a dictionary of lists.\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/miniconda3/envs/enztrans_test/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[  0, 262,  12,  71,  21, 268,  21,  13,  50,  21, 262,  63,  39,  36,\n",
       "           44,  65,  21,  63,  39,  36,  36,  44, 336,  50,  13,  71,  21, 276,\n",
       "           12, 275,  13,  71,  21,   2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  0, 372,  12,  71,  21, 268,  21,  13,  50,  21, 262,  63,  39,  36,\n",
       "           44,  65,  21,  63,  39,  36,  36,  44, 336,  50,  13,  71,  21, 276,\n",
       "           12, 275,  13,  71,  21,   2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  0, 265,  21, 264,  12,  39,  12,  71,  22, 264,  12,  39,  13, 261,\n",
       "           22,  13,  50,  22, 262,  63,  39,  36,  44,  65,  22,  63,  39,  36,\n",
       "           44, 336,  50,  13,  71,  22, 276,  12, 275,  13,  71,  22,  13, 261,\n",
       "           21,   2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chemberta_token_smis = [chemberta_tokenizer(smi) for smi in smis]\n",
    "#chemberta_token_smis[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize assay_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assay_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assay_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(sent:str):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence for assay description using Tokenizer from HuggingFace. Optimze for GPCR assay description.\n",
    "\n",
    "    :param sent: (str)sentence to be tokenized\n",
    "    \"\"\"\n",
    "\n",
    "    from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "    # Initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # Customize pre-tokenization and decoding\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "    # And then train\n",
    "    trainer = trainers.BpeTrainer(special_tokens=['affinity', 'displacement', '3H', '125I', 'camp', 'gtp', 'calcium', 'ca2+', 'IP1', 'IP3', 'arrest', 'agonist'])\n",
    "    tokenizer.train_from_iterator([sent], trainer=trainer)\n",
    "\n",
    "    encoded = tokenizer.encode(sent)\n",
    "    my_list = [item for item in encoded.tokens if 'Ä ' != item]\n",
    "    my_list = [item.replace('Ä ', '_') for item in my_list]\n",
    "    my_list = ' '.join(my_list)\n",
    "\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_Binding affinity _at _human _opioid _receptor _mu _1 _was _determined _by _using _[ 3H ] diprenorphine _radioligand _in _CHO _cell _membranes _at _a _concentration _of _0 . 12 _nM',\n",
       " '_Binding affinity _at _human _opioid _receptor _mu _1 _was _determined _by _using _[ 3H ] diprenorphine _radioligand _in _CHO _cell _membranes _at _a _concentration _of _0 . 12 _nM',\n",
       " '_Binding affinity _at _human _opioid _receptor _mu _1 _was _determined _by _using _[ 3H ] diprenorphine _radioligand _in _CHO _cell _membranes _at _a _concentration _of _0 . 12 _nM']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assay_descs = ki_mor_1_df['assay_desc'].values\n",
    "sent_token_descs = [sentence_tokenizer(sent) for sent in assay_descs]\n",
    "sent_token_descs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assay-related fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate the tokenized SMILES and assay_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the ki_mor_1_df is 298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set\n",
       "TRAIN    229\n",
       "TEST      37\n",
       "VAL       32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# split the data using random sampling\n",
    "ki_mor_1_df['set'] = ''\n",
    "smis = list(ki_mor_1_df['canonical_smiles'].values)\n",
    "random_20per = random.sample(smis, int(0.2*len(smis)))\n",
    "random_10per_1 = random_20per[0:int(0.1*len(smis))]\n",
    "random_10per_2 = random_20per[int(0.1*len(smis)):len(random_20per)]\n",
    "\n",
    "# Assign the set\n",
    "ki_mor_1_df.loc[ki_mor_1_df['canonical_smiles'].isin(random_10per_1), 'set'] = 'VAL'\n",
    "ki_mor_1_df.loc[ki_mor_1_df['canonical_smiles'].isin(random_10per_2), 'set'] = 'TEST'\n",
    "ki_mor_1_df.loc[ki_mor_1_df['set'] == '', 'set'] = 'TRAIN'\n",
    "\n",
    "# check the distribution of the set\n",
    "print(f'The length of the ki_mor_1_df is {len(ki_mor_1_df)}')\n",
    "ki_mor_1_df['set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define `train_src` is the list of value in the column 'canonical_smiles' where the column 'set' is 'TRAIN'\n",
    "train_src = list(ki_mor_1_df.loc[ki_mor_1_df['set'] == 'TRAIN', 'regex_token_smi'].values)\n",
    "# save the `train_src` to a text file\n",
    "with open(os.path.join(workdir, 'enztrans_test', 'data', 'ki_mor_1', 'train_src.txt'), 'w') as f:\n",
    "    for item in train_src:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "val_src = list(ki_mor_1_df.loc[ki_mor_1_df['set'] == 'VAL', 'regex_token_smi'].values)\n",
    "with open(os.path.join(workdir, 'enztrans_test', 'data', 'ki_mor_1', 'valid_src.txt'), 'w') as f:\n",
    "    for item in val_src:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "test_src = list(ki_mor_1_df.loc[ki_mor_1_df['set'] == 'TEST', 'regex_token_smi'].values)\n",
    "with open(os.path.join(workdir, 'enztrans_test', 'data', 'ki_mor_1', 'test_src.txt'), 'w') as f:\n",
    "    for item in test_src:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "train_tgt = list(ki_mor_1_df.loc[ki_mor_1_df['set'] == 'TRAIN', 'pchembl_value'].values)\n",
    "with open(os.path.join(workdir, 'enztrans_test', 'data', 'ki_mor_1', 'train_tgt.txt'), 'w') as f:\n",
    "    for item in train_tgt:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "val_tgt = list(ki_mor_1_df.loc[ki_mor_1_df['set'] == 'VAL', 'pchembl_value'].values)\n",
    "with open(os.path.join(workdir, 'enztrans_test', 'data', 'ki_mor_1', 'valid_tgt.txt'), 'w') as f:\n",
    "    for item in val_tgt:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "test_tgt = list(ki_mor_1_df.loc[ki_mor_1_df['set'] == 'TEST', 'pchembl_value'].values)\n",
    "with open(os.path.join(workdir, 'enztrans_test', 'data', 'ki_mor_1', 'test_tgt.txt'), 'w') as f:\n",
    "    for item in test_tgt:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/OpenNMT-py/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "# print the path for module `preprocess`\n",
    "print(preprocess.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser('Preprocess the data')\n",
    "\n",
    "parser.add_argument('--train_src', required=True, help='Path(s) to the training source data')\n",
    "parser.add_argument('--train_tgt', required=True, help='Path(s) to the training target data')\n",
    "parser.add_argument('--val_src', help='Path(s) to the validation source data')\n",
    "parser.add_argument('--val_tgt', help='Path(s) to the validation target data')\n",
    "\n",
    "parser.add_argument('--save_data', required=True, help='Path to save the preprocessed data')\n",
    "parser.add_argument('--src_seq_length', type=int, default=50, help='Maximal allowable source sequence length')\n",
    "parser.add_argument('--tgt_seq_length', type=int, default=50, help='Maximal allowable target sequence length')\n",
    "\n",
    "parser.add_argument('--src_vocab_size', type=int, default=50000, help='Size of the source vocabulary')\n",
    "parser.add_argument('--tgt_vocab_size', type=int, default=50000, help='Size of the target vocabulary')\n",
    "parser.add_argument('--share_vocab', action='store_true', help='share source and target vocabulary')\n",
    "parser.add_argument('--lower', action='store_true', help='lowercase data')\n",
    "\n",
    "# make a directory 'preprocess' in the current working directory\n",
    "os.makedirs('preprocessed', exist_ok=True)\n",
    "\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Preprocess the data [-h] --train_src TRAIN_SRC --train_tgt TRAIN_TGT\n",
      "                           [--val_src VAL_SRC] [--val_tgt VAL_TGT] --save_data\n",
      "                           SAVE_DATA [--src_seq_length SRC_SEQ_LENGTH]\n",
      "                           [--tgt_seq_length TGT_SEQ_LENGTH]\n",
      "                           [--src_vocab_size SRC_VOCAB_SIZE]\n",
      "                           [--tgt_vocab_size TGT_VOCAB_SIZE] [--share_vocab]\n",
      "                           [--lower]\n",
      "Preprocess the data: error: the following arguments are required: --train_src, --train_tgt, --save_data\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess.main(\n",
    "    TRAIN_SRC=train_src,\n",
    "    train_tgt=train_tgt,\n",
    "    valid_src=val_src,\n",
    "    valid_tgt=val_tgt,\n",
    "    save_data='preprocess',\n",
    "    src_seq_length=3000, \n",
    "    tgt_seq_length=3000,\n",
    "    src_vocab_size=3000,\n",
    "    tgt_vocab_size=3000,\n",
    "    share_vocab=True,\n",
    "    lower=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enztrans_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
