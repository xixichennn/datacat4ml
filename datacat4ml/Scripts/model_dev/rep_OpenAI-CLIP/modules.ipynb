{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/yc24j783/miniconda3/envs/pyg/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/storage/homefs/yc24j783/miniconda3/envs/pyg/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "import config as CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__() # `super()` is used to call the constructor of the parent class (nn.Module).This ensures this class is properly initialized as a nn.Module. \n",
    "        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool='avg')\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "        \n",
    "        # we are using the CLS token hidden representation as sentence's embedding\n",
    "        # Yu: this sets the index of the target token to 0. In Bert-based models, the CLS token (classification token) is typically the first token in the sequence and its index is 0.\n",
    "        # The CLS token is used as a representation of the entire input sequence.\n",
    "        self.target_token_idx = 0 # Yu:?\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "\n",
    "        # last_hidden_state is a tensor with shape (batch_size, seq_len, hidden_size),\n",
    "        # : (first position) selects all batches,\n",
    "        # self.target_token_idx (second position) selects the CLS token (the first token in the sequence),\n",
    "        # : (third position) selects all hidden dimensions\n",
    "        # therefore, thus function returns the hidden state of the CLS token for each input in the batch\n",
    "\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim, \n",
    "            projection_dim=CFG.projection_dim,\n",
    "            dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim) # a linear layer that projects the input embeddings from the embedding_dim to the projection_dim\n",
    "        self.gelu = nn.GELU() # A GELU(Gaussian Error Linear Unit) activation function.\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim) # another linear layer that maintains the 'project_dim'\n",
    "        self.dropout = nn.Dropout(dropout) # a dropout layer for regularization\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim) # a layer normalization layer to normalize the output\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x is the input tensor\n",
    "        projected = self.projection(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected # adds the original projected embeddings to the output(residual connection)\n",
    "        x = self.layer_norm(x) # applies layer normalization to the output\n",
    "\n",
    "        return x\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
